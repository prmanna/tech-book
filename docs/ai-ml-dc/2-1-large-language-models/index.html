<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  Large Language Models (LLM) - Part 1/2: Word Embedding
  #


  Introduction
  #

This chapter introduces the basic operations of Transformer-based Large Language Models (LLMs), focusing on fundamental concepts rather than any specific LLM, such as OpenAI’s GPT (Generative Pretrained Transformer).The chapter begins with an introduction to tokenization and word embeddings, which convert input words into a format the model can process. Next, it explains how the transformer component leverages decoder architecture for input processing and prediction. "><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://prasenjitmanna.com/tech-book/docs/ai-ml-dc/2-1-large-language-models/"><meta property="og:site_name" content="Technical Book"><meta property="og:title" content="Large Language Models (LLM)"><meta property="og:description" content=" Large Language Models (LLM) - Part 1/2: Word Embedding # Introduction # This chapter introduces the basic operations of Transformer-based Large Language Models (LLMs), focusing on fundamental concepts rather than any specific LLM, such as OpenAI’s GPT (Generative Pretrained Transformer).The chapter begins with an introduction to tokenization and word embeddings, which convert input words into a format the model can process. Next, it explains how the transformer component leverages decoder architecture for input processing and prediction. "><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:modified_time" content="2025-05-05T18:06:10+05:30"><title>Large Language Models (LLM) | Technical Book</title>
<link rel=manifest href=/tech-book/manifest.json><link rel=icon href=/tech-book/favicon.png><link rel=canonical href=https://prasenjitmanna.com/tech-book/docs/ai-ml-dc/2-1-large-language-models/><link rel=stylesheet href=/tech-book/book.min.a61cdb2979f3c2bece54ef69131fba427dd57d55c232d3bb5fdb62ac41aa8354.css integrity="sha256-phzbKXnzwr7OVO9pEx+6Qn3VfVXCMtO7X9tirEGqg1Q=" crossorigin=anonymous><script defer src=/tech-book/fuse.min.js></script><script defer src=/tech-book/en.search.min.c2c354c8626cd85775d8f153338ee7bd352fae7c24c490a56a7843fe3fff7a74.js integrity="sha256-wsNUyGJs2Fd12PFTM47nvTUvrnwkxJClanhD/j//enQ=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/tech-book/><img src=/tech-book/logo.png alt=Logo><span>Technical Book</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><input type=checkbox id=section-99f552133860bc21797a47fe73e93434 class=toggle>
<label for=section-99f552133860bc21797a47fe73e93434 class="flex justify-between"><a href=/tech-book/docs/5g/>5G</a></label><ul><li><input type=checkbox id=section-dfc790b73acb0410a0114547cbf5af32 class=toggle>
<label for=section-dfc790b73acb0410a0114547cbf5af32 class="flex justify-between"><a href=/tech-book/docs/5g/5g-intro/>An Overview of 5G Networking</a></label></li></ul></li><li><input type=checkbox id=section-7c4a3b16aeeb5b7194b232c1eef2f4fb class=toggle>
<label for=section-7c4a3b16aeeb5b7194b232c1eef2f4fb class="flex justify-between"><a href=/tech-book/docs/algorithms/>Algorithms</a></label><ul><li><input type=checkbox id=section-8956d82fbe6869140758f7e8679174bc class=toggle>
<label for=section-8956d82fbe6869140758f7e8679174bc class="flex justify-between"><a href=/tech-book/docs/algorithms/breadth-first-search/>Breadth First Search</a></label></li><li><input type=checkbox id=section-5a049cfad1740f3fd30565524385fa57 class=toggle>
<label for=section-5a049cfad1740f3fd30565524385fa57 class="flex justify-between"><a href=/tech-book/docs/algorithms/depth-first-search/>Depth First Search</a></label></li><li><input type=checkbox id=section-ebc049f26d82165be8c6f1f9e504e799 class=toggle>
<label for=section-ebc049f26d82165be8c6f1f9e504e799 class="flex justify-between"><a href=/tech-book/docs/algorithms/easy/>Easy Complexity</a></label></li><li><input type=checkbox id=section-1071946392bd1f431993e950147fa054 class=toggle>
<label for=section-1071946392bd1f431993e950147fa054 class="flex justify-between"><a href=/tech-book/docs/algorithms/priority-queue-and-heap/>Priority Queue and Heap</a></label></li><li><input type=checkbox id=section-08afbeb294c43ca4908c1c89a4be9d0a class=toggle>
<label for=section-08afbeb294c43ca4908c1c89a4be9d0a class="flex justify-between"><a href=/tech-book/docs/algorithms/two-pointers/>Two Pointers & Sliding Window</a></label></li><li><input type=checkbox id=section-ef36c7c4f7e0dec6525068c3c409100c class=toggle>
<label for=section-ef36c7c4f7e0dec6525068c3c409100c class="flex justify-between"><a href=/tech-book/docs/algorithms/medium/>Medium Complexity</a></label></li></ul></li><li><input type=checkbox id=section-8c7c5c4a8382299873178820b1d91be1 class=toggle checked>
<label for=section-8c7c5c4a8382299873178820b1d91be1 class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/>Data Center Networking for AI Clusters</a></label><ul><li><input type=checkbox id=section-433ccede4154db01f6c601940d2949e0 class=toggle>
<label for=section-433ccede4154db01f6c601940d2949e0 class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/1-ai-ml-networking/>AI/ML Networking</a></label></li><li><input type=checkbox id=section-65f71f2455a94ea3d1143666d556b0ed class=toggle>
<label for=section-65f71f2455a94ea3d1143666d556b0ed class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/2-ai-deep-learning-basics/>Deep Learning Basics | Artificial Neuron</a></label></li><li><input type=checkbox id=section-af7b72510cdccfb9feb048bbf70c6f27 class=toggle checked>
<label for=section-af7b72510cdccfb9feb048bbf70c6f27 class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/2-1-large-language-models/ class=active>Large Language Models (LLM)</a></label></li><li><input type=checkbox id=section-f11d3aa2e337730e1e3345f8530fe134 class=toggle>
<label for=section-f11d3aa2e337730e1e3345f8530fe134 class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/3-challenges-in-ai-fabric/>Challenges in AI Fabric Design</a></label></li><li><input type=checkbox id=section-e28420ec7507646128f3695b6f9badbb class=toggle>
<label for=section-e28420ec7507646128f3695b6f9badbb class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/4-congestion-avoidance-in-ai-fabric/>Congestion Avoidance in AI Fabric</a></label></li><li><input type=checkbox id=section-67bb7cbb1dd95e59f8515201219c090f class=toggle>
<label for=section-67bb7cbb1dd95e59f8515201219c090f class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/5-load-balancing-in-ai-fabric/>Load Balancing in AI Fabric</a></label></li><li><input type=checkbox id=section-5f3e07f6cf7921e5291ff7894e06b19b class=toggle>
<label for=section-5f3e07f6cf7921e5291ff7894e06b19b class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/6-backend-network-topologies-for-ai-fabric/>Backend Network Topologies for AI Fabrics</a></label></li><li><input type=checkbox id=section-adb8e29f405ea627a0e41d43e94e3453 class=toggle>
<label for=section-adb8e29f405ea627a0e41d43e94e3453 class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/7-rail-desings-in-gpu-fabric/>Rail Designs in GPU Fabric</a></label></li></ul></li><li><input type=checkbox id=section-3272b2d28b2b247027bf478619ca416f class=toggle>
<label for=section-3272b2d28b2b247027bf478619ca416f class="flex justify-between"><a href=/tech-book/docs/data-center/>Data Center Tips</a></label><ul><li><input type=checkbox id=section-984f932c7aba4f0a1841e8413165c947 class=toggle>
<label for=section-984f932c7aba4f0a1841e8413165c947 class="flex justify-between"><a href=/tech-book/docs/data-center/data-center-ethernet/>Data Center Ethernet</a></label></li><li><input type=checkbox id=section-bc5ac88940153a700e63e3be886c63cc class=toggle>
<label for=section-bc5ac88940153a700e63e3be886c63cc class="flex justify-between"><a href=/tech-book/docs/data-center/data-center-technologies/>Data Center Technologies</a></label></li><li><input type=checkbox id=section-86fde1ddf43700ef8191e11c59a82cf1 class=toggle>
<label for=section-86fde1ddf43700ef8191e11c59a82cf1 class="flex justify-between"><a href=/tech-book/docs/data-center/data-center-network-virtualization/>Network Virtualization in Cloud Data Centers</a></label></li></ul></li><li><input type=checkbox id=section-ddf784688e0c6d5abb681d2c57851559 class=toggle>
<label for=section-ddf784688e0c6d5abb681d2c57851559 class="flex justify-between"><a href=/tech-book/docs/manageability/>Manageability</a></label><ul><li><input type=checkbox id=section-33ac730897af6feca81c1fdb7869c57e class=toggle>
<label for=section-33ac730897af6feca81c1fdb7869c57e class="flex justify-between"><a href=/tech-book/docs/manageability/why-grpc-on-http2/>gRPC on HTTP/2</a></label></li></ul></li><li><input type=checkbox id=section-c14ae944424668ef125e10cd791a3d3d class=toggle>
<label for=section-c14ae944424668ef125e10cd791a3d3d class="flex justify-between"><a href=/tech-book/docs/networking-tips/>Networking Tips</a></label><ul><li><input type=checkbox id=section-78fdf21c03c55935d3146441b06faf3e class=toggle>
<label for=section-78fdf21c03c55935d3146441b06faf3e class="flex justify-between"><a href=/tech-book/docs/networking-tips/dns/>DNS Overview</a></label></li><li><input type=checkbox id=section-bbcb027a658dc7a2333d59078ee507f9 class=toggle>
<label for=section-bbcb027a658dc7a2333d59078ee507f9 class="flex justify-between"><a href=/tech-book/docs/networking-tips/ecmp/>ECMP Load Balancing</a></label></li><li><input type=checkbox id=section-3b39b86f18b3451e5b8a1b81c369549c class=toggle>
<label for=section-3b39b86f18b3451e5b8a1b81c369549c class="flex justify-between"><a href=/tech-book/docs/networking-tips/ip-fragmentation/>IP Fragmentation - IPv4 & IPv6</a></label></li><li><input type=checkbox id=section-c6d06c54cc91b0bc3f948d33b437fa8b class=toggle>
<label for=section-c6d06c54cc91b0bc3f948d33b437fa8b class="flex justify-between"><a href=/tech-book/docs/networking-tips/ip-tos-dscp/>IP Precedence And TOS | DSCP</a></label></li><li><input type=checkbox id=section-5f3260c76dd37177e3f89057bfe520ae class=toggle>
<label for=section-5f3260c76dd37177e3f89057bfe520ae class="flex justify-between"><a href=/tech-book/docs/networking-tips/traceroute/>Linux traceroute tool</a></label></li><li><input type=checkbox id=section-a26cc3fafb36cbc55527adf39ec83849 class=toggle>
<label for=section-a26cc3fafb36cbc55527adf39ec83849 class="flex justify-between"><a href=/tech-book/docs/networking-tips/mlag/>Multi Chassis Link Aggregation Basics</a></label></li><li><input type=checkbox id=section-36409a0abcb2d4d4baf2e0b682d1a5dd class=toggle>
<label for=section-36409a0abcb2d4d4baf2e0b682d1a5dd class="flex justify-between"><a href=/tech-book/docs/networking-tips/qos/>QoS</a></label></li><li><input type=checkbox id=section-db41e3547d316b01acf9a0ce9c04ef34 class=toggle>
<label for=section-db41e3547d316b01acf9a0ce9c04ef34 class="flex justify-between"><a href=/tech-book/docs/networking-tips/spine-leaf-arch/>Spine-leaf Architecture Basics</a></label></li><li><input type=checkbox id=section-0eb0d409074a76b8cb02624655e2434d class=toggle>
<label for=section-0eb0d409074a76b8cb02624655e2434d class="flex justify-between"><a href=/tech-book/docs/networking-tips/tcp-congestion/>TCP Congestion Control</a></label></li><li><input type=checkbox id=section-3d1710947b3c5be1a1cc33d88fcc6f54 class=toggle>
<label for=section-3d1710947b3c5be1a1cc33d88fcc6f54 class="flex justify-between"><a href=/tech-book/docs/networking-tips/tcp-data-transfer/>TCP Data Transfer</a></label></li></ul></li><li><input type=checkbox id=section-f0a392f2f083f28a4991336773716a63 class=toggle>
<label for=section-f0a392f2f083f28a4991336773716a63 class="flex justify-between"><a href=/tech-book/docs/optical-knowledge/>Optical Knowledge</a></label><ul><li><input type=checkbox id=section-18261b95a92d4fa86116243edca4e9fb class=toggle>
<label for=section-18261b95a92d4fa86116243edca4e9fb class="flex justify-between"><a href=/tech-book/docs/optical-knowledge/optical-breakout/>Optical Transceiver(Grey) & Breakout Model</a></label></li></ul></li><li><input type=checkbox id=section-a55840d746138b3d1fedb81acbccdded class=toggle>
<label for=section-a55840d746138b3d1fedb81acbccdded class="flex justify-between"><a href=/tech-book/docs/programming-tips/>Programming Tips</a></label><ul><li><input type=checkbox id=section-f929f1fe13cb5d6cc96ca3e98f9d9777 class=toggle>
<label for=section-f929f1fe13cb5d6cc96ca3e98f9d9777 class="flex justify-between"><a href=/tech-book/docs/programming-tips/c++/>C++ Tips</a></label></li></ul></li><li><input type=checkbox id=section-b35dbf5ebcd4c19926cf5a9aab6c7655 class=toggle>
<label for=section-b35dbf5ebcd4c19926cf5a9aab6c7655 class="flex justify-between"><a href=/tech-book/docs/systemdesign-tips/>SystemDesign-Tips</a></label><ul><li><input type=checkbox id=section-4a618bc3b0b30107f0cec6d3bd6c025f class=toggle>
<label for=section-4a618bc3b0b30107f0cec6d3bd6c025f class="flex justify-between"><a href=/tech-book/docs/systemdesign-tips/code-deployment-system/>Design A Code-Deployment System</a></label></li><li><input type=checkbox id=section-e600754306aa95ce9c5a72b5efec6d7a class=toggle>
<label for=section-e600754306aa95ce9c5a72b5efec6d7a class="flex justify-between"><a href=/tech-book/docs/systemdesign-tips/stock-broker/>Design A Stock-Broker System</a></label></li><li><input type=checkbox id=section-bfa7a29878f65cfbb179e491c1211fa8 class=toggle>
<label for=section-bfa7a29878f65cfbb179e491c1211fa8 class="flex justify-between"><a href=/tech-book/docs/systemdesign-tips/design-amazon/>Design Amazon</a></label></li><li><input type=checkbox id=section-5456837e25872f6352d861a9b5662cb1 class=toggle>
<label for=section-5456837e25872f6352d861a9b5662cb1 class="flex justify-between"><a href=/tech-book/docs/systemdesign-tips/design-slack/>Design Slack</a></label></li><li><input type=checkbox id=section-5a78d16f54536a400b654f17f917bce1 class=toggle>
<label for=section-5a78d16f54536a400b654f17f917bce1 class="flex justify-between"><a href=/tech-book/docs/systemdesign-tips/google-drive/>Google Drive - Design</a></label></li></ul></li></ul><ul><li><a href=/tech-book/posts/>Blog</a></li><li><a href=https://prasenjitmanna.com/ target=_blank rel=noopener>Prasenjit's Blog</a></li><li><a href=https://prasenjitmanna.com/tech-book/ target=_blank rel=noopener>Prasenjit - Tech Book</a></li><li><a href=https://prasenjitmanna.com/upskills/ target=_blank rel=noopener>Prasenjit - Upskills</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/tech-book/svg/menu.svg class=book-icon alt=Menu>
</label><strong>Large Language Models (LLM)</strong>
<label for=toc-control><img src=/tech-book/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><ul><li><ul><li><a href=#large-language-models-llm---part-12-word-embedding>Large Language Models (LLM) - Part 1/2: Word Embedding</a></li><li><a href=#introduction>Introduction</a></li><li><a href=#tokenizer-and-word-embedding-matrix>Tokenizer and Word Embedding Matrix</a></li><li><a href=#word-embedding>Word Embedding</a></li><li><a href=#positional-embeddings>Positional Embeddings</a></li><li><a href=#calculating-the-final-word-embedding>Calculating the Final Word Embedding</a></li></ul></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h3 id=large-language-models-llm---part-12-word-embedding>Large Language Models (LLM) - Part 1/2: Word Embedding
<a class=anchor href=#large-language-models-llm---part-12-word-embedding>#</a></h3><h3 id=introduction>Introduction
<a class=anchor href=#introduction>#</a></h3><p>This chapter introduces the basic operations of Transformer-based Large Language Models (LLMs), focusing on fundamental concepts rather than any specific LLM, such as OpenAI’s GPT (Generative Pretrained Transformer).The chapter begins with an introduction to tokenization and word embeddings, which convert input words into a format the model can process. Next, it explains how the transformer component leverages decoder architecture for input processing and prediction. </p><p>This chapter has two main goals. First, it explains how an LLM understands the context of a word. For example, the word “clear” can be used as a verb (Please, clear the table.) or as an adjective (The sky was clear.), depending on the context. Second, it discusses why LLMs require parallelization across hundreds or even thousands of GPUs due to the large model size, massive datasets, and the computational complexity involved.</p><h3 id=tokenizer-and-word-embedding-matrix>Tokenizer and Word Embedding Matrix
<a class=anchor href=#tokenizer-and-word-embedding-matrix>#</a></h3><p>As a first step, we import a vocabulary into the model. The vocabulary used for training large language models (LLMs) typically consists of a mix of general and domain-specific terms, including basic vocabulary, technical terminology, academic and formal language, idiomatic expressions, cultural references, as well as synonyms and antonyms. Each word and character is stored in a word lookup table and assigned a unique token. This process is called tokenization.</p><p>Many LLMs use Byte Pair Encoding (BPE), which splits words into subword units. For example, the word &ldquo;unhappiness&rdquo; might be broken down into &ldquo;un,&rdquo; &ldquo;happi,&rdquo; and &ldquo;ness.&rdquo; BPE is widely used because it effectively balances vocabulary size and tokenization efficiency, particularly for handling rare words and sub-words. For simplicity, we use complete words in all our examples.</p><p>Figure 7-1 illustrates the relationship between words in the vocabulary and their corresponding tokens. Token values start from 2 because token 0 is reserved for padding and token 1 for unknown words.</p><p>Each token, representing a word, is mapped to a Word Embedding Vector, which is initially assigned random values. The collection of these vectors forms a Word Embedding Matrix. The dimensionality of each vector determines how much contextual information it can encode.</p><p>For example, consider the word “clear.” A two-dimensional vector may distinguish it as either an adjective or a verb but lacks further contextual information. By increasing the number of dimensions, the model can capture more context and better understand the meaning of the word. In the sentence “The sky was clear,” the phrase “The sky was” suggests that &ldquo;clear&rdquo; is an adjective. However, if we extend the sentence to “She decided to clear the backyard of junk,” the word &ldquo;clear&rdquo; now functions as a verb. More dimensions allow the model to utilize surrounding words more effectively for next-word prediction. For instance, GPT-3 uses 12,288-dimensional vectors. Given a vocabulary size of 50,000 words used by GPT-3, the Word Embedding Matrix has dimensions of 12,288 × 50,000, resulting in 614,400,000 parameters.</p><p>The context size, defined as the sequence length of vectors, determines how many preceding words the model considers when predicting the next word. In GPT-3, the context size is 2,048 tokens.</p><p><a href=https://blogger.googleusercontent.com/img/a/AVvXsEhE3s3vC-QZBZ5Xdb3q6X0JR8uSa25smRVB0pk-r_IGUGBIb6mnS4CumYCwPYKtqsKK-3nE4qCsVCd3m62iraz124xvdhtkaWs4YgVyfsrH8TjI2iiMsd_XIUpBhSnl_TvyWCxkCb916cXjChKDaQ3DWFXwlcF9p2b1X6qNzoXPa6F0fAH7xsp86cGZU_k><img src="https://blogger.googleusercontent.com/img/a/AVvXsEhE3s3vC-QZBZ5Xdb3q6X0JR8uSa25smRVB0pk-r_IGUGBIb6mnS4CumYCwPYKtqsKK-3nE4qCsVCd3m62iraz124xvdhtkaWs4YgVyfsrH8TjI2iiMsd_XIUpBhSnl_TvyWCxkCb916cXjChKDaQ3DWFXwlcF9p2b1X6qNzoXPa6F0fAH7xsp86cGZU_k=w640-h324" alt></a></p><p><strong>Figure 7-1:</strong> <em>Tokenization and Word Embedding Matrix.</em></p><h3 id=word-embedding>Word Embedding
<a class=anchor href=#word-embedding>#</a></h3><p>As a first step, when we feed input words into a Natural Language Processing (NLP) model, we must convert them into a format the model can understand. This is a two-step process:</p><ol><li><strong>Tokenization</strong> – Each word is assigned a corresponding token from a lookup table.</li><li><strong>Word Embedding</strong> – These token IDs are then mapped to vectors using a word embedding lookup table.</li></ol><p>To keep things simple, Figure 7-2 uses two-dimensional vectors in the embedding matrix. Instead of complete sentences, we use words, which can be categorized into four groups: female, male, adult, and child.</p><p>The first word, &ldquo;Wife,&rdquo; appears in the lookup table with the token value 2. The corresponding word vector in the lookup table for token 2 is [-4.5, -3.0]. Note that Figure 7-2 represents vectors as column vectors, whereas in the text, I use row vectors—but they contain the same values.</p><p>The second word, &ldquo;Mother,&rdquo; is assigned the token 3, which is associated with the word vector [-2.5, +3.0], and so on.</p><p><a href=https://blogger.googleusercontent.com/img/a/AVvXsEiJNUFCwHAwH0IzyA13HyM6gLoRNvaW0dXpO-SyB7C3dHaRtK0Vu1Qwk6aAKXNTwi1aZ5pmoHQShSVckIXrs-W_aJ5qGBH_MVPmhHbXswCxPqtrvkOkeBCXtqFe_qYM6vAOLyh5gv36ZfGw_Q1ADLNhhStm4ycchpTvIcpRvwpxtETZhiTzpptddMr4gtE><img src="https://blogger.googleusercontent.com/img/a/AVvXsEiJNUFCwHAwH0IzyA13HyM6gLoRNvaW0dXpO-SyB7C3dHaRtK0Vu1Qwk6aAKXNTwi1aZ5pmoHQShSVckIXrs-W_aJ5qGBH_MVPmhHbXswCxPqtrvkOkeBCXtqFe_qYM6vAOLyh5gv36ZfGw_Q1ADLNhhStm4ycchpTvIcpRvwpxtETZhiTzpptddMr4gtE=w640-h362" alt></a></p><p><strong>Figure 7-2:</strong> <em>Word Tokenization and Word Embedding.</em></p><p>In Figure 7-3, we have a two-dimensional vector space divided into four quadrants, representing gender (male/female) and age (child/adult). Tokenized words are mapped into this space.</p><p>At the start of the first iteration, all words are placed randomly within the two-dimensional space. During training, our goal is to adjust the word vector values so that adults are positioned on the positive side of the Y-axis and children on the negative side. Similarly, males are placed in the negative space of the X-axis, while females are positioned on the positive side.</p><p><a href=https://blogger.googleusercontent.com/img/a/AVvXsEhmDPYthV-EqTbhPrj3Dumz5g6wzevO-cwmPartMfq-EEdfOzqmJKYAJfqYxmiQnE657ZZlqxPDSuc5nk7W7lLj7LEkn4BQvx9rWEBZTfM3_84rIq4rtAzs-GlMzuf3p3Srw1HIIby9StqD8Pcmz8MebV4TH_iN8xpAOx7Z-NIs3e6ZWbSQPaZwEt788XY><img src="https://blogger.googleusercontent.com/img/a/AVvXsEhmDPYthV-EqTbhPrj3Dumz5g6wzevO-cwmPartMfq-EEdfOzqmJKYAJfqYxmiQnE657ZZlqxPDSuc5nk7W7lLj7LEkn4BQvx9rWEBZTfM3_84rIq4rtAzs-GlMzuf3p3Srw1HIIby9StqD8Pcmz8MebV4TH_iN8xpAOx7Z-NIs3e6ZWbSQPaZwEt788XY=w640-h460" alt></a></p><p><strong>Figure 7-3:</strong> <em>Words in the 2 Dimensional Vector Space in the Initial State.</em></p><p>Figure 7-4 illustrates how words may be positioned after successful training. All words representing a male adult are placed in the upper-left quadrant (adult/male). Similarly, all other words are positioned in the two-dimensional vector space based on their corresponding age and gender.</p><p><a href=https://blogger.googleusercontent.com/img/a/AVvXsEiP9XM5LfT6OpI-CLQdQYphnj9jeAeBg3jY2JqlWBAOB4RxvR48uhks43cX3_9Noddz1ObKN1IaPLnlHv2OHU68zifASvm8_y_lCqk7jWAMbalyKB9tXF25NSiHMWOSLDlf0r6N6YYOT8nvzht5tYV1aO1MQgmDaDVOynNqcdMl3iDWLmIKG7ARwyMstTg><img src="https://blogger.googleusercontent.com/img/a/AVvXsEiP9XM5LfT6OpI-CLQdQYphnj9jeAeBg3jY2JqlWBAOB4RxvR48uhks43cX3_9Noddz1ObKN1IaPLnlHv2OHU68zifASvm8_y_lCqk7jWAMbalyKB9tXF25NSiHMWOSLDlf0r6N6YYOT8nvzht5tYV1aO1MQgmDaDVOynNqcdMl3iDWLmIKG7ARwyMstTg=w640-h460" alt></a></p><p><strong>Figure 7-4:</strong> <em>Words in the 2 Dimensional Vector Space After Training.</em></p><p>In addition to grouping similar words, such as &ldquo;adult/female,&rdquo; close to each other in an n-dimensional space, there should also be positional similarities between words in different quadrants. For example, if we calculate the Euclidean distance between the words Father and Mother, we might find that their distance is approximately 4.3. The same pattern applies to word pairs like Nephew-Niece, Brother-Sister, Husband-Wife, and Father-in-Law–Mother-in-Law.</p><p>However, it is important to note that this example is purely theoretical. In practice, Euclidean distances in high-dimensional word embeddings are not fixed but vary depending on the training data and optimization process. The relationships between words are often better captured through cosine similarity rather than absolute Euclidean distances.</p><p><a href=https://blogger.googleusercontent.com/img/a/AVvXsEj_ucF4kW1YBUkvZ82DjUXNnuy2FH-Ltvmnb1zhWyUiS86xZ-ncN3h3CEsBQVMwbwkvzuvARbGz8NEAYIVTWIwz_DZ6OP0u24dK0WdMKRJk7OcXxUWtHeRK9BiTc9na6rD4rnQpd09Mx2OX1ZOV4vS0KluRIqao_tWVd0BHf_kFoI5qgijiHpjpaXZEtR8><img src="https://blogger.googleusercontent.com/img/a/AVvXsEj_ucF4kW1YBUkvZ82DjUXNnuy2FH-Ltvmnb1zhWyUiS86xZ-ncN3h3CEsBQVMwbwkvzuvARbGz8NEAYIVTWIwz_DZ6OP0u24dK0WdMKRJk7OcXxUWtHeRK9BiTc9na6rD4rnQpd09Mx2OX1ZOV4vS0KluRIqao_tWVd0BHf_kFoI5qgijiHpjpaXZEtR8=w640-h436" alt></a></p><p><strong>Figure 7-5:</strong> <em>Euclidean Distance.</em></p><h3 id=positional-embeddings>Positional Embeddings
<a class=anchor href=#positional-embeddings>#</a></h3><p>Since input text often contains repeated words with different meanings depending on their position, an LLM must distinguish between them. To achieve this, the word embedding process in Natural Language Processing (NLP) incorporates a Positional Encoding Vector alongside the Word Embedding Vector, resulting in the final word representation.</p><p>In Figure 7-6, the sentence &ldquo;The sky is clear, so she finally decided to clear the backyard&rdquo; contains the word clear twice. Repeated words share the same token ID instead of receiving unique ones. In this example, the is assigned token ID 2, and clear is assigned 5. These token IDs are then mapped to vectors using a word embedding lookup table. However, without positional encoding, words with different meanings would share the same vector representation.</p><p>Focusing on clear (token ID 5), it maps to the word embedding vector [+2.5, +1.0] from the lookup table. Since token IDs do not capture word position, identical words always receive the same embedding.</p><p>Positional encoding is essential for capturing context and semantic meaning. As shown in Figure 7-6, each input word receives a Positional Encoding Vector (PE) in addition to its word embedding. PE can either be learned and adjusted during training or remain fixed. The final Word Embedding Vector is computed by combining both the Word Embedding Vector and Positional Encoding Vector.</p><p><a href=https://blogger.googleusercontent.com/img/a/AVvXsEhDazqAestp6Nt6DDKIpfyZwIqfqF0Pr-m_Ca9wrL8YWQT5JytF0u4b-QtaqdjC5gpA2hDWhZzq8p7MeZymgUfYlY-zty1JU5jYewAwm1v0MQJimsKjL0ykj5BBiSwocYQJ3-FX05KuS0b6AFg2Z68LoHFDxbGLvo0GLSZ5rcLMWh3vNa9xACf5Hf4TzGk><img src="https://blogger.googleusercontent.com/img/a/AVvXsEhDazqAestp6Nt6DDKIpfyZwIqfqF0Pr-m_Ca9wrL8YWQT5JytF0u4b-QtaqdjC5gpA2hDWhZzq8p7MeZymgUfYlY-zty1JU5jYewAwm1v0MQJimsKjL0ykj5BBiSwocYQJ3-FX05KuS0b6AFg2Z68LoHFDxbGLvo0GLSZ5rcLMWh3vNa9xACf5Hf4TzGk=w640-h360" alt></a></p><p><strong>Figure 7-6:</strong> <em>Tokenization – Positional Embedding Vector.</em></p><h3 id=calculating-the-final-word-embedding>Calculating the Final Word Embedding
<a class=anchor href=#calculating-the-final-word-embedding>#</a></h3><p>Figure 7-2 presents the equations for computing the final word embedding by incorporating positional embeddings. There are three variables:</p><ul><li><strong>Position (pos)</strong> → The word’s position in the sentence. In our example, the first occurrence of clear is the fourth word, so pos = 4.</li><li><strong>Dimension (d)</strong> → The depth of the vector. We use a 2-dimensional vector, so d = 2.</li><li><strong>Index (i)</strong> → Specifies the axis of the vector: 0 for the x-axis and 1 for the y-axis.</li></ul><p>The positional embedding is computed using the following equations:</p><ul><li>x-axis: sin(pos/100002i/d), where i = 0</li><li>y-axis: cos(pos/100002i/d, where i = 1</li></ul><p>For clear at position 4, with d = 2, the resulting 2D positional vector is [-0.8, +1.0]. This vector is then added to the input word embedding vector [+2.5, +1.0], resulting in the final word embedding vector [+1.7, +2.0].</p><p>Figure 7-7 also shows the final word embedding for the second occurrence of clear, but the computation is omitted.</p><p><a href=https://blogger.googleusercontent.com/img/a/AVvXsEg6OQIgMQrC-X5XHfGUwfFxCUqydZXPsrNU91o69ia9K4xOHaD2MGyZ3AubPCNK-obbP_sJS1NyioSfjJHDBWpV7AvzX24esyCVt6vLLVz-lEog6LmAvcLnOf0BFCQbgsBVltN9KFqyyKzO_MA1I9tq1AWiqaEWzkU-V1VlEhTPEHwQ55q79aBio8p21kE><img src="https://blogger.googleusercontent.com/img/a/AVvXsEg6OQIgMQrC-X5XHfGUwfFxCUqydZXPsrNU91o69ia9K4xOHaD2MGyZ3AubPCNK-obbP_sJS1NyioSfjJHDBWpV7AvzX24esyCVt6vLLVz-lEog6LmAvcLnOf0BFCQbgsBVltN9KFqyyKzO_MA1I9tq1AWiqaEWzkU-V1VlEhTPEHwQ55q79aBio8p21kE=w640-h380" alt></a></p><p><strong>Figure 7-7:</strong> <em>Finals Word Embedding for the 4th Word.</em></p><p><strong>References:</strong></p></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/prmanna/tech-book/commit/e7ba1570661bfa6aa36a64e1104f173cca6b7e46 title='Last modified by Prasenjit Manna | May 5, 2025' target=_blank rel=noopener><img src=/tech-book/svg/calendar.svg class=book-icon alt>
<span>May 5, 2025</span></a></div></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><ul><li><ul><li><a href=#large-language-models-llm---part-12-word-embedding>Large Language Models (LLM) - Part 1/2: Word Embedding</a></li><li><a href=#introduction>Introduction</a></li><li><a href=#tokenizer-and-word-embedding-matrix>Tokenizer and Word Embedding Matrix</a></li><li><a href=#word-embedding>Word Embedding</a></li><li><a href=#positional-embeddings>Positional Embeddings</a></li><li><a href=#calculating-the-final-word-embedding>Calculating the Final Word Embedding</a></li></ul></li></ul></li></ul></nav></div></aside></main></body></html>