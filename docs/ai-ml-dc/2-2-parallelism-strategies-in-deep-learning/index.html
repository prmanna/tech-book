<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  Introduction
  #


Data Parallelism
Model Parallelism
Pipeline Parallelism
Tensor Parallelism

Figure 8-1 depicts some of the model parameters that need to be stored in GPU memory: a) Weight matrices associated with connections to the preceding layer, b) Weighted sum (z), c) Activation values (y), d) Errors (E), e) Local gradients (local ∇), f) Gradients received from peer GPUs (remote ∇), g) Learning rates (LR), and h) Weight adjustment values (Δw)."><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://prasenjitmanna.com/tech-book/docs/ai-ml-dc/2-2-parallelism-strategies-in-deep-learning/"><meta property="og:site_name" content="Technical Book"><meta property="og:title" content="Parallelism Strategies in Deep Learning"><meta property="og:description" content=" Introduction # Data Parallelism Model Parallelism Pipeline Parallelism Tensor Parallelism Figure 8-1 depicts some of the model parameters that need to be stored in GPU memory: a) Weight matrices associated with connections to the preceding layer, b) Weighted sum (z), c) Activation values (y), d) Errors (E), e) Local gradients (local ∇), f) Gradients received from peer GPUs (remote ∇), g) Learning rates (LR), and h) Weight adjustment values (Δw)."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:modified_time" content="2025-05-16T21:15:25+05:30"><title>Parallelism Strategies in Deep Learning | Technical Book</title>
<link rel=manifest href=/tech-book/manifest.json><link rel=icon href=/tech-book/favicon.png><link rel=canonical href=https://prasenjitmanna.com/tech-book/docs/ai-ml-dc/2-2-parallelism-strategies-in-deep-learning/><link rel=stylesheet href=/tech-book/book.min.a61cdb2979f3c2bece54ef69131fba427dd57d55c232d3bb5fdb62ac41aa8354.css integrity="sha256-phzbKXnzwr7OVO9pEx+6Qn3VfVXCMtO7X9tirEGqg1Q=" crossorigin=anonymous><script defer src=/tech-book/fuse.min.js></script><script defer src=/tech-book/en.search.min.04d8fdda0a2583e3f371c53a5e6a9260003a878eaa02d4f7245151af06cfd560.js integrity="sha256-BNj92golg+PzccU6XmqSYAA6h46qAtT3JFFRrwbP1WA=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/tech-book/><img src=/tech-book/logo.png alt=Logo><span>Technical Book</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><input type=checkbox id=section-99f552133860bc21797a47fe73e93434 class=toggle>
<label for=section-99f552133860bc21797a47fe73e93434 class="flex justify-between"><a href=/tech-book/docs/5g/>5G</a></label><ul><li><input type=checkbox id=section-dfc790b73acb0410a0114547cbf5af32 class=toggle>
<label for=section-dfc790b73acb0410a0114547cbf5af32 class="flex justify-between"><a href=/tech-book/docs/5g/5g-intro/>An Overview of 5G Networking</a></label></li></ul></li><li><input type=checkbox id=section-7c4a3b16aeeb5b7194b232c1eef2f4fb class=toggle>
<label for=section-7c4a3b16aeeb5b7194b232c1eef2f4fb class="flex justify-between"><a href=/tech-book/docs/algorithms/>Algorithms</a></label><ul><li><input type=checkbox id=section-8956d82fbe6869140758f7e8679174bc class=toggle>
<label for=section-8956d82fbe6869140758f7e8679174bc class="flex justify-between"><a href=/tech-book/docs/algorithms/breadth-first-search/>Breadth First Search</a></label></li><li><input type=checkbox id=section-5a049cfad1740f3fd30565524385fa57 class=toggle>
<label for=section-5a049cfad1740f3fd30565524385fa57 class="flex justify-between"><a href=/tech-book/docs/algorithms/depth-first-search/>Depth First Search</a></label></li><li><input type=checkbox id=section-ebc049f26d82165be8c6f1f9e504e799 class=toggle>
<label for=section-ebc049f26d82165be8c6f1f9e504e799 class="flex justify-between"><a href=/tech-book/docs/algorithms/easy/>Easy Complexity</a></label></li><li><input type=checkbox id=section-1071946392bd1f431993e950147fa054 class=toggle>
<label for=section-1071946392bd1f431993e950147fa054 class="flex justify-between"><a href=/tech-book/docs/algorithms/priority-queue-and-heap/>Priority Queue and Heap</a></label></li><li><input type=checkbox id=section-08afbeb294c43ca4908c1c89a4be9d0a class=toggle>
<label for=section-08afbeb294c43ca4908c1c89a4be9d0a class="flex justify-between"><a href=/tech-book/docs/algorithms/two-pointers/>Two Pointers & Sliding Window</a></label></li><li><input type=checkbox id=section-ef36c7c4f7e0dec6525068c3c409100c class=toggle>
<label for=section-ef36c7c4f7e0dec6525068c3c409100c class="flex justify-between"><a href=/tech-book/docs/algorithms/medium/>Medium Complexity</a></label></li></ul></li><li><input type=checkbox id=section-8c7c5c4a8382299873178820b1d91be1 class=toggle checked>
<label for=section-8c7c5c4a8382299873178820b1d91be1 class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/>Data Center Networking for AI Clusters</a></label><ul><li><input type=checkbox id=section-433ccede4154db01f6c601940d2949e0 class=toggle>
<label for=section-433ccede4154db01f6c601940d2949e0 class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/1-ai-ml-networking/>AI/ML Networking</a></label></li><li><input type=checkbox id=section-65f71f2455a94ea3d1143666d556b0ed class=toggle>
<label for=section-65f71f2455a94ea3d1143666d556b0ed class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/2-ai-deep-learning-basics/>Deep Learning Basics | Artificial Neuron</a></label></li><li><input type=checkbox id=section-af7b72510cdccfb9feb048bbf70c6f27 class=toggle>
<label for=section-af7b72510cdccfb9feb048bbf70c6f27 class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/2-1-large-language-models/>Large Language Models (LLM)</a></label></li><li><input type=checkbox id=section-b3eb6a6d3a1b87cba26dcfbb99658303 class=toggle checked>
<label for=section-b3eb6a6d3a1b87cba26dcfbb99658303 class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/2-2-parallelism-strategies-in-deep-learning/ class=active>Parallelism Strategies in Deep Learning</a></label></li><li><input type=checkbox id=section-f11d3aa2e337730e1e3345f8530fe134 class=toggle>
<label for=section-f11d3aa2e337730e1e3345f8530fe134 class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/3-challenges-in-ai-fabric/>Challenges in AI Fabric Design</a></label></li><li><input type=checkbox id=section-e28420ec7507646128f3695b6f9badbb class=toggle>
<label for=section-e28420ec7507646128f3695b6f9badbb class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/4-congestion-avoidance-in-ai-fabric/>Congestion Avoidance in AI Fabric</a></label></li><li><input type=checkbox id=section-67bb7cbb1dd95e59f8515201219c090f class=toggle>
<label for=section-67bb7cbb1dd95e59f8515201219c090f class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/5-load-balancing-in-ai-fabric/>Load Balancing in AI Fabric</a></label></li><li><input type=checkbox id=section-5f3e07f6cf7921e5291ff7894e06b19b class=toggle>
<label for=section-5f3e07f6cf7921e5291ff7894e06b19b class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/6-backend-network-topologies-for-ai-fabric/>Backend Network Topologies for AI Fabrics</a></label></li><li><input type=checkbox id=section-e19a1e9030cc104536fa46dd0bdb082c class=toggle>
<label for=section-e19a1e9030cc104536fa46dd0bdb082c class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/7-backend-network-in-gpu-fabric/>Backend Network/Rail Designs</a></label></li></ul></li><li><input type=checkbox id=section-3272b2d28b2b247027bf478619ca416f class=toggle>
<label for=section-3272b2d28b2b247027bf478619ca416f class="flex justify-between"><a href=/tech-book/docs/data-center/>Data Center Tips</a></label><ul><li><input type=checkbox id=section-984f932c7aba4f0a1841e8413165c947 class=toggle>
<label for=section-984f932c7aba4f0a1841e8413165c947 class="flex justify-between"><a href=/tech-book/docs/data-center/data-center-ethernet/>Data Center Ethernet</a></label></li><li><input type=checkbox id=section-bc5ac88940153a700e63e3be886c63cc class=toggle>
<label for=section-bc5ac88940153a700e63e3be886c63cc class="flex justify-between"><a href=/tech-book/docs/data-center/data-center-technologies/>Data Center Technologies</a></label></li><li><input type=checkbox id=section-86fde1ddf43700ef8191e11c59a82cf1 class=toggle>
<label for=section-86fde1ddf43700ef8191e11c59a82cf1 class="flex justify-between"><a href=/tech-book/docs/data-center/data-center-network-virtualization/>Network Virtualization in Cloud Data Centers</a></label></li></ul></li><li><input type=checkbox id=section-ddf784688e0c6d5abb681d2c57851559 class=toggle>
<label for=section-ddf784688e0c6d5abb681d2c57851559 class="flex justify-between"><a href=/tech-book/docs/manageability/>Manageability</a></label><ul><li><input type=checkbox id=section-33ac730897af6feca81c1fdb7869c57e class=toggle>
<label for=section-33ac730897af6feca81c1fdb7869c57e class="flex justify-between"><a href=/tech-book/docs/manageability/why-grpc-on-http2/>gRPC on HTTP/2</a></label></li></ul></li><li><input type=checkbox id=section-c14ae944424668ef125e10cd791a3d3d class=toggle>
<label for=section-c14ae944424668ef125e10cd791a3d3d class="flex justify-between"><a href=/tech-book/docs/networking-tips/>Networking Tips</a></label><ul><li><input type=checkbox id=section-78fdf21c03c55935d3146441b06faf3e class=toggle>
<label for=section-78fdf21c03c55935d3146441b06faf3e class="flex justify-between"><a href=/tech-book/docs/networking-tips/dns/>DNS Overview</a></label></li><li><input type=checkbox id=section-bbcb027a658dc7a2333d59078ee507f9 class=toggle>
<label for=section-bbcb027a658dc7a2333d59078ee507f9 class="flex justify-between"><a href=/tech-book/docs/networking-tips/ecmp/>ECMP Load Balancing</a></label></li><li><input type=checkbox id=section-3b39b86f18b3451e5b8a1b81c369549c class=toggle>
<label for=section-3b39b86f18b3451e5b8a1b81c369549c class="flex justify-between"><a href=/tech-book/docs/networking-tips/ip-fragmentation/>IP Fragmentation - IPv4 & IPv6</a></label></li><li><input type=checkbox id=section-c6d06c54cc91b0bc3f948d33b437fa8b class=toggle>
<label for=section-c6d06c54cc91b0bc3f948d33b437fa8b class="flex justify-between"><a href=/tech-book/docs/networking-tips/ip-tos-dscp/>IP Precedence And TOS | DSCP</a></label></li><li><input type=checkbox id=section-5f3260c76dd37177e3f89057bfe520ae class=toggle>
<label for=section-5f3260c76dd37177e3f89057bfe520ae class="flex justify-between"><a href=/tech-book/docs/networking-tips/traceroute/>Linux traceroute tool</a></label></li><li><input type=checkbox id=section-a26cc3fafb36cbc55527adf39ec83849 class=toggle>
<label for=section-a26cc3fafb36cbc55527adf39ec83849 class="flex justify-between"><a href=/tech-book/docs/networking-tips/mlag/>Multi Chassis Link Aggregation Basics</a></label></li><li><input type=checkbox id=section-36409a0abcb2d4d4baf2e0b682d1a5dd class=toggle>
<label for=section-36409a0abcb2d4d4baf2e0b682d1a5dd class="flex justify-between"><a href=/tech-book/docs/networking-tips/qos/>QoS</a></label></li><li><input type=checkbox id=section-db41e3547d316b01acf9a0ce9c04ef34 class=toggle>
<label for=section-db41e3547d316b01acf9a0ce9c04ef34 class="flex justify-between"><a href=/tech-book/docs/networking-tips/spine-leaf-arch/>Spine-leaf Architecture Basics</a></label></li><li><input type=checkbox id=section-0eb0d409074a76b8cb02624655e2434d class=toggle>
<label for=section-0eb0d409074a76b8cb02624655e2434d class="flex justify-between"><a href=/tech-book/docs/networking-tips/tcp-congestion/>TCP Congestion Control</a></label></li><li><input type=checkbox id=section-3d1710947b3c5be1a1cc33d88fcc6f54 class=toggle>
<label for=section-3d1710947b3c5be1a1cc33d88fcc6f54 class="flex justify-between"><a href=/tech-book/docs/networking-tips/tcp-data-transfer/>TCP Data Transfer</a></label></li></ul></li><li><input type=checkbox id=section-f0a392f2f083f28a4991336773716a63 class=toggle>
<label for=section-f0a392f2f083f28a4991336773716a63 class="flex justify-between"><a href=/tech-book/docs/optical-knowledge/>Optical Knowledge</a></label><ul><li><input type=checkbox id=section-18261b95a92d4fa86116243edca4e9fb class=toggle>
<label for=section-18261b95a92d4fa86116243edca4e9fb class="flex justify-between"><a href=/tech-book/docs/optical-knowledge/optical-breakout/>Optical Transceiver(Grey) & Breakout Model</a></label></li></ul></li><li><input type=checkbox id=section-a55840d746138b3d1fedb81acbccdded class=toggle>
<label for=section-a55840d746138b3d1fedb81acbccdded class="flex justify-between"><a href=/tech-book/docs/programming-tips/>Programming Tips</a></label><ul><li><input type=checkbox id=section-f929f1fe13cb5d6cc96ca3e98f9d9777 class=toggle>
<label for=section-f929f1fe13cb5d6cc96ca3e98f9d9777 class="flex justify-between"><a href=/tech-book/docs/programming-tips/c++/>C++ Tips</a></label></li></ul></li><li><input type=checkbox id=section-b35dbf5ebcd4c19926cf5a9aab6c7655 class=toggle>
<label for=section-b35dbf5ebcd4c19926cf5a9aab6c7655 class="flex justify-between"><a href=/tech-book/docs/systemdesign-tips/>SystemDesign-Tips</a></label><ul><li><input type=checkbox id=section-4a618bc3b0b30107f0cec6d3bd6c025f class=toggle>
<label for=section-4a618bc3b0b30107f0cec6d3bd6c025f class="flex justify-between"><a href=/tech-book/docs/systemdesign-tips/code-deployment-system/>Design A Code-Deployment System</a></label></li><li><input type=checkbox id=section-e600754306aa95ce9c5a72b5efec6d7a class=toggle>
<label for=section-e600754306aa95ce9c5a72b5efec6d7a class="flex justify-between"><a href=/tech-book/docs/systemdesign-tips/stock-broker/>Design A Stock-Broker System</a></label></li><li><input type=checkbox id=section-bfa7a29878f65cfbb179e491c1211fa8 class=toggle>
<label for=section-bfa7a29878f65cfbb179e491c1211fa8 class="flex justify-between"><a href=/tech-book/docs/systemdesign-tips/design-amazon/>Design Amazon</a></label></li><li><input type=checkbox id=section-5456837e25872f6352d861a9b5662cb1 class=toggle>
<label for=section-5456837e25872f6352d861a9b5662cb1 class="flex justify-between"><a href=/tech-book/docs/systemdesign-tips/design-slack/>Design Slack</a></label></li><li><input type=checkbox id=section-5a78d16f54536a400b654f17f917bce1 class=toggle>
<label for=section-5a78d16f54536a400b654f17f917bce1 class="flex justify-between"><a href=/tech-book/docs/systemdesign-tips/google-drive/>Google Drive - Design</a></label></li></ul></li></ul><ul><li><a href=/tech-book/posts/>Blog</a></li><li><a href=https://prasenjitmanna.com/ target=_blank rel=noopener>Prasenjit's Blog</a></li><li><a href=https://prasenjitmanna.com/tech-book/ target=_blank rel=noopener>Prasenjit - Tech Book</a></li><li><a href=https://prasenjitmanna.com/upskills/ target=_blank rel=noopener>Prasenjit - Upskills</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/tech-book/svg/menu.svg class=book-icon alt=Menu>
</label><strong>Parallelism Strategies in Deep Learning</strong>
<label for=toc-control><img src=/tech-book/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><ul><li><ul><li><a href=#introduction>Introduction</a></li><li><a href=#data-parallelism>Data Parallelism</a></li><li><a href=#model-parallelism-with-pipeline-parallelism>Model Parallelism with Pipeline Parallelism</a></li><li><a href=#tensor-parallelism>Tensor Parallelism</a></li><li><a href=#self-attention-layer>Self-Attention Layer</a><ul><li><a href=#feedforward-layer>Feedforward Layer</a></li><li><a href=#backpropagation>Backpropagation</a><ul><li><a href=#forward-pass>Forward pass</a></li><li><a href=#backward-pass>Backward pass</a></li></ul></li></ul></li></ul></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h3 id=introduction>Introduction
<a class=anchor href=#introduction>#</a></h3><ol><li>Data Parallelism</li><li>Model Parallelism</li><li>Pipeline Parallelism</li><li>Tensor Parallelism</li></ol><p>Figure 8-1 depicts some of the model parameters that need to be stored in GPU memory: a) Weight matrices associated with connections to the preceding layer, b) Weighted sum (z), c) Activation values (y), d) Errors (E), e) Local gradients (local ∇), f) Gradients received from peer GPUs (remote ∇), g) Learning rates (LR), and h) Weight adjustment values (Δw).</p><p>In addition, the training and test datasets, along with the model code, must also be stored in GPU memory. However, a single GPU may not have enough memory to accommodate all these elements. To address this limitation, an appropriate parallelization strategy must be chosen to efficiently distribute computations across multiple GPUs.</p><p>This chapter introduces the most common strategies include data parallelism, model parallelism, pipeline parallelism, and tensor parallelism.</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/parallel/image1-1.png alt=img|320x271>
<strong>Figure 8-1:</strong> <em>Overview of Neural Networks Parameters.</em></p><h3 id=data-parallelism>Data Parallelism
<a class=anchor href=#data-parallelism>#</a></h3><p>In data parallelization, each GPU has an identical copy of the complete model but processes different mini-batches of data. Gradients from all GPUs are averaged and synchronized before updating the model. This approach is effective when the model fits within a single GPU’s memory.</p><p>In Figure 8-2, the batch of training data is split into eight micro-batches. The first four micro-batches are processed by GPU A1, while the remaining four micro-batches are processed by GPU A2. Both GPUs share the same model, and their input data are processed through all layers to generate a model prediction. The computation during the forward pass does not involve network load traffic. After computing the model error, the backpropagation algorithm starts the backward pass. The first step involves calculating the derivative of the model error, which is synchronized across the GPUs. Next, the error is propagated backward to calculate neuron-based errors, which are then used to compute gradients for each weight parameter. These gradients are synchronized across the GPUs.</p><p>The backpropagation algorithm running on GPUs then sums the gradients and divides the result by the number of GPUs. This averaged result is also synchronized across GPUs. In this way, during the backward pass, both local gradients and averaged gradients are synchronized.</p><p>In our simple two-GPU example, this process does not generate excessive network traffic, although the GPUs can use 100% of their NICs (network interface cards) forwarding capacity. However, if hundreds or even thousands of GPUs are used, the network traffic becomes significantly larger.</p><p>Inter-GPU network communication within a single server (using PCIe, NVLink) or between multiple servers (over InfiniBand, Ethernet, wireless) requires packet forwarding with minimal latency and in a lossless manner. Minimal latency is required to keep the training time as short as possible, while lossless transport is essential because training will pause if even a single packet is lost during synchronization. In the worst-case scenario, if no snapshot of the training progress is taken, the entire training process must be restarted from the beginning. Training a Large Language Model can take months or more.</p><p>Now, consider the electricity costs if training had already been running for two months and had to be restarted due to a single packet loss.</p><p>Power Consumption Example:</p><ul><li>A single GPU consumes roughly 350W under full load.</li><li>Total power consumption for 50,000 GPUs: 350 W×50,000=17,500,000 W=17.5 MW</li><li>For two months (60 days = 1,440 hours) of training: 17.5 MW×1,440 hours=25,200 MWh=25,200,000</li><li>If electricity costs $0.10 per kWh, the total training cost will be: 25,200,000 kWh×0.10=2,520,000 USD</li></ul><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/parallel/image1-2.png alt=img|320x271>
<strong>Figure 8-2:</strong> <em>Data Parallelism Overview.</em></p><h3 id=model-parallelism-with-pipeline-parallelism>Model Parallelism with Pipeline Parallelism
<a class=anchor href=#model-parallelism-with-pipeline-parallelism>#</a></h3><p>In <strong>Model Parallelism</strong>, the neural network is partitioned across multiple GPUs, with each GPU responsible for specific layers of the model. This strategy is particularly beneficial for large-scale models that surpass the memory limitations of a single GPU.</p><p>Conversely, <strong>Pipeline Parallelism</strong> involves dividing the model into consecutive stages, assigning each stage to a different GPU. This setup allows data to be processed in a pipeline fashion, akin to an assembly line, enabling simultaneous processing of multiple training samples. Without pipeline parallelism, each GPU would process its inputs sequentially from the complete dataset, while all other GPUs remain idle.</p><p>Our example neural network in Figure 8-3 consists of three hidden layers and an output layer. The first hidden layer is assigned to <strong>GPU A1</strong>, while the second and third hidden layers are assigned to <strong>GPU A2</strong> and <strong>GPU B1</strong>, respectively. The output layer is placed on <strong>GPU B2</strong>. The training dataset is divided into four micro-batches and stored on the GPUs. These micro-batches are fed sequentially into the first hidden layer on GPU A1. </p><p><strong><em>Note 8-1.</em></strong> <em>In this example, we use a small training dataset. However, if the dataset is too large to fit on a single GPU, we combine model parallelism, pipeline parallelism, and data parallelism to distribute the workload efficiently. See the note 8-2 for more detail.</em></p><p>I have divided the forward pass and backward pass into time steps, which are further split into computation and communication phases.</p><p>During the forward pass, neurons first calculate the weighted sum of inputs, apply the activation function, and produce an output y (computation phase). The computed outputs y, stored in GPU memory, are then transferred to peer GPU(s) using Remote Direct Memory Access (RDMA) (communication phase).</p><p>During the backward pass, the backpropagation algorithm computes the model error (computation phase) and propagates it backward across GPUs using RDMA (communication phase). This process was explained in detail in Chapter 2. </p><p><strong><em>Note 8-2:</em></strong> <em>In our example, Hidden Layer 1 fits entirely on GPU A1. The same applies to other layers—they each fit within a single GPU. However, if the input dataset is too large to fit into a single GPU, it must be split across multiple GPUs. In that case, Hidden Layer 1 will be distributed across multiple GPUs, with each GPU handling a different portion of the dataset. When this happens, the gradients of Hidden Layer 1 must be synchronized across all GPUs that store part of the layer.</em></p><p><em>Time step 1:</em></p><p>     <strong>Computing:</strong></p><p>·    A1 processes the input x1 resulting the output y1.</p><p><strong>Communication:</strong></p><p>·    A1 transports y1 to A2. </p><p><strong>Active GPUs (25%):</strong> A1</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/parallel/image2-1.png alt=img|320x271></p><p><strong>Figure 8-3:</strong> <em>Model Parallelism with Pipeline Parallelism – Time Step 1.</em></p><p><em>Time step 2:</em> </p><p><strong>Computing:</strong></p><ul><li>A1 processes the input x2 and produces the output y2.</li><li>A2 processes the input y1 and produces the output y1.</li></ul><p><strong>Communication:</strong></p><ul><li>A1 transports y2 to A2.</li><li>A2 transports y1 to B3. </li></ul><p><strong>Active GPUs (50%):</strong> A1, A2</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/parallel/image2-2.png alt=img|320x271></p><p><strong>Figure 8-4:</strong> <em>Model Parallelism with Pipeline Parallelism – Time Step 2.</em></p><p><em>Time step 3:</em></p><p><strong>Computing:</strong></p><ul><li>A1 processes the input x3 and produces the output y3.</li><li>A2 processes the input y2 and produces the output y2.</li><li>B1 processes the input y1 and produces the output y1.</li></ul><p><strong>Communication:</strong></p><ul><li>A1 transports y3 to A2.</li><li>A2 transports y2 to B1.</li><li>B1 transports y1 to B2</li></ul><p> <strong>Active GPUs (75%):</strong> A1, A2, B1</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/parallel/image2-3.png alt=img|320x271></p><p><strong>Figure 8-5:</strong> <em>Model Parallelism with Pipeline Parallelism – Time Step 3.</em> </p><p><em>Time step 4:</em> </p><p><strong>Computing:</strong></p><p>·    A1 processes the input x4 and produces the output y4.
·    A2 processes the input y3 and produces the output y3.
·    B1 processes the input y2 and produces the output y2.
·    B2 processes the input y1 and produces the model output 1.</p><p><strong>Communication:</strong></p><p>·    A1 transports y3 to A2.
·    A2 transports y2 to B1.
·    B1 transports y1 to B2 </p><p><strong>Active GPUs (100%):</strong> A1, A2, B1, B2</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/parallel/image2-4.png alt=img|320x271>
<strong>Figure 8-6:</strong> <em>Model Parallelism with Pipeline Parallelism – Time Step 4.</em> </p><p><em>Time step 5:</em></p><p><strong>Computing:</strong></p><p>·    A2 processes the input y4 and produces the output y4.</p><p>·    B1 processes the input y3 and produces the output y3.</p><p>·    B2 processes the input y2 and produces the model output 2.</p><p>·    B2 Computes local neuron error E1, and gradient G1.</p><p><strong>Communication:</strong></p><p>·    A2 transports y4 to B1.
·    B1 transports y3 to B2.
·    B2 transports error E1 to B1</p><p><strong>Active GPUs (75%):</strong> A2, B1, B2</p><p> The notation x3 above G1 on GPU B2 indicates that the algorithm computes gradients from the error for each weight associated with the inputs, including the bias. This process is repeated with all four micro-batches. This notation will be used in the upcoming figures as well.</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/parallel/image2-5.png alt=img|320x271></p><p><strong>Figure 8-7:</strong> <em>Model Parallelism with Pipeline Parallelism – Time Step 5.</em> </p><p><em>Time step 6:</em> </p><p><strong>Computing:</strong></p><p>·    B1 processes the input y4 and produces the output y4.</p><p>·    B2 processes the input y3 and produces model output 3.</p><p>·    B2 Computes local neuron error E2, and gradient G2.</p><p>·    B1 Computes local neuron error E1, and gradient G1.</p><p><strong>Communication:</strong></p><p>·    B1 transports y4 to B2.</p><p>·    B2 transports error E2 to B1</p><p> <strong>Active GPUs (50%):</strong> B1, B2</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/parallel/image2-6.png alt=img|320x271></p><p> <strong>Figure 8-8:</strong> <em>Model Parallelism with Pipeline Parallelism – Time Step 6.</em> </p><p><em>Time step 7:</em> </p><p><strong>Computing:</strong></p><p>·    B2 processes the input y4 and produces model output 4.</p><p>·    B2 Computes local neuron error E3, and gradient G3.</p><p>·    B1 Computes local neuron error E2, and gradient G2.</p><p>·    A2 Computes local neuron error E1, and gradient G1.</p><p><strong>Communication:</strong></p><p>·    B2 transports error E3 to B1</p><p>·    B1 transports error E2 to A2</p><p>·    A2 transports error E1 to A1 </p><p><strong>Active GPUs (75%):</strong> A2, B1, B2</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/parallel/image2-7.png alt=img|320x271></p><p><strong>Figure 8-9:</strong> <em>Model Parallelism with Pipeline Parallelism – Time Step 7.</em> </p><p><em>Time step 8:</em> </p><p><strong>Computing:</strong> </p><p>·    B2 Computes local neuron error E4, and gradient G4.</p><p>·    B1 Computes local neuron error E3, and gradient G3.</p><p>·    A2 Computes local neuron error E2, and gradient G2.</p><p>·    A1 Computes local neuron error E1, and gradient G1.</p><p><strong>Communication:</strong></p><p>·    B2 transports error E4 to B1</p><p>·    B1 transports error E3 to A2</p><p>·    A2 transports error E2 to A1 </p><p><strong>Active GPUs (100%):</strong> A1, A2, B1, B2</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/parallel/image2-8.png alt=img|320x271></p><p><strong>Figure 8-10:</strong> <em>Model Parallelism with Pipeline Parallelism – Time Step 8.</em> </p><p><em>Time step 9:</em> </p><p><strong>Computing:</strong> </p><p>·    B1 Computes local neuron error E4, and gradient G4.</p><p>·    A2 Computes local neuron error E3, and gradient G3.</p><p>·    A1 Computes local neuron error E2, and gradient G2.</p><p><strong>Communication:</strong> </p><p>·    B1 transports error E4 to A2</p><p>·    A2 transports error E3 to A1 </p><p><strong>Active GPUs (75%):</strong> A1, A2, B1</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/parallel/image2-9.png alt=img|320x271></p><p><strong>Figure 8-11:</strong> <em>Model Parallelism with Pipeline Parallelism – Time Step 9.</em> </p><p><em>Time step 10:</em></p><p><strong>Computing:</strong> </p><p>·    A2 Computes local neuron error E4, and gradient G4.</p><p>·    A1 Computes local neuron error E3, and gradient G3.</p><p><strong>Communication:</strong> </p><p>·    A2 transports error E4 to A1 </p><p><strong>Active GPUs (50%):</strong> A1, A2</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/parallel/image2-10.png alt=img|320x271></p><p><strong>Figure 8-12:</strong> <em>Model Parallelism with Pipeline Parallelism – Time Step 10.</em> </p><p><em>Time step 11:</em> </p><p><strong>Computing:</strong> </p><p>·    A1 Computes local neuron error E4, and gradient G4.</p><p><strong>Communication:</strong> </p><p><strong>Active GPUs (25%):</strong> A1</p><p>In our example, the micro-batches fit into a single GPU, so we don’t need to split them across multiple GPUs. That said, once GPU A1 has computed the gradients for the last micro-batch, the weights are adjusted, and the second iteration of the forward pass begins.</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/parallel/image2-11.png alt=img|320x271></p><p><strong>Figure 8-13:</strong> <em>Model Parallelism with Pipeline Parallelism – Time Step 11.</em> </p><p>If the test dataset is too large for a single GPU and must be split across multiple GPUs, the layers must also be shared between GPUs. For example, hidden layer 1 is on GPUs A1 and C1, while hidden layer 2 is on GPUs A2 and C2. This requires intra-layer gradient synchronization between GPUs sharing the same layer, resulting in inter-GPU packet transport. Figure 8-14 illustrates how gradients are first synchronized (inter-layer). Then, each GPU averages the gradients (sum of gradients divided by the number of GPUs). Finally, the averaged gradients are synchronized.</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/parallel/image2-12.png alt=img|320x271></p><p><strong>Figure 8-14:</strong> <em>Model Parallelism with Pipeline Parallelism – Synchronization.</em></p><h3 id=tensor-parallelism>Tensor Parallelism
<a class=anchor href=#tensor-parallelism>#</a></h3><p> The previous section described how Pipeline Parallelism distributes entire layers across multiple GPUs. However, Large Language Models (LLMs) based on transformer architectures contain billions of parameters, making this approach insufficient.</p><p>For example, GPT-3 has approximately 605 million parameters in a single self-attention layer and about 1.2 billion parameters in a feedforward layer, and these figures apply to just one transformer block. Since GPT-3 has 96 transformer blocks, the total parameter count reaches approximately 173 billion. When adding embedding and normalization parameters, the total increases to roughly 175 billion parameters.</p><p>The number of parameters in a single layer alone often exceeds the memory capacity of a single GPU, making Pipeline Parallelism insufficient. Additionally, performing large matrix multiplications on a single GPU would be extremely slow and inefficient. Tensor Parallelism addresses this challenge by splitting computations within individual layers across multiple GPUs rather than assigning whole layers to separate GPUs, as done in Pipeline Parallelism.</p><p>Chapter 7 introduces Transformer architecture but for memory refreshing, figure 8-15 illustrates a stack of decoder modules in a transformer architecture. Each decoder module consists of a Self-Attention layer and a Feedforward layer. The figure also shows how an input word, represented by x1, is first mapped to a token. The token, in turn, receives a positional word embedding vector through lookups in the word embedding and position embedding tables.</p><p>The resulting word vector is used to compute Query (Q) and Key (K) matrices, which, in turn, produces logits via dot products. These logits are then passed through the SoftMax function. The resulting matrix from the SoftMax function is multiplied with the Value (V) matrices. After Add & Normalization computation, the resulting matrix is fed into the Feedforward, fully connected, neural network.</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/parallel/image3-1.png alt=img|320x271>
<strong>Figure 8-15:</strong> <em>An Overview of a Transformer Architecture.</em></p><h3 id=self-attention-layer>Self-Attention Layer
<a class=anchor href=#self-attention-layer>#</a></h3><p>In most cases, the word embedding matrix fits within a single GPU and is not split across multiple GPUs when using Tensor Parallelism. This is because a typical embedding matrix is approximately 200 MB, which is significantly smaller than large Transformer layers that can contain billions of parameters.</p><p>Another reason for keeping the embedding matrix on a single GPU is efficient lookup operations. Unlike large matrix multiplications, embedding lookups are memory-efficient and do not impose significant computational overhead. Splitting the embedding matrix across multiple GPUs would introduce high communication costs, as each GPU would store only a fraction of the vocabulary. This would require frequent cross-GPU communication for token lookups, increasing latency and reducing efficiency. After the embedding lookup, the embedding vectors are broadcasted to all GPUs before the Transformer computations start. </p><p>However, in very large-scale models (such as GPT-3 with 175 billion parameters), embeddings may be sharded across multiple GPUs using distributed embeddings or model parallelism techniques. One approach is row-wise parallelism, where the vocabulary is split across GPUs, and each GPU stores only a fraction of the embeddings, handling lookups for the tokens it owns. </p><p>Figure 8-16 illustrates how the positional word embedding matrix (Eepv) is multiplied with the Query (Q), Key (K), and Value (V) matrices to produce the corresponding Q, K, and V vectors. The Query and Key vectors are then used as inputs to the self-attention layer.</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/parallel/image3-2.png alt=img|320x271>
<strong>Figure 8-16:</strong> <em>Local Query (Q), Key (K), and Value (V) Matrices.</em></p><p>Figure 8-17 illustrates how the Query, Key, and Value matrices are sharded across two GPUs. The first fragments of these matrices are assigned to GPU A1, while the second fragments are assigned to GPU A2. The positional word embedding matrix (Eevp ) is also distributed between GPU A1 and GPU A2. Matrix multiplication is then performed between the corresponding fragment of Eevp  and the respective shards of the Q, K, and V matrices. </p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/parallel/image3-3.png alt=img|320x271>
<strong>Figure 8-17:</strong> <em>Shared Query (Q), Key (K), and Value (V) Matrices.</em></p><p>Figure 8-18 illustrates the cross-GPU communication involved in the forward pass of the Self-Attention layer when using Tensor Parallelism. In this example, both the word embedding, and positional embedding matrices fit within GPU A1. After computing the positional word embeddings for the input words, the resulting vectors are broadcasted to GPU A2.</p><p>Since we are using Tensor Parallelism, the Query (Q), Key (K), and Value (V) matrices are partitioned across GPU A1 and GPU A2. Once each GPU has computed its assigned slices of the Q, K, and V vectors, the Q and K vectors are shared between GPUs using an All-Gather operation. This ensures that each GPU receives the missing parts of the Q and K matrices, reconstructing the complete matrices across GPUs. Only the Q and K matrices are synchronized; the V matrix remains local to each GPU.</p><p>The Q and K matrices are then used in the Self-Attention layer, where the first operation is a matrix multiplication between the Query vectors and Key vectors for all tokens. The process is explained in detail in Chapter 7. The resulting scores are used to compute logits, which are inputs to the SoftMax function, using scaled dot-product attention. The output of the SoftMax function is then multiplied by the local fragment of the V matrix on each GPU.</p><p>The SoftMax operation produces a Context Vector (Cv) for each input word, which serves as the input to the Feedforward Neural Network (FFN) layer. That said, the SoftMax in the self-attention layer is not the final prediction layer, it’s used to compute attention weights. The feedforward network processes the context vectors token representations produced by self-attention, not the predicted token. The final prediction is typically made by a separate output projection followed by a SoftMax over the vocabulary.</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/parallel/image3-4.png alt=img|320x271>
<strong>Figure 8-18:</strong> <em>Tensor Parallelism in Self-Attention Layer.</em></p><h4 id=feedforward-layer>Feedforward Layer
<a class=anchor href=#feedforward-layer>#</a></h4><p>Figure 8-19 illustrates a Feedforward layer in the decoder module of a transformer. The feedforward network consists of two hidden layers and an output layer. In addition to Tensor Parallelism, we also employ Model Parallelism with Pipeline Parallelism.</p><p>The first hidden layer is split between GPU A1 and GPU B1, both located in the same server. The weight matrices for neurons 1–3 reside in GPU A1, while the weight matrices for neurons 4–6 are in GPU B1. The inter-GPU communication between GPU A1 and GPU B1 occurs over NVLinks, which I refer to as the High-speed Domain (HsD).</p><p>The second hidden layer is distributed across GPU A2 and GPU B2 within the same server. GPU A2 holds the weight matrices for neurons 1–2, while GPU B2 contains the weight matrices for neurons 3–4. The inter-GPU connection between GPU A2 and GPU B2 also utilizes NVLinks.</p><p>The output layer is divided between GPU A3 and GPU B3, both residing in the same server. The weight matrix for neuron 1 is stored in GPU A3, while the weight matrix for neuron 2 is in GPU B3. Inter-GPU communication occurs over NVLinks.</p><p>Additionally, GPU A1, GPU A2, and GPU A3 are interconnected via Rail Switch-1 across the Backend Network. Similarly, GPU B1, GPU B2, and GPU B3 are connected via Rail Switch-2 across the Backend Network.</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/parallel/image3-5.png alt=img|320x271>
<strong>Figure 8-19:</strong> <em>Tensor, Model and Pipeline Parallelism in Feedforward Layer.</em></p><h4 id=backpropagation>Backpropagation
<a class=anchor href=#backpropagation>#</a></h4><h5 id=forward-pass>Forward pass
<a class=anchor href=#forward-pass>#</a></h5><p>First Hidden Layer (H1): The input to H1, the output of the Self-Attention block after the Add & Norm step (context vectors), is shared with GPU A1 and GPU B1. Each GPU then performs its local matrix multiplication. After these local computations are complete, the partial outputs are synchronized between GPU A1 and GPU B1 using an All-Gather operation. This synchronization ensures that the complete H1 output (ynA1+B1) is calculated before it is passed to the next stage. Because GPU A1 and GPU B1 reside on the same server, the communication occurs over a high-speed domain via NVLink.</p><p>In the context of pipeline parallelism, H1 constitutes one pipeline stage. Once its context vector-based output is fully assembled, it is sent to the GPUs responsible for the next layer. Specifically, GPU A1 and GPU B1 first pass the output computed from the first context vector (C1), and then the GPUs process the next context vector. This communication occurs over the backend network. GPU A1, GPU A2, and GPU A3 are all connected to the same rail switch, so the RDMA packets traverse only one switch. The same design applies to GPU B1, GPU B2, and GPU B3. If communication between GPUs connected to different rail switches is required, the rail switches must be interconnected via spine switches. Alternatively, the RDMA packets may be sent over the high-speed domain to a GPU on the same rail as the destination GPU.</p><p>Second Hidden Layer (H2): The complete output from H1 (obtained after synchronization in the previous stage) is pipelined to GPUs A2 and B2. Each of these GPUs performs its own local matrix multiplication. As before, after the local computations, the partial outputs from GPU A2 and GPU B2 are synchronized via an All-Gather operation, forming the complete H2 output (ynA2+B2).</p><p>The synchronization and forwarding between hidden layer 2 and output layer, and within an output layer follow the same model as in the previous hidden layers.</p><p>This hybrid approach, using tensor parallelism within each stage and pipeline parallelism across stages, helps balance the computational load and memory usage across the six GPUs while minimizing idle time.</p><p>Although the focus of this section is on tensor parallelism, pipeline parallelism is also discussed because large language models (LLMs) can process multiple sentences from their vocabulary simultaneously during the training process.</p><p>On the other hand, during the inference when answering to our questions, LLMs use autoregressive next-word prediction. In this process, the final SoftMax layer of the Transformer calculates the probabilities over the vocabulary to predict the next token. This predicted token is then converted into a word and mapped to a new token. The lookup process assigns the token a positional embedding vector, which is used to compute the Query, Key, and Value vectors that feed into the Transformer&rsquo;s self-attention layer. Consequently, pipeline parallelism is not required during the inference phase.</p><h5 id=backward-pass>Backward pass
<a class=anchor href=#backward-pass>#</a></h5><p>The error propagates backward from the Feedforward Neural Network (FFNN) layer to the Self-Attention layer. The backpropagation process in a Transformer follows a sequential order, meaning the error from the output propagates first to the FFNN layer, and from there, it continues backward to the Self-Attention mechanism.</p><p>The process begins at the output layer, where the error is computed using the SoftMax function and cross-entropy loss. This error is then backpropagated through the FFNN layer, where gradients for the weight matrices are computed. Since the FFNN weights are split across multiple GPUs in Tensor Parallelism, each GPU computes its local gradient. An All-Reduce operation is then performed to synchronize these gradients across GPUs, ensuring that all GPUs have the correct weight updates before proceeding.</p><p>Once the gradients for the FFNN weights are synchronized, the error propagates back to the Self-Attention layer. Here, gradients for the Query (Q), Key (K), and Value (V) matrices are computed. Since these matrices were split across GPUs during the forward pass, the missing Q and K fragments must be gathered before calculating gradients. An All-Gather operation is used to collect Q and K values across GPUs. Once each GPU has a complete Q and K matrix, it computes the required gradients locally. After the local gradient computation, an All-Reduce operation is performed to ensure all GPUs have the synchronized gradients before updating the weights.</p><p>After both layers complete their gradient computations and synchronizations, the optimizer updates the weights, and the next iteration begins. The key communication phases include All-Gather for assembling required Q and K values before gradient computation and All-Reduce for synchronizing gradients before weight updates.</p><p><strong>References:</strong></p><ul><li><a href=https://nwktimes.blogspot.com/2025/03/parallelism-strategies-in-deep-learning.html>https://nwktimes.blogspot.com/2025/03/parallelism-strategies-in-deep-learning.html</a></li><li><a href=https://nwktimes.blogspot.com/2025/03/model-parallelism-with-pipeline.html>https://nwktimes.blogspot.com/2025/03/model-parallelism-with-pipeline.html</a></li><li><a href=https://nwktimes.blogspot.com/2025/03/tensor-parallelism.html>https://nwktimes.blogspot.com/2025/03/tensor-parallelism.html</a></li></ul></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/prmanna/tech-book/commit/ef6176840052f945883707b8a3d88eeaf49ab235 title='Last modified by Prasenjit Manna | May 16, 2025' target=_blank rel=noopener><img src=/tech-book/svg/calendar.svg class=book-icon alt>
<span>May 16, 2025</span></a></div></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><ul><li><ul><li><a href=#introduction>Introduction</a></li><li><a href=#data-parallelism>Data Parallelism</a></li><li><a href=#model-parallelism-with-pipeline-parallelism>Model Parallelism with Pipeline Parallelism</a></li><li><a href=#tensor-parallelism>Tensor Parallelism</a></li><li><a href=#self-attention-layer>Self-Attention Layer</a><ul><li><a href=#feedforward-layer>Feedforward Layer</a></li><li><a href=#backpropagation>Backpropagation</a><ul><li><a href=#forward-pass>Forward pass</a></li><li><a href=#backward-pass>Backward pass</a></li></ul></li></ul></li></ul></li></ul></li></ul></nav></div></aside></main></body></html>