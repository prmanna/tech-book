<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  Content
  #


Introduction 
Artificial Neuron  

Weighted Sum for Pre-Activation Value  
ReLU Activation Function for Post-Activation  
Bias Term 
S-Shaped Functions – TANH and SIGMOID


Network Impact
Summary


  Introduction
  #

Artificial Intelligence (AI) is a broad term for solutions that aim to mimic the functions of the human brain. Machine Learning (ML), in turn, is a subset of AI, suitable for tasks like simple pattern recognition and prediction. Deep Learning (DL), the focus of this section, is a subset of ML that leverages algorithms to extract meaningful patterns from data. Unlike ML, DL does not necessarily require human intervention, such as providing structured, labeled datasets (e.g., 1,000 bird images labeled as “bird” and 1,000 cat images labeled as “cat”). "><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://prasenjitmanna.com/tech-book/docs/ai-ml-dc/2-ai-deep-learning-basics/"><meta property="og:site_name" content="Technical Book"><meta property="og:title" content="Deep Learning Basics | Artificial Neuron"><meta property="og:description" content=" Content # Introduction Artificial Neuron Weighted Sum for Pre-Activation Value ReLU Activation Function for Post-Activation Bias Term S-Shaped Functions – TANH and SIGMOID Network Impact Summary Introduction # Artificial Intelligence (AI) is a broad term for solutions that aim to mimic the functions of the human brain. Machine Learning (ML), in turn, is a subset of AI, suitable for tasks like simple pattern recognition and prediction. Deep Learning (DL), the focus of this section, is a subset of ML that leverages algorithms to extract meaningful patterns from data. Unlike ML, DL does not necessarily require human intervention, such as providing structured, labeled datasets (e.g., 1,000 bird images labeled as “bird” and 1,000 cat images labeled as “cat”). "><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:modified_time" content="2025-05-05T17:43:45+05:30"><title>Deep Learning Basics | Artificial Neuron | Technical Book</title>
<link rel=manifest href=/tech-book/manifest.json><link rel=icon href=/tech-book/favicon.png><link rel=canonical href=https://prasenjitmanna.com/tech-book/docs/ai-ml-dc/2-ai-deep-learning-basics/><link rel=stylesheet href=/tech-book/book.min.a61cdb2979f3c2bece54ef69131fba427dd57d55c232d3bb5fdb62ac41aa8354.css integrity="sha256-phzbKXnzwr7OVO9pEx+6Qn3VfVXCMtO7X9tirEGqg1Q=" crossorigin=anonymous><script defer src=/tech-book/fuse.min.js></script><script defer src=/tech-book/en.search.min.d6fefc4754185a3aa4ecc98e3e066e5fab9fd6381dc023666013a968d238cbfd.js integrity="sha256-1v78R1QYWjqk7MmOPgZuX6uf1jgdwCNmYBOpaNI4y/0=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/tech-book/><img src=/tech-book/logo.png alt=Logo><span>Technical Book</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><input type=checkbox id=section-99f552133860bc21797a47fe73e93434 class=toggle>
<label for=section-99f552133860bc21797a47fe73e93434 class="flex justify-between"><a href=/tech-book/docs/5g/>5G</a></label><ul><li><input type=checkbox id=section-dfc790b73acb0410a0114547cbf5af32 class=toggle>
<label for=section-dfc790b73acb0410a0114547cbf5af32 class="flex justify-between"><a href=/tech-book/docs/5g/5g-intro/>An Overview of 5G Networking</a></label></li></ul></li><li><input type=checkbox id=section-7c4a3b16aeeb5b7194b232c1eef2f4fb class=toggle>
<label for=section-7c4a3b16aeeb5b7194b232c1eef2f4fb class="flex justify-between"><a href=/tech-book/docs/algorithms/>Algorithms</a></label><ul><li><input type=checkbox id=section-8956d82fbe6869140758f7e8679174bc class=toggle>
<label for=section-8956d82fbe6869140758f7e8679174bc class="flex justify-between"><a href=/tech-book/docs/algorithms/breadth-first-search/>Breadth First Search</a></label></li><li><input type=checkbox id=section-5a049cfad1740f3fd30565524385fa57 class=toggle>
<label for=section-5a049cfad1740f3fd30565524385fa57 class="flex justify-between"><a href=/tech-book/docs/algorithms/depth-first-search/>Depth First Search</a></label></li><li><input type=checkbox id=section-ebc049f26d82165be8c6f1f9e504e799 class=toggle>
<label for=section-ebc049f26d82165be8c6f1f9e504e799 class="flex justify-between"><a href=/tech-book/docs/algorithms/easy/>Easy Complexity</a></label></li><li><input type=checkbox id=section-1071946392bd1f431993e950147fa054 class=toggle>
<label for=section-1071946392bd1f431993e950147fa054 class="flex justify-between"><a href=/tech-book/docs/algorithms/priority-queue-and-heap/>Priority Queue and Heap</a></label></li><li><input type=checkbox id=section-08afbeb294c43ca4908c1c89a4be9d0a class=toggle>
<label for=section-08afbeb294c43ca4908c1c89a4be9d0a class="flex justify-between"><a href=/tech-book/docs/algorithms/two-pointers/>Two Pointers & Sliding Window</a></label></li><li><input type=checkbox id=section-ef36c7c4f7e0dec6525068c3c409100c class=toggle>
<label for=section-ef36c7c4f7e0dec6525068c3c409100c class="flex justify-between"><a href=/tech-book/docs/algorithms/medium/>Medium Complexity</a></label></li></ul></li><li><input type=checkbox id=section-8c7c5c4a8382299873178820b1d91be1 class=toggle checked>
<label for=section-8c7c5c4a8382299873178820b1d91be1 class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/>Data Center Networking for AI Clusters</a></label><ul><li><input type=checkbox id=section-433ccede4154db01f6c601940d2949e0 class=toggle>
<label for=section-433ccede4154db01f6c601940d2949e0 class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/1-ai-ml-networking/>AI/ML Networking</a></label></li><li><input type=checkbox id=section-65f71f2455a94ea3d1143666d556b0ed class=toggle checked>
<label for=section-65f71f2455a94ea3d1143666d556b0ed class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/2-ai-deep-learning-basics/ class=active>Deep Learning Basics | Artificial Neuron</a></label></li><li><input type=checkbox id=section-f11d3aa2e337730e1e3345f8530fe134 class=toggle>
<label for=section-f11d3aa2e337730e1e3345f8530fe134 class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/3-challenges-in-ai-fabric/>Challenges in AI Fabric Design</a></label></li><li><input type=checkbox id=section-e28420ec7507646128f3695b6f9badbb class=toggle>
<label for=section-e28420ec7507646128f3695b6f9badbb class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/4-congestion-avoidance-in-ai-fabric/>Congestion Avoidance in AI Fabric</a></label></li><li><input type=checkbox id=section-67bb7cbb1dd95e59f8515201219c090f class=toggle>
<label for=section-67bb7cbb1dd95e59f8515201219c090f class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/5-load-balancing-in-ai-fabric/>Load Balancing in AI Fabric</a></label></li><li><input type=checkbox id=section-adb8e29f405ea627a0e41d43e94e3453 class=toggle>
<label for=section-adb8e29f405ea627a0e41d43e94e3453 class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/7-rail-desings-in-gpu-fabric/>Load Balancing in AI Fabric</a></label></li><li><input type=checkbox id=section-5f3e07f6cf7921e5291ff7894e06b19b class=toggle>
<label for=section-5f3e07f6cf7921e5291ff7894e06b19b class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/6-backend-network-topologies-for-ai-fabric/>Backend Network Topologies for AI Fabrics</a></label></li></ul></li><li><input type=checkbox id=section-3272b2d28b2b247027bf478619ca416f class=toggle>
<label for=section-3272b2d28b2b247027bf478619ca416f class="flex justify-between"><a href=/tech-book/docs/data-center/>Data Center Tips</a></label><ul><li><input type=checkbox id=section-984f932c7aba4f0a1841e8413165c947 class=toggle>
<label for=section-984f932c7aba4f0a1841e8413165c947 class="flex justify-between"><a href=/tech-book/docs/data-center/data-center-ethernet/>Data Center Ethernet</a></label></li><li><input type=checkbox id=section-bc5ac88940153a700e63e3be886c63cc class=toggle>
<label for=section-bc5ac88940153a700e63e3be886c63cc class="flex justify-between"><a href=/tech-book/docs/data-center/data-center-technologies/>Data Center Technologies</a></label></li><li><input type=checkbox id=section-86fde1ddf43700ef8191e11c59a82cf1 class=toggle>
<label for=section-86fde1ddf43700ef8191e11c59a82cf1 class="flex justify-between"><a href=/tech-book/docs/data-center/data-center-network-virtualization/>Network Virtualization in Cloud Data Centers</a></label></li></ul></li><li><input type=checkbox id=section-ddf784688e0c6d5abb681d2c57851559 class=toggle>
<label for=section-ddf784688e0c6d5abb681d2c57851559 class="flex justify-between"><a href=/tech-book/docs/manageability/>Manageability</a></label><ul><li><input type=checkbox id=section-33ac730897af6feca81c1fdb7869c57e class=toggle>
<label for=section-33ac730897af6feca81c1fdb7869c57e class="flex justify-between"><a href=/tech-book/docs/manageability/why-grpc-on-http2/>gRPC on HTTP/2</a></label></li></ul></li><li><input type=checkbox id=section-c14ae944424668ef125e10cd791a3d3d class=toggle>
<label for=section-c14ae944424668ef125e10cd791a3d3d class="flex justify-between"><a href=/tech-book/docs/networking-tips/>Networking Tips</a></label><ul><li><input type=checkbox id=section-78fdf21c03c55935d3146441b06faf3e class=toggle>
<label for=section-78fdf21c03c55935d3146441b06faf3e class="flex justify-between"><a href=/tech-book/docs/networking-tips/dns/>DNS Overview</a></label></li><li><input type=checkbox id=section-bbcb027a658dc7a2333d59078ee507f9 class=toggle>
<label for=section-bbcb027a658dc7a2333d59078ee507f9 class="flex justify-between"><a href=/tech-book/docs/networking-tips/ecmp/>ECMP Load Balancing</a></label></li><li><input type=checkbox id=section-3b39b86f18b3451e5b8a1b81c369549c class=toggle>
<label for=section-3b39b86f18b3451e5b8a1b81c369549c class="flex justify-between"><a href=/tech-book/docs/networking-tips/ip-fragmentation/>IP Fragmentation - IPv4 & IPv6</a></label></li><li><input type=checkbox id=section-c6d06c54cc91b0bc3f948d33b437fa8b class=toggle>
<label for=section-c6d06c54cc91b0bc3f948d33b437fa8b class="flex justify-between"><a href=/tech-book/docs/networking-tips/ip-tos-dscp/>IP Precedence And TOS | DSCP</a></label></li><li><input type=checkbox id=section-5f3260c76dd37177e3f89057bfe520ae class=toggle>
<label for=section-5f3260c76dd37177e3f89057bfe520ae class="flex justify-between"><a href=/tech-book/docs/networking-tips/traceroute/>Linux traceroute tool</a></label></li><li><input type=checkbox id=section-a26cc3fafb36cbc55527adf39ec83849 class=toggle>
<label for=section-a26cc3fafb36cbc55527adf39ec83849 class="flex justify-between"><a href=/tech-book/docs/networking-tips/mlag/>Multi Chassis Link Aggregation Basics</a></label></li><li><input type=checkbox id=section-36409a0abcb2d4d4baf2e0b682d1a5dd class=toggle>
<label for=section-36409a0abcb2d4d4baf2e0b682d1a5dd class="flex justify-between"><a href=/tech-book/docs/networking-tips/qos/>QoS</a></label></li><li><input type=checkbox id=section-db41e3547d316b01acf9a0ce9c04ef34 class=toggle>
<label for=section-db41e3547d316b01acf9a0ce9c04ef34 class="flex justify-between"><a href=/tech-book/docs/networking-tips/spine-leaf-arch/>Spine-leaf Architecture Basics</a></label></li><li><input type=checkbox id=section-0eb0d409074a76b8cb02624655e2434d class=toggle>
<label for=section-0eb0d409074a76b8cb02624655e2434d class="flex justify-between"><a href=/tech-book/docs/networking-tips/tcp-congestion/>TCP Congestion Control</a></label></li><li><input type=checkbox id=section-3d1710947b3c5be1a1cc33d88fcc6f54 class=toggle>
<label for=section-3d1710947b3c5be1a1cc33d88fcc6f54 class="flex justify-between"><a href=/tech-book/docs/networking-tips/tcp-data-transfer/>TCP Data Transfer</a></label></li></ul></li><li><input type=checkbox id=section-f0a392f2f083f28a4991336773716a63 class=toggle>
<label for=section-f0a392f2f083f28a4991336773716a63 class="flex justify-between"><a href=/tech-book/docs/optical-knowledge/>Optical Knowledge</a></label><ul><li><input type=checkbox id=section-18261b95a92d4fa86116243edca4e9fb class=toggle>
<label for=section-18261b95a92d4fa86116243edca4e9fb class="flex justify-between"><a href=/tech-book/docs/optical-knowledge/optical-breakout/>Optical Transceiver(Grey) & Breakout Model</a></label></li></ul></li><li><input type=checkbox id=section-a55840d746138b3d1fedb81acbccdded class=toggle>
<label for=section-a55840d746138b3d1fedb81acbccdded class="flex justify-between"><a href=/tech-book/docs/programming-tips/>Programming Tips</a></label><ul><li><input type=checkbox id=section-f929f1fe13cb5d6cc96ca3e98f9d9777 class=toggle>
<label for=section-f929f1fe13cb5d6cc96ca3e98f9d9777 class="flex justify-between"><a href=/tech-book/docs/programming-tips/c++/>C++ Tips</a></label></li></ul></li><li><input type=checkbox id=section-b35dbf5ebcd4c19926cf5a9aab6c7655 class=toggle>
<label for=section-b35dbf5ebcd4c19926cf5a9aab6c7655 class="flex justify-between"><a href=/tech-book/docs/systemdesign-tips/>SystemDesign-Tips</a></label><ul><li><input type=checkbox id=section-4a618bc3b0b30107f0cec6d3bd6c025f class=toggle>
<label for=section-4a618bc3b0b30107f0cec6d3bd6c025f class="flex justify-between"><a href=/tech-book/docs/systemdesign-tips/code-deployment-system/>Design A Code-Deployment System</a></label></li><li><input type=checkbox id=section-e600754306aa95ce9c5a72b5efec6d7a class=toggle>
<label for=section-e600754306aa95ce9c5a72b5efec6d7a class="flex justify-between"><a href=/tech-book/docs/systemdesign-tips/stock-broker/>Design A Stock-Broker System</a></label></li><li><input type=checkbox id=section-bfa7a29878f65cfbb179e491c1211fa8 class=toggle>
<label for=section-bfa7a29878f65cfbb179e491c1211fa8 class="flex justify-between"><a href=/tech-book/docs/systemdesign-tips/design-amazon/>Design Amazon</a></label></li><li><input type=checkbox id=section-5456837e25872f6352d861a9b5662cb1 class=toggle>
<label for=section-5456837e25872f6352d861a9b5662cb1 class="flex justify-between"><a href=/tech-book/docs/systemdesign-tips/design-slack/>Design Slack</a></label></li><li><input type=checkbox id=section-5a78d16f54536a400b654f17f917bce1 class=toggle>
<label for=section-5a78d16f54536a400b654f17f917bce1 class="flex justify-between"><a href=/tech-book/docs/systemdesign-tips/google-drive/>Google Drive - Design</a></label></li></ul></li></ul><ul><li><a href=/tech-book/posts/>Blog</a></li><li><a href=https://prasenjitmanna.com/ target=_blank rel=noopener>Prasenjit's Blog</a></li><li><a href=https://prasenjitmanna.com/tech-book/ target=_blank rel=noopener>Prasenjit - Tech Book</a></li><li><a href=https://prasenjitmanna.com/upskills/ target=_blank rel=noopener>Prasenjit - Upskills</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/tech-book/svg/menu.svg class=book-icon alt=Menu>
</label><strong>Deep Learning Basics | Artificial Neuron</strong>
<label for=toc-control><img src=/tech-book/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><ul><li><ul><li><ul><li><a href=#content>Content</a></li></ul></li></ul></li><li><a href=#introduction>Introduction</a></li><li><a href=#artificial-neuron>Artificial Neuron</a><ul><li><a href=#weighted-sum-for-pre-activation-value>Weighted Sum for Pre-Activation Value</a></li><li><a href=#relu-activation-function-for-post-activation>ReLU Activation Function for Post-Activation</a></li><li><a href=#bias-term>Bias Term</a></li><li><a href=#s-shaped-functions--tanh-and-sigmoid>S-Shaped Functions – TANH and SIGMOID</a></li></ul></li><li><a href=#backpropagation-algorithm-introduction>Backpropagation Algorithm: Introduction</a><ul><li><a href=#introduction-1>Introduction </a></li><li><a href=#the-first-iteration---forward-pass>The First Iteration - Forward Pass</a></li><li><a href=#neuron-a-forward-pass-calculations>Neuron-a Forward Pass Calculations</a><ul><li><a href=#weighted-sum>Weighted Sum</a></li><li><a href=#activation-function>Activation Function</a></li></ul></li><li><a href=#neuron-b-forward-pass-calculations>Neuron-b Forward Pass Calculations</a><ul><li><a href=#weighted-sum-1>Weighted Sum</a></li><li><a href=#activation-function-1>Activation Function</a></li></ul></li><li><a href=#error-function>Error Function</a></li><li><a href=#backward-pass>Backward Pass</a><ul><li><a href=#partial-derivative-for-error-function--output-error-gradient>Partial Derivative for Error Function – Output Error (Gradient)</a></li><li><a href=#partial-derivative-for-the-activation-function>Partial Derivative for the Activation Function</a></li><li><a href=#error-term-for-neurons-gradient>Error Term for Neurons (Gradient)</a></li><li><a href=#weight-adjustment-value>Weight Adjustment Value</a></li><li><a href=#refine-weights>Refine Weights</a></li></ul></li><li><a href=#the-second-iteration---forward-pass>The Second Iteration - Forward Pass</a></li><li><a href=#network-impact>Network Impact</a></li></ul></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h4 id=content>Content
<a class=anchor href=#content>#</a></h4><ul><li>Introduction </li><li>Artificial Neuron  <ul><li>Weighted Sum for Pre-Activation Value  </li><li>ReLU Activation Function for Post-Activation  </li><li>Bias Term </li><li>S-Shaped Functions – TANH and SIGMOID</li></ul></li><li>Network Impact</li><li>Summary</li></ul><h2 id=introduction>Introduction
<a class=anchor href=#introduction>#</a></h2><p>Artificial Intelligence (AI) is a broad term for solutions that aim to mimic the functions of the human brain. Machine Learning (ML), in turn, is a subset of AI, suitable for tasks like simple pattern recognition and prediction. Deep Learning (DL), the focus of this section, is a subset of ML that leverages algorithms to extract meaningful patterns from data. Unlike ML, DL does not necessarily require human intervention, such as providing structured, labeled datasets (e.g., 1,000 bird images labeled as “bird” and 1,000 cat images labeled as “cat”). </p><p>DL utilizes layered, hierarchical Deep Neural Networks (DNNs), where hidden and output layers consist of computational units, artificial neurons, which individually process input data. The nodes in the input layer pass the input data to the first hidden layer without performing any computations, which is why they are not considered neurons or computational units. Each neuron calculates a pre-activation value (z) based on the input received from the previous layer and then applies an activation function to this value, producing a post-activation output (ŷ) value. There are various DNN models, such as Feed-Forward Neural Networks (FNN), Convolutional Neural Networks (CNN), and Recurrent Neural Networks (RNN), each designed for different use cases. For example, FNNs are suitable for simple, structured tasks like handwritten digit recognition using the MNIST dataset [1], CNNs are effective for larger image recognition tasks such as with the CIFAR-10 dataset [2], and RNNs are commonly used for time-series forecasting, like predicting future sales based on historical sales data. </p><p>To provide accurate predictions based on input data, neural networks are trained using labeled datasets. The MNIST (Modified National Institute of Standards and Technology) dataset [1] contains 60,000 training and 10,000 test images of handwritten digits (grayscale, 28x28 pixels). The CIFAR-10 [2] dataset consists of 60,000 color images (32x32 pixels), with 50,000 training images and 10,000 test images, divided into 10 classes. The CIFAR-100 dataset [3], as the name implies, has 100 image classes, with each class containing 600 images (500 training and 100 test images per class). Once the test results reach the desired level, the neural network can be deployed to production.</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/deep-learning-basics/1-1.jpg alt=img|320x271></p><p><strong>Figure 1-1:</strong> <em>Deep Learning Introduction.</em></p><h2 id=artificial-neuron>Artificial Neuron
<a class=anchor href=#artificial-neuron>#</a></h2><p>As we saw in the previous section, DNNs leverage a hierarchical, layered, role-based (input layer, n hidden layers, and output layer) network model, where each layer consists of artificial neurons. Neurons in different layers may be fully or partially connected to neurons in the other layers, depending on the DNN’s network model. However, neurons within the same layer are not connected.</p><p>The logical structure and functionality of an artificial neuron aim to emulate those of a biological neuron. A biological neuron transmits electrical signals to its peer neuron via the output axon to the axon terminal, which connects to the dendrites of the peer neuron at a connection point called a synapse. A biological neuron may receive signals from multiple dendrites, and if the signal is strong enough, it activates the neuron, causing the cell body to trigger a signal through its output axon, which connects to another neuron, and the process continues.</p><p>An artificial neuron, as a computational unit, calculates the weighted sum (z = pre-activation value) of the input (x) received from the previous layer, adds a bias value (b), and applies an activation function to produce the output (ŷ = post-activation value). </p><p>The weight value for the input can be loosely compared to a synapse since it represents a connection, input values are assigned with weight. The weights-to-neuron association, in turn, can be seen as analogous to dendrites. The computational processes (weighted sum of inputs, bias addition, and activation functions) represent the cell body, while the connections to other neurons can be compared to output axons. In Figure 1-2, bn refers to a biological neuron. From now on, &ldquo;neuron&rdquo; will refer to an artificial neuron.</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/deep-learning-basics/1-2.jpg alt=img|320x271></p><p><strong>Figure 1-2:</strong> <em>Construct of an Artificial Neuron.</em></p><h3 id=weighted-sum-for-pre-activation-value>Weighted Sum for Pre-Activation Value
<a class=anchor href=#weighted-sum-for-pre-activation-value>#</a></h3><p>The lower part of Figure 1-2 depicts the mathematical formulas of a neuron. The <em>pre-activation</em> value z is the weighted sum of the inputs. Although the bias is not part of the input data, a neuron treats it as an input variable when calculating the weighted sum. Each input x has a corresponding weight w. The calculation process is straightforward: each input value is multiplied by its corresponding weight, and the results are summed to obtain the weighted sum z.</p><p>The capital Greek letter sigma ∑ in the formula indicates that we are summing a series of terms, which, in this case, are the input values multiplied by their respective weights. The letter n specifies how many terms are being summed (four pairs of inputs and weights), while the letter iii denotes the starting point of the summation (bias b0 and weight w0). The equation for the weighted sum is:</p><p>z = b0w0 + x1w1 + x2w2 + x3w3. </p><h3 id=relu-activation-function-for-post-activation>ReLU Activation Function for Post-Activation
<a class=anchor href=#relu-activation-function-for-post-activation>#</a></h3><p>Next, the process applies an activation function, ReLU (Rectified Linear Unit) [4] in our example, to the weighted sum z obtain the post-activation value ŷ. The output of the ReLU function is z if z is greater than zero (0); otherwise, the result is zero (0). This can be written as: </p><p>ŷ = MAX (0, z) </p><p>which selects the larger value between 0 and variable z.  The figure 1-3 depicts the ReLU activation function.</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/deep-learning-basics/1-3.jpg alt=img|320x271></p><p><strong>Figure 1-3:</strong> <em>Construct of an Artificial Neuron.</em></p><p>Based on the figure 1-3 we can use the mathematical definition below for ReLU:</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/deep-learning-basics/1-3-kaava.jpg alt=img|320x271></p><h3 id=bias-term>Bias Term
<a class=anchor href=#bias-term>#</a></h3><p>In the example calculation above, imagine that all input values are zero. Without a bias term, the activation value will be zero, regardless of how large the weight parameters are. Therefore, the bias term allows the neuron to produce non-zero outputs, even when all input values are zero.</p><h3 id=s-shaped-functions--tanh-and-sigmoid>S-Shaped Functions – TANH and SIGMOID
<a class=anchor href=#s-shaped-functions--tanh-and-sigmoid>#</a></h3><p>The ReLU function is a non-linear activation function. Naturally, there are other activation functions as well. The Hyperbolic Tangent (tanh) [5] and the logistic Sigmoid [6] functions are examples of S-shaped functions that are symmetric around zero. Figure 1-4 illustrates that as the positive z value increases, the tanh function approaches one (1), while as the negative z value decreases, it approaches -1. Thus, the range of the tanh function is from -1 to 1. Similarly, the sigmoid function&rsquo;s S-curve is also symmetric around zero, but its range is from 0 to 1.</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/deep-learning-basics/1-5.jpg alt=img|320x271></p><p><strong>Figure 1-4:</strong> <em>Tanh and Sigmoid functions.</em></p><p>Here are examples of both functions using the same pre-activation value of 3.02, as in the ReLU activation function example.</p><p>Note, the ⅇ represents <strong>Euler’s Number ⅇ≈ 2.718.</strong> The symbol σ represents the sigmoid function.</p><p>The formula for tanh function is:</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/deep-learning-basics/1-4-tanh-1.jpg alt=img|320x271></p><p>The tanh function for z = 3,02</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/deep-learning-basics/1-4-tanh-2.jpg alt=img|320x271></p><p>The formula for sigmoid function is:</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/deep-learning-basics/1-4-sigmoid-1.jpg alt=img|320x271></p><p>The sigmoid function for z = 3,02</p><p>The symbol σ represents sigmoid function.</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/deep-learning-basics/1-4-sigmoid-2.jpg alt=img|320x271></p><p>For z=3.02, both the sigmoid and tanh functions return values close to 1, but the tanh function is slightly higher, approaching its maximum of 1 more quickly than the sigmoid.</p><p>Which activation function should we use? The ReLU function has largely replaced the tanh and sigmoid functions due to its simplicity, which reduces the required computation cycles. However, if tanh and sigmoid are used, tanh is typically applied in the hidden layers, while sigmoid is used in the output layer.</p><h2 id=backpropagation-algorithm-introduction>Backpropagation Algorithm: Introduction
<a class=anchor href=#backpropagation-algorithm-introduction>#</a></h2><p>This chapter introduces the training model of a neural network based on the Backpropagation algorithm. The goal is to provide a clear and solid understanding of the process without delving deeply into the mathematical formulas, while still explaining the fundamental operations of the involved functions. The chapter also briefly explains why, and in which phases the training job generates traffic to the network, and why lossless packet transport is required. The Backpropagation algorithm is composed of two phases: the Forward pass (computation phase) and the Backward pass (adjustment and communication phase).</p><p>In the Forward pass, neurons in the first hidden layer calculate the weighted sum of input parameters received from the input layer, which is then passed to the neuron&rsquo;s activation function. Note that neurons in the input layer are not computational units; they simply pass the input variables to the connected neurons in the first hidden layer. The output from the activation function of a neuron is then used as input for the connected neurons in the next layer, whether it is another hidden layer or the output layer. The result of the activation function in the output layer represents the model&rsquo;s prediction, which is compared to the expected value (ground truth) using the error function. The output of the error function indicates the accuracy of the current training iteration. If the result is sufficiently close to the expected value, the training is complete. Otherwise, it triggers the Backward pass process.</p><p>In the Backward pass process, the Backpropagation algorithm first calculates the derivative of the error function. This derivative is then used to compute the error term for each neuron in the model. Neurons use their calculated error terms to determine how much and in which direction the current weight values must be fine-tuned. Depending on the model and the parallelization strategy, GPUs in multi-GPU clusters synchronize information during the Backpropagation process. This process affects network utilization.</p><p>Our feedforward neural network, shown in Figure 2-1, has one hidden layer and one output layer. If we wanted our example to be a deep neural network, we would need to add additional layers, as the definition of &ldquo;deep&rdquo; requires two or more hidden layers. For simplicity, the input layer is not shown in the figure.</p><p>We have three input parameters connected to neuron-a in the hidden layer as follows:</p><ul><li>Input X1 = 0.2 > neuron-a via weight Wa1 = 0.1</li><li>Input X2 = 0.1 > neuron-a via weight Wa2 = 0.2</li><li>Input X3 = 0.4 > neuron-a via weight Wa3 = 0.3</li><li>Bias ba0 = 1.0 > neuron-a via weight Wa0 = 0.6</li></ul><p>The bias term helps ensure that the neuron is active, meaning its output value is not zero.</p><p>The input parameters are treated as constant values, while the weight values are variables that will be adjusted during the Backward pass if the training result does not meet expectations. The initial weight values are our best guess for achieving the desired training outcome. The result of the weighted sum calculation is passed to the activation function, which provides the input for neuron-b in the output layer. We use the ReLU (Rectified Linear Unit) activation function in both layers due to its simplicity. There are other activation functions, such as hyperbolic tangent (tanh), sigmoid, and softmax, but those are outside the scope of this chapter.</p><p>The input values and weights for neuron-b are:</p><ul><li>Neuron-a activation function output f(af) > neuron-b via weight Wb1</li><li>Bias ba0 = 1.0 > neuron-b via weight Wa0 = 0.5</li></ul><p>The output, Ŷ, from neuron-b represents our feedforward neural network&rsquo;s prediction. This value is used along with the expected result, y, as input for the error function. In this example, we use the Mean Squared Error (MSE) error function. As we will see, the result of the first training iteration does not match our expected value, leading us to initiate the Backward pass process.</p><p>In the first step of the Backward pass, the Backpropagation algorithm calculates the derivative of the error function (MSE’). Neurons-a and b use this result as input to compute their respective error terms by multiplying MSE’ with the result of the activation function and the weight value associated with the connection to the next neuron. Note that for neuron-b, there is no next layer—just the error function—so the weight parameter is excluded from the error term calculation of neuron-b. Next, the error term value is multiplied by an input value and learning rate, and this adjustment value is added to the current weight.</p><p>After completing the Backward pass, the Backpropagation algorithm starts a new iteration of the Forward pass, gradually improving the model&rsquo;s prediction until it closely matches the expected value, at which point the training is complete.</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/deep-learning-basics/2-1.jpg alt=img|320x271></p><p><strong>Figure 2-1:</strong> <em>Backpropagation Algorithm.</em></p><h3 id=introduction-1>Introduction 
<a class=anchor href=#introduction-1>#</a></h3><p>This chapter introduces the training model of a neural network based on the Backpropagation algorithm. The goal is to provide a clear and solid understanding of the process without delving deeply into the mathematical formulas, while still explaining the fundamental operations of the involved functions. The chapter also briefly explains why, and in which phases the training job generates traffic to the network, and why lossless packet transport is required. The Backpropagation algorithm is composed of two phases: the Forward pass (computation phase) and the Backward pass (adjustment and communication phase).</p><p>In the Forward pass, neurons in the first hidden layer calculate the weighted sum of input parameters received from the input layer, which is then passed to the neuron&rsquo;s activation function. Note that neurons in the input layer are not computational units; they simply pass the input variables to the connected neurons in the first hidden layer. The output from the activation function of a neuron is then used as input for the connected neurons in the next layer. The result of the activation function in the output layer represents the model&rsquo;s prediction, which is compared to the expected value (ground truth) using the error function. The output of the error function indicates the accuracy of the current training iteration. If the result is sufficiently close to the expected value (error function close to zero), the training is complete. Otherwise, it triggers the Backward pass process.</p><p>As the first step in the backward pass, the backpropagation algorithm calculates the derivative of the error function, providing the output error (gradient) of the model. Next, the algorithm computes the error term (gradient) for the neuron(s) in the output layer by multiplying the derivative of each neuron’s activation function by the model&rsquo;s error term. Then, the algorithm moves to the preceding layer and calculates the error term (gradient) for its neuron(s). This error term is now calculated using the error term of the connected neuron(s) in the next layer, the derivative of each neuron’s activation function, and the value of the weight parameter associated with the connection to the next layer.</p><p>After calculating the error terms, the algorithm determines the weight adjustment values for all neurons simultaneously. This computation is based on the input values, the adjustment values, and a user-defined learning rate. Finally, the backpropagation algorithm refines all weight values by adding the adjustment values to the initial weights. Once the backward pass is complete, the backpropagation algorithm starts a new iteration of the forward pass, gradually improving the model&rsquo;s predictions until they closely match the expected values, at which point the training is complete.</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/deep-learning-basics/image-1.png alt=img|320x271></p><p><strong>Figure 2-1:</strong> <em>Backpropagation Overview.</em></p><h3 id=the-first-iteration---forward-pass>The First Iteration - Forward Pass
<a class=anchor href=#the-first-iteration---forward-pass>#</a></h3><p>Training a model often requires multiple iterations of forward and backward passes. In the forward pass, neurons in the first hidden layer calculate the weighted sum of input values, each multiplied by its associated weight parameter. These neurons then apply an activation function to the result. Neurons in subsequent layers use the activation output from previous layers as input for their own weighted sum calculations. This process continues through all the layers until reaching the output layer, where the activation function produces the model&rsquo;s prediction.</p><p>After the forward pass, the backpropagation algorithm calculates the error by comparing the model&rsquo;s output with the expected value, providing a measure of accuracy. If the model&rsquo;s output is close to the expected value, training is complete. Otherwise, the backpropagation algorithm initiates the backward pass to adjust the weights and reduce the error in subsequent iterations.</p><h3 id=neuron-a-forward-pass-calculations>Neuron-a Forward Pass Calculations
<a class=anchor href=#neuron-a-forward-pass-calculations>#</a></h3><h4 id=weighted-sum>Weighted Sum
<a class=anchor href=#weighted-sum>#</a></h4><p>In Figure 2-2, we have an imaginary training dataset with three inputs and a bias term. Input values and their respective initial weight values are listed below: </p><ul><li>x1 = 0.2 , initial weight wa1 = 0.1</li><li>x2 = 0.1, initial weight wa2 = 0.2</li><li>x3 = 0.4 , initial weight wa3 = 0.3</li><li>ba0 = 1.0 , initial weight wa0 = 0.6</li></ul><p>From the model training perspective, the input values are constant, unchageable values, while weight values are variables which will be refined during the backward pass process.</p><p>The standard way to write the weighted sum formula is: </p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/deep-learning-basics/image-2.png alt=img|320x271></p><p>Where:</p><ul><li>n = 3 represents the number of input values (x1, x2, x3).</li><li>Each input xi  is multiplied by its respective weight wi, and the sum of these products is added to the bias term b.</li></ul><p>In this case, the equation can be explicitly stated as:</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/deep-learning-basics/image-3.png alt=img|320x271></p><p>Which with our parameters gives:</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/deep-learning-basics/image-4.png alt=img|320x271></p><h4 id=activation-function>Activation Function
<a class=anchor href=#activation-function>#</a></h4><p>Neuron-a uses the previously calculated weighted sum as input for the activation function. We are using the ReLU function (Rectified Linear Unit), which is more popular than the hyperbolic tangent and sigmoid functions due to its simplicity and lower computational cost.</p><p>The standard way to write the ReLU function is:</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/deep-learning-basics/image-5.png alt=img|320x271></p><p>Where:</p><p>f(a) represents the activation function.
z  is the weighted sum of inputs.</p><p>The ReLU function returns the z if z > 0. Otherwise, it returns 0 if z ≤ 0.</p><p>In our example, the weighted sum za is 0.76, so the ReLU function returns:</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/deep-learning-basics/image-6.png alt=img|320x271></p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/deep-learning-basics/image-7.png alt=img|320x271></p><p><strong>Figure 2-2:</strong> <em>Activation Function for Neuron-a.</em></p><h3 id=neuron-b-forward-pass-calculations>Neuron-b Forward Pass Calculations
<a class=anchor href=#neuron-b-forward-pass-calculations>#</a></h3><h4 id=weighted-sum-1>Weighted Sum
<a class=anchor href=#weighted-sum-1>#</a></h4><p>Besides the bias term value of 1.0,  Neuron-b uses the result provided by the activation function of neuron-a as an input to weighted sum calculation. Input values and their respective initial weight values are listed below: </p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/deep-learning-basics/image-8.png alt=img|320x271></p><p>This gives us:</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/deep-learning-basics/image-9.png alt=img|320x271></p><h4 id=activation-function-1>Activation Function
<a class=anchor href=#activation-function-1>#</a></h4><p>Just like Neuron-a, Neuron-b uses the previously calculated weighted sum as input for the activation function. Because the zb = 0.804 is greater than zero, the ReLU activation function f(b) returns:</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/deep-learning-basics/image-10.png alt=img|320x271></p><p>Neuron-b is in the output layer, so its activation function result <em>y</em>_<sub><span style="font-family:cambria math,serif;mso-bidi-font-family:Calibri;mso-bidi-font-size:6pt;mso-bidi-theme-font:minor-latin">b</span></sub>_ represents the prediction of the model. </p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/deep-learning-basics/image-11.png alt=img|320x271></p><p><strong>Figure 2-3:</strong> Activation Function for Neuron-b.</p><h3 id=error-function>Error Function
<a class=anchor href=#error-function>#</a></h3><p>To keep things simple, we have used only one training example. However, in real-life scenarios, there will always be more than one training example. For instance, a training dataset might contain 10 images of cats and 10 images of dogs, each having 28x28 pixels. Each image represents a training example, giving us a total of 20 training examples. The purpose of the error function is to provide a single error metric for all training examples. In this case, we are using the Mean Squared Error (MSE).</p><p>We can calculate the MSE using the formula below where the expected value y is 1.0 and the model’s  prediction for the training example yb = 0.804. This gives an error metric of 0.019, which can be interpreted as an indicator of the model&rsquo;s accuracy.</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/deep-learning-basics/image-12.png alt=img|320x271></p><p>The result of the error function is not sufficiently close to the desired value, which is why this result triggers the backward pass process.</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/deep-learning-basics/image-13.png alt=img|320x271></p><p><strong>Figure 2-4:</strong> <em>Calculating the Error Function for Training Examples.</em></p><h3 id=backward-pass>Backward Pass
<a class=anchor href=#backward-pass>#</a></h3><p>In the forward pass, we calculate the model’s accuracy using several functions. First, Neuron-a computes the weighted sum Σ(za ) by multiplying the input values and the bias term with their respective weights. The output, za, is then passed through the activation function f(a), producing ya. Neuron-b, in turn, takes ya and the bias term to calculate the weighted sum Σ(zb ). The activation function f(b) then uses zb to compute the model’s output, yb. Finally, the error function f(e) calculates the model’s accuracy based on the output.</p><p>So, dependencies between function can be seen as:</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/deep-learning-basics/image-14.png alt=img|320x271></p><p>The backpropagation algorithm combines these five functions to create a new error function, enew(x), using function composition and the chain rule. The following expression shows how the error function relates to the weight parameter w1 used by Neuron-a:</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/deep-learning-basics/image-15.png alt=img|320x271></p><p>This can be expressed using the composition operator (∘) between functions:</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/deep-learning-basics/image-16.png alt=img|320x271></p><p>Next, we use a method called gradient descent to gradually adjust the initial weight values, refining them to bring the model&rsquo;s output closer to the expected result. To do this, we compute the derivative of the composite function using the chain rule, where we take the derivatives of:</p><ol><li>The error function (e) with respect to the activation function (b).</li><li>The activation function b with respect to the weighted sum (zb). </li><li>The weighted sum (zb) with respect to the activation function (a).</li><li>The activation function (a) with respect to weighted sum (za(w1)). </li></ol><p>In Leibniz’s notation, this looks like:</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/deep-learning-basics/image-17.png alt=img|320x271></p><p>Figure 2-5 illustrates the components of the backpropagation algorithm, along with their relationships and dependencies.</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/deep-learning-basics/image-18.png alt=img|320x271></p><p><strong>Figure 2-5:</strong> <em>The Backward Pass Overview.</em></p><h4 id=partial-derivative-for-error-function--output-error-gradient>Partial Derivative for Error Function – Output Error (Gradient)
<a class=anchor href=#partial-derivative-for-error-function--output-error-gradient>#</a></h4><p>As a recap, and for illustrating that the prediction of the first iteration fails, Figure 2-6 includes the computation for the error function (MSE = 0.019). </p><p>As a first step, we calculate the partial derivative of the error function. In this case, the partial derivative describes the rate of change of the error function when the input variable yb changes. The derivative is called partial when one of its input values is held constant (i.e., not adjusted by the algorithm). In our example, the expected value y is constant input. The result of the partial derivative of the error function describes how the predicted output should change yb to minimize the model’s error.</p><p>We use the following formula for computing the derivative of the error function:</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/deep-learning-basics/image-19.png alt=img|320x271></p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/deep-learning-basics/image-20.png alt=img|320x271></p><p><strong>Figure 2-6:</strong> <em>The Backward Pass – Derivative of the Error Function.</em></p><p>The following explanation is meant for readers interested in why there is a minus sign in front of the function.</p><p>When calculating the derivative, we use the Power Rule. The Power Rule states that if we have a function f(x) = xn , then its derivative is f’(x) = n ⋅ xn-1. In our case, this applies to the error function:</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/deep-learning-basics/image-21.png alt=img|320x271></p><p>Using the Power Rule, the derivative becomes:</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/deep-learning-basics/image-22.png alt=img|320x271></p><p>Next, we apply the chain rule by multiplying this result by the derivative of the inner function (y − yb), with respect to yb . Since y is treated as a constant (because it represents our target value, which doesn&rsquo;t change during optimization), the derivative of (y − yb) with respect to yb  is simply −1, as the derivative of − yb  with respect to yb  is −1, and the derivative of y (a constant) is 0.</p><p>Therefore, the final derivative of the error function with respect to yb  is:</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/deep-learning-basics/image-23.png alt=img|320x271></p><h4 id=partial-derivative-for-the-activation-function>Partial Derivative for the Activation Function
<a class=anchor href=#partial-derivative-for-the-activation-function>#</a></h4><p>After computing the output error, we calculate the derivative of the activation function f(b) with respect to zb . Neuron b uses the ReLU activation function, which states that if the input to the function is greater than 0, the derivative is 1; otherwise, it is 0. In our case, the result of the activation function f(b)=0.804, so the derivative is 1.</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/deep-learning-basics/image-24.png alt=img|320x271></p><h4 id=error-term-for-neurons-gradient>Error Term for Neurons (Gradient)
<a class=anchor href=#error-term-for-neurons-gradient>#</a></h4><p>The error term (Gradient) for neuron-b is calculated by multiplying the output error, the partial derivative of the error function,  by the derivative of the neuron&rsquo;s activation function. This means that now we propagate the model&rsquo;s error backward using it as a base value for finetuning the model accuracy (i.e., refining new weight values). This is why the term backward pass fits perfectly for the process.</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/deep-learning-basics/image-25.png alt=img|320x271></p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/deep-learning-basics/image-26.png alt=img|320x271></p><p><strong>Figure 2-7:</strong> <em>The Backward Pass – Error Term (Gradient) for Neuron-b.</em></p><p>After computing the error term for Neuron-b, the backward pass moves to the preceding layer, the hidden layer, and calculates the error term for Neuron-a. The algorithm computes the derivative for the activation function f(a) = 1, as it did with the Neuron-b. Next, it multiplies the result by Neuron-b&rsquo;s error term (-0.196) and the connected weight parameter , wb1 =0.4. The result -0.0784 is the error term for Neuron-a.</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/deep-learning-basics/image-27.png alt=img|320x271></p><p><strong>Figure 2-8:</strong> <em>The Backward Pass – Error Term (Gradient) for Neuron-a.</em></p><h4 id=weight-adjustment-value>Weight Adjustment Value
<a class=anchor href=#weight-adjustment-value>#</a></h4><p>After computing error terms for all neurons in every layer, the algorithm simultaneously calculates the weight adjustment value for every weight. The process is simple, the error term is multiplied with an input value connected to weight and with learning rate (η). The learning rate balances convergence speed and training stability. We have set it to -0.6 for the first iteration. The learning rate is a hyper-parameter, meaning it is set by the user rather than learned by the model during training. It affects the behavior of the backpropagation algorithm by controlling the size of the weight updates. It is also possible to adjust the learning rate during training—using a higher learning rate at the start to allow faster convergence and lowering it later to avoid overshooting the optimal result. </p><p>Weight adjustment value for weight wb1 and wa1 respectively:</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/deep-learning-basics/image-28.png alt=img|320x271></p><p><strong>Note!</strong> <em>It is not recommended to use a negative learning rate. I use it here because we get a good enough output for the second forward pass iteration.</em></p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/deep-learning-basics/image-29.png alt=img|320x271></p><p><strong>Figure 2-9:</strong> <em>The Backward Pass – Weight Adjustment Value for Neurons.</em></p><h4 id=refine-weights>Refine Weights
<a class=anchor href=#refine-weights>#</a></h4><p>As the last step, the backpropagation algorithm computes new values for every weight parameter in the model by simply summing the initial weight value and weight adjustment value.</p><p>New values for weight  parameters wb1 and wa1 respectively:</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/deep-learning-basics/image-30.png alt=img|320x271></p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/deep-learning-basics/image-31.png alt=img|320x271></p><p><strong>Figure 2-10:</strong> <em>The Backward Pass – Compute New Weight Values.</em></p><h3 id=the-second-iteration---forward-pass>The Second Iteration - Forward Pass
<a class=anchor href=#the-second-iteration---forward-pass>#</a></h3><p>After updating all the weight values (wa0, wa1, wa2, and wa3 ), the backpropagation process begins the second iteration of the forward pass. As shown in Figure 2-11, the model output yb = 0.9982 is very close to the expected value y = 1.0. The new MSE = 0.0017, is much better than 0.019 computed in the first iteration.</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/deep-learning-basics/image-32.png alt=img|320x271></p><p><strong>Figure 2-11:</strong> The Second Iteration of the Forward Pass.</p><h3 id=network-impact>Network Impact
<a class=anchor href=#network-impact>#</a></h3><p>Figure 2-12 shows a hypothetical example of Data Parallelization, where our training data set is split into two batches, A and B, which are processed by GPU-A and GPU-B, respectively. The training model is the same on both GPUs: Fully-Connected, with two hidden layers of four neurons each, and one output neuron in the output layer.</p><p>After computing a model prediction during the forward pass, the backpropagation algorithm begins the backward pass by calculating the gradient (output error) for the error function. Once computed, the gradients are synchronized between the GPUs. The algorithm then averages the gradients, and the process moves to the preceding layer. Neurons in the preceding layer calculate their gradient by multiplying the weighted sum of their connected neurons’ averaged gradients and connected weight with the local activation function’s partial derivative. These neuron-based gradients are then synchronized over connections. Before the process moves to the preceding layer, gradients are averaged. The backpropagation algorithm executes the same process through all layers. </p><p>If packet loss occurs during the synchronization, it can ruin the entire training process, which would need to be restarted unless snapshots were taken. The cost of losing even a single packet could be enormous, especially if training has been ongoing for several days or weeks. Why is a single packet so important? If the synchronization between the gradients of two parallel neurons fails due to packet loss, the algorithm cannot compute the average, and the neurons in the preceding layer cannot calculate their gradient. Besides, if the connection, whether the synchronization happens over NVLink, InfiniBand, Ethernet (RoCE or RoCEv2), or wireless connection, causes a delay, the completeness of the training slows down. This causes GPU under-utilization which is not efficient from the business perspective.</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/deep-learning-basics/image-33.png alt=img|320x271></p><p><strong>Figure 2-12:</strong> <em>Backward Pass – Gradient Synchronization and Averaging.</em></p><p>To be conntinued&mldr;</p><p><strong>References:</strong></p><ul><li><a href=https://nwktimes.blogspot.com/2024/09/ai4ne-ch1-artificial-neuron.html>https://nwktimes.blogspot.com/2024/09/ai4ne-ch1-artificial-neuron.html</a></li><li><a href=https://nwktimes.blogspot.com/2024/10/ai-for-network-engineers.html>https://nwktimes.blogspot.com/2024/10/ai-for-network-engineers.html</a></li></ul></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/prmanna/tech-book/commit/2ad9d125fc542fd3713a54b063c2d309190004bb title='Last modified by Prasenjit Manna | May 5, 2025' target=_blank rel=noopener><img src=/tech-book/svg/calendar.svg class=book-icon alt>
<span>May 5, 2025</span></a></div></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><ul><li><ul><li><ul><li><a href=#content>Content</a></li></ul></li></ul></li><li><a href=#introduction>Introduction</a></li><li><a href=#artificial-neuron>Artificial Neuron</a><ul><li><a href=#weighted-sum-for-pre-activation-value>Weighted Sum for Pre-Activation Value</a></li><li><a href=#relu-activation-function-for-post-activation>ReLU Activation Function for Post-Activation</a></li><li><a href=#bias-term>Bias Term</a></li><li><a href=#s-shaped-functions--tanh-and-sigmoid>S-Shaped Functions – TANH and SIGMOID</a></li></ul></li><li><a href=#backpropagation-algorithm-introduction>Backpropagation Algorithm: Introduction</a><ul><li><a href=#introduction-1>Introduction </a></li><li><a href=#the-first-iteration---forward-pass>The First Iteration - Forward Pass</a></li><li><a href=#neuron-a-forward-pass-calculations>Neuron-a Forward Pass Calculations</a><ul><li><a href=#weighted-sum>Weighted Sum</a></li><li><a href=#activation-function>Activation Function</a></li></ul></li><li><a href=#neuron-b-forward-pass-calculations>Neuron-b Forward Pass Calculations</a><ul><li><a href=#weighted-sum-1>Weighted Sum</a></li><li><a href=#activation-function-1>Activation Function</a></li></ul></li><li><a href=#error-function>Error Function</a></li><li><a href=#backward-pass>Backward Pass</a><ul><li><a href=#partial-derivative-for-error-function--output-error-gradient>Partial Derivative for Error Function – Output Error (Gradient)</a></li><li><a href=#partial-derivative-for-the-activation-function>Partial Derivative for the Activation Function</a></li><li><a href=#error-term-for-neurons-gradient>Error Term for Neurons (Gradient)</a></li><li><a href=#weight-adjustment-value>Weight Adjustment Value</a></li><li><a href=#refine-weights>Refine Weights</a></li></ul></li><li><a href=#the-second-iteration---forward-pass>The Second Iteration - Forward Pass</a></li><li><a href=#network-impact>Network Impact</a></li></ul></li></ul></li></ul></nav></div></aside></main></body></html>