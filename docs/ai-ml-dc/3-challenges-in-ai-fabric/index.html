<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  Intro
  #

Figure 10-1 illustrates a simple distributed GPU cluster consisting of three GPU hosts. Each host has two GPUs and a Network Interface Card (NIC) with two interfaces. Intra-host GPU communication uses high-speed NVLink interfaces, while inter-host communication takes place via NICs over slower PCIe buses.
GPU-0 on each host is connected to Rail Switch A through interface E1. GPU-1 uses interface E2 and connects to Rail Switch B. In this setup, inter-host communication between GPUs connected to the same rail passes through a single switch. However, communication between GPUs on different rails goes over three hops  Rail–Spine–Rail switches."><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://prasenjitmanna.com/tech-book/docs/ai-ml-dc/3-challenges-in-ai-fabric/"><meta property="og:site_name" content="Technical Book"><meta property="og:title" content="Challenges in AI Fabric"><meta property="og:description" content=" Intro # Figure 10-1 illustrates a simple distributed GPU cluster consisting of three GPU hosts. Each host has two GPUs and a Network Interface Card (NIC) with two interfaces. Intra-host GPU communication uses high-speed NVLink interfaces, while inter-host communication takes place via NICs over slower PCIe buses.
GPU-0 on each host is connected to Rail Switch A through interface E1. GPU-1 uses interface E2 and connects to Rail Switch B. In this setup, inter-host communication between GPUs connected to the same rail passes through a single switch. However, communication between GPUs on different rails goes over three hops Rail–Spine–Rail switches."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:modified_time" content="2025-07-06T22:55:13+05:30"><title>Challenges in AI Fabric | Technical Book</title>
<link rel=manifest href=/tech-book/manifest.json><link rel=icon href=/tech-book/favicon.png><link rel=canonical href=https://prasenjitmanna.com/tech-book/docs/ai-ml-dc/3-challenges-in-ai-fabric/><link rel=stylesheet href=/tech-book/book.min.a61cdb2979f3c2bece54ef69131fba427dd57d55c232d3bb5fdb62ac41aa8354.css integrity="sha256-phzbKXnzwr7OVO9pEx+6Qn3VfVXCMtO7X9tirEGqg1Q=" crossorigin=anonymous><script defer src=/tech-book/fuse.min.js></script><script defer src=/tech-book/en.search.min.e34205f34c8209a2ddcfbfe73c86e8e268fdf2f9564e24f2e5180bdc86bcf429.js integrity="sha256-40IF80yCCaLdz7/nPIbo4mj98vlWTiTy5RgL3Ia89Ck=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/tech-book/><img src=/tech-book/logo.png alt=Logo><span>Technical Book</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><input type=checkbox id=section-99f552133860bc21797a47fe73e93434 class=toggle>
<label for=section-99f552133860bc21797a47fe73e93434 class="flex justify-between"><a href=/tech-book/docs/5g/>5G</a></label><ul><li><input type=checkbox id=section-dfc790b73acb0410a0114547cbf5af32 class=toggle>
<label for=section-dfc790b73acb0410a0114547cbf5af32 class="flex justify-between"><a href=/tech-book/docs/5g/5g-intro/>An Overview of 5G Networking</a></label></li></ul></li><li><input type=checkbox id=section-7c4a3b16aeeb5b7194b232c1eef2f4fb class=toggle>
<label for=section-7c4a3b16aeeb5b7194b232c1eef2f4fb class="flex justify-between"><a href=/tech-book/docs/algorithms/>Algorithms</a></label><ul><li><input type=checkbox id=section-8956d82fbe6869140758f7e8679174bc class=toggle>
<label for=section-8956d82fbe6869140758f7e8679174bc class="flex justify-between"><a href=/tech-book/docs/algorithms/breadth-first-search/>Breadth First Search</a></label></li><li><input type=checkbox id=section-5a049cfad1740f3fd30565524385fa57 class=toggle>
<label for=section-5a049cfad1740f3fd30565524385fa57 class="flex justify-between"><a href=/tech-book/docs/algorithms/depth-first-search/>Depth First Search</a></label></li><li><input type=checkbox id=section-ebc049f26d82165be8c6f1f9e504e799 class=toggle>
<label for=section-ebc049f26d82165be8c6f1f9e504e799 class="flex justify-between"><a href=/tech-book/docs/algorithms/easy/>Easy Complexity</a></label></li><li><input type=checkbox id=section-1071946392bd1f431993e950147fa054 class=toggle>
<label for=section-1071946392bd1f431993e950147fa054 class="flex justify-between"><a href=/tech-book/docs/algorithms/priority-queue-and-heap/>Priority Queue and Heap</a></label></li><li><input type=checkbox id=section-08afbeb294c43ca4908c1c89a4be9d0a class=toggle>
<label for=section-08afbeb294c43ca4908c1c89a4be9d0a class="flex justify-between"><a href=/tech-book/docs/algorithms/two-pointers/>Two Pointers & Sliding Window</a></label></li><li><input type=checkbox id=section-ef36c7c4f7e0dec6525068c3c409100c class=toggle>
<label for=section-ef36c7c4f7e0dec6525068c3c409100c class="flex justify-between"><a href=/tech-book/docs/algorithms/medium/>Medium Complexity</a></label></li></ul></li><li><input type=checkbox id=section-8c7c5c4a8382299873178820b1d91be1 class=toggle checked>
<label for=section-8c7c5c4a8382299873178820b1d91be1 class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/>Data Center Networking for AI Clusters</a></label><ul><li><input type=checkbox id=section-433ccede4154db01f6c601940d2949e0 class=toggle>
<label for=section-433ccede4154db01f6c601940d2949e0 class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/1-ai-ml-networking/>AI/ML Networking</a></label></li><li><input type=checkbox id=section-65f71f2455a94ea3d1143666d556b0ed class=toggle>
<label for=section-65f71f2455a94ea3d1143666d556b0ed class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/2-ai-deep-learning-basics/>Deep Learning Basics | Artificial Neuron</a></label></li><li><input type=checkbox id=section-af7b72510cdccfb9feb048bbf70c6f27 class=toggle>
<label for=section-af7b72510cdccfb9feb048bbf70c6f27 class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/2-1-large-language-models/>Large Language Models (LLM)</a></label></li><li><input type=checkbox id=section-b3eb6a6d3a1b87cba26dcfbb99658303 class=toggle>
<label for=section-b3eb6a6d3a1b87cba26dcfbb99658303 class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/2-2-parallelism-strategies-in-deep-learning/>Parallelism Strategies in Deep Learning</a></label></li><li><input type=checkbox id=section-f11d3aa2e337730e1e3345f8530fe134 class=toggle checked>
<label for=section-f11d3aa2e337730e1e3345f8530fe134 class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/3-challenges-in-ai-fabric/ class=active>Challenges in AI Fabric</a></label></li><li><input type=checkbox id=section-e28420ec7507646128f3695b6f9badbb class=toggle>
<label for=section-e28420ec7507646128f3695b6f9badbb class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/4-congestion-avoidance-in-ai-fabric/>Congestion Avoidance in AI Fabric</a></label></li><li><input type=checkbox id=section-67bb7cbb1dd95e59f8515201219c090f class=toggle>
<label for=section-67bb7cbb1dd95e59f8515201219c090f class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/5-load-balancing-in-ai-fabric/>Load Balancing in AI Fabric</a></label></li><li><input type=checkbox id=section-5f3e07f6cf7921e5291ff7894e06b19b class=toggle>
<label for=section-5f3e07f6cf7921e5291ff7894e06b19b class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/6-backend-network-topologies-for-ai-fabric/>Backend Network Topologies for AI Fabrics</a></label></li><li><input type=checkbox id=section-e19a1e9030cc104536fa46dd0bdb082c class=toggle>
<label for=section-e19a1e9030cc104536fa46dd0bdb082c class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/7-backend-network-in-gpu-fabric/>Backend Network/Rail Designs</a></label></li></ul></li><li><input type=checkbox id=section-3272b2d28b2b247027bf478619ca416f class=toggle>
<label for=section-3272b2d28b2b247027bf478619ca416f class="flex justify-between"><a href=/tech-book/docs/data-center/>Data Center Tips</a></label><ul><li><input type=checkbox id=section-984f932c7aba4f0a1841e8413165c947 class=toggle>
<label for=section-984f932c7aba4f0a1841e8413165c947 class="flex justify-between"><a href=/tech-book/docs/data-center/data-center-ethernet/>Data Center Ethernet</a></label></li><li><input type=checkbox id=section-bc5ac88940153a700e63e3be886c63cc class=toggle>
<label for=section-bc5ac88940153a700e63e3be886c63cc class="flex justify-between"><a href=/tech-book/docs/data-center/data-center-technologies/>Data Center Technologies</a></label></li><li><input type=checkbox id=section-86fde1ddf43700ef8191e11c59a82cf1 class=toggle>
<label for=section-86fde1ddf43700ef8191e11c59a82cf1 class="flex justify-between"><a href=/tech-book/docs/data-center/data-center-network-virtualization/>Network Virtualization in Cloud Data Centers</a></label></li></ul></li><li><input type=checkbox id=section-ddf784688e0c6d5abb681d2c57851559 class=toggle>
<label for=section-ddf784688e0c6d5abb681d2c57851559 class="flex justify-between"><a href=/tech-book/docs/manageability/>Manageability</a></label><ul><li><input type=checkbox id=section-33ac730897af6feca81c1fdb7869c57e class=toggle>
<label for=section-33ac730897af6feca81c1fdb7869c57e class="flex justify-between"><a href=/tech-book/docs/manageability/why-grpc-on-http2/>gRPC on HTTP/2</a></label></li></ul></li><li><input type=checkbox id=section-c14ae944424668ef125e10cd791a3d3d class=toggle>
<label for=section-c14ae944424668ef125e10cd791a3d3d class="flex justify-between"><a href=/tech-book/docs/networking-tips/>Networking Tips</a></label><ul><li><input type=checkbox id=section-78fdf21c03c55935d3146441b06faf3e class=toggle>
<label for=section-78fdf21c03c55935d3146441b06faf3e class="flex justify-between"><a href=/tech-book/docs/networking-tips/dns/>DNS Overview</a></label></li><li><input type=checkbox id=section-bbcb027a658dc7a2333d59078ee507f9 class=toggle>
<label for=section-bbcb027a658dc7a2333d59078ee507f9 class="flex justify-between"><a href=/tech-book/docs/networking-tips/ecmp/>ECMP Load Balancing</a></label></li><li><input type=checkbox id=section-3b39b86f18b3451e5b8a1b81c369549c class=toggle>
<label for=section-3b39b86f18b3451e5b8a1b81c369549c class="flex justify-between"><a href=/tech-book/docs/networking-tips/ip-fragmentation/>IP Fragmentation - IPv4 & IPv6</a></label></li><li><input type=checkbox id=section-c6d06c54cc91b0bc3f948d33b437fa8b class=toggle>
<label for=section-c6d06c54cc91b0bc3f948d33b437fa8b class="flex justify-between"><a href=/tech-book/docs/networking-tips/ip-tos-dscp/>IP Precedence And TOS | DSCP</a></label></li><li><input type=checkbox id=section-5f3260c76dd37177e3f89057bfe520ae class=toggle>
<label for=section-5f3260c76dd37177e3f89057bfe520ae class="flex justify-between"><a href=/tech-book/docs/networking-tips/traceroute/>Linux traceroute tool</a></label></li><li><input type=checkbox id=section-a26cc3fafb36cbc55527adf39ec83849 class=toggle>
<label for=section-a26cc3fafb36cbc55527adf39ec83849 class="flex justify-between"><a href=/tech-book/docs/networking-tips/mlag/>Multi Chassis Link Aggregation Basics</a></label></li><li><input type=checkbox id=section-36409a0abcb2d4d4baf2e0b682d1a5dd class=toggle>
<label for=section-36409a0abcb2d4d4baf2e0b682d1a5dd class="flex justify-between"><a href=/tech-book/docs/networking-tips/qos/>QoS</a></label></li><li><input type=checkbox id=section-db41e3547d316b01acf9a0ce9c04ef34 class=toggle>
<label for=section-db41e3547d316b01acf9a0ce9c04ef34 class="flex justify-between"><a href=/tech-book/docs/networking-tips/spine-leaf-arch/>Spine-leaf Architecture Basics</a></label></li><li><input type=checkbox id=section-0eb0d409074a76b8cb02624655e2434d class=toggle>
<label for=section-0eb0d409074a76b8cb02624655e2434d class="flex justify-between"><a href=/tech-book/docs/networking-tips/tcp-congestion/>TCP Congestion Control</a></label></li><li><input type=checkbox id=section-3d1710947b3c5be1a1cc33d88fcc6f54 class=toggle>
<label for=section-3d1710947b3c5be1a1cc33d88fcc6f54 class="flex justify-between"><a href=/tech-book/docs/networking-tips/tcp-data-transfer/>TCP Data Transfer</a></label></li></ul></li><li><input type=checkbox id=section-f0a392f2f083f28a4991336773716a63 class=toggle>
<label for=section-f0a392f2f083f28a4991336773716a63 class="flex justify-between"><a href=/tech-book/docs/optical-knowledge/>Optical Knowledge</a></label><ul><li><input type=checkbox id=section-18261b95a92d4fa86116243edca4e9fb class=toggle>
<label for=section-18261b95a92d4fa86116243edca4e9fb class="flex justify-between"><a href=/tech-book/docs/optical-knowledge/optical-breakout/>Optical Transceiver(Grey) & Breakout Model</a></label></li></ul></li><li><input type=checkbox id=section-a55840d746138b3d1fedb81acbccdded class=toggle>
<label for=section-a55840d746138b3d1fedb81acbccdded class="flex justify-between"><a href=/tech-book/docs/programming-tips/>Programming Tips</a></label><ul><li><input type=checkbox id=section-f929f1fe13cb5d6cc96ca3e98f9d9777 class=toggle>
<label for=section-f929f1fe13cb5d6cc96ca3e98f9d9777 class="flex justify-between"><a href=/tech-book/docs/programming-tips/c++/>C++ Tips</a></label></li></ul></li><li><input type=checkbox id=section-b35dbf5ebcd4c19926cf5a9aab6c7655 class=toggle>
<label for=section-b35dbf5ebcd4c19926cf5a9aab6c7655 class="flex justify-between"><a href=/tech-book/docs/systemdesign-tips/>SystemDesign-Tips</a></label><ul><li><input type=checkbox id=section-4a618bc3b0b30107f0cec6d3bd6c025f class=toggle>
<label for=section-4a618bc3b0b30107f0cec6d3bd6c025f class="flex justify-between"><a href=/tech-book/docs/systemdesign-tips/code-deployment-system/>Design A Code-Deployment System</a></label></li><li><input type=checkbox id=section-e600754306aa95ce9c5a72b5efec6d7a class=toggle>
<label for=section-e600754306aa95ce9c5a72b5efec6d7a class="flex justify-between"><a href=/tech-book/docs/systemdesign-tips/stock-broker/>Design A Stock-Broker System</a></label></li><li><input type=checkbox id=section-bfa7a29878f65cfbb179e491c1211fa8 class=toggle>
<label for=section-bfa7a29878f65cfbb179e491c1211fa8 class="flex justify-between"><a href=/tech-book/docs/systemdesign-tips/design-amazon/>Design Amazon</a></label></li><li><input type=checkbox id=section-5456837e25872f6352d861a9b5662cb1 class=toggle>
<label for=section-5456837e25872f6352d861a9b5662cb1 class="flex justify-between"><a href=/tech-book/docs/systemdesign-tips/design-slack/>Design Slack</a></label></li><li><input type=checkbox id=section-5a78d16f54536a400b654f17f917bce1 class=toggle>
<label for=section-5a78d16f54536a400b654f17f917bce1 class="flex justify-between"><a href=/tech-book/docs/systemdesign-tips/google-drive/>Google Drive - Design</a></label></li></ul></li></ul><ul><li><a href=/tech-book/posts/>Blog</a></li><li><a href=https://prasenjitmanna.com/ target=_blank rel=noopener>Prasenjit's Blog</a></li><li><a href=https://prasenjitmanna.com/tech-book/ target=_blank rel=noopener>Prasenjit - Tech Book</a></li><li><a href=https://prasenjitmanna.com/upskills/ target=_blank rel=noopener>Prasenjit - Upskills</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/tech-book/svg/menu.svg class=book-icon alt=Menu>
</label><strong>Challenges in AI Fabric</strong>
<label for=toc-control><img src=/tech-book/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><ul><li><a href=#intro>Intro</a><ul><li><a href=#egress-interface-congestions>Egress Interface Congestions</a></li><li><a href=#single-point-of-failure>Single Point of Failure</a></li><li><a href=#head-of-line-blocking>Head-of-Line Blocking </a></li><li><a href=#hash-polarization-with-ecmp>Hash-Polarization with ECMP</a></li></ul></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h2 id=intro>Intro
<a class=anchor href=#intro>#</a></h2><p>Figure 10-1 illustrates a simple distributed GPU cluster consisting of three GPU hosts. Each host has two GPUs and a Network Interface Card (NIC) with two interfaces. Intra-host GPU communication uses high-speed NVLink interfaces, while inter-host communication takes place via NICs over slower PCIe buses.</p><p>GPU-0 on each host is connected to Rail Switch A through interface E1. GPU-1 uses interface E2 and connects to Rail Switch B. In this setup, inter-host communication between GPUs connected to the same rail passes through a single switch. However, communication between GPUs on different rails goes over three hops  Rail–Spine–Rail switches.</p><p>In Figure 10-1, we use a data parallelization strategy where a training dataset is split into six micro-batches, which are distributed across the GPUs. All GPUs use the shared feedforward neural network model and compute local model outputs. Next, each GPU calculates the model error and begins the backward pass to compute neuron-based gradients. These gradients indicate how much, and in which direction, the weight parameters should be adjusted to improve the training result (see Chapter 2 for details).</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/challenges-in-ai-fabric/image10-1.png alt=img|320x271></p><p><strong>Figure 10-1:</strong> <em>Rail-Optimized Topology.</em></p><h3 id=egress-interface-congestions>Egress Interface Congestions
<a class=anchor href=#egress-interface-congestions>#</a></h3><p>After computing all gradients, each GPU stores the results in a local memory buffer and starts a communication phase where computed gradients are shared with other GPUs. During this process, the data (gradients) is being sent from one GPU and written to another GPU&rsquo;s memory (RDMA Write operation). RDMA is explained in detail in Chapter 9.</p><p>Once all gradients have been received, each GPU averages the results (AllReduce) and broadcasts the aggregated gradients to the other GPUs. This ensures that all GPUs update their model parameters (weights) using the same gradient values. The Backward pass process and gradient calculation are explained in Chapter 2.</p><p>Figure 10-2 illustrates the traffic generated during gradient synchronization from the perspective of GPU-0 on Host-2. Gradients from the local host’s GPU-1 are received via the high-speed NVLink interface, while gradients from GPUs in other hosts are transmitted over the backend switching fabric. In this example, all hosts are connected to Rail Switches using 200 Gbps fiber links. Since GPUs can communicate at line rate, gradient synchronization results in up to 800 Gbps of egress traffic toward interface E1 on Host-2, via Rail Switch A. This may cause congestion, and packet drops if the egress buffer on Rail Switch A or the ingress buffer on interface E1 is not deep enough to accommodate the queued packets.</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/challenges-in-ai-fabric/image10-2.png alt=img|320x271></p><p><strong>Figure 10-2:</strong> <em>Congestion During Backward Pass.</em></p><h3 id=single-point-of-failure>Single Point of Failure
<a class=anchor href=#single-point-of-failure>#</a></h3><p>The training process of a neural network is a long-running, iterative task where GPUs must communicate with each other. The frequency and pattern of this communication depend on the chosen parallelization strategy. For example, in data parallelism, communication occurs during the backward pass, where GPUs synchronize gradients. In contrast, model parallelism and pipeline parallelism involve communication even during the forward pass, as one GPU sends activation results to the next GPU holding the subsequent layer. It is important to understand that communication issues affecting even a single GPU can delay or interrupt the entire training process. This makes the AI fabric significantly more sensitive to single points of failure compared to traditional data center fabrics.</p><p>Figure 10-3 highlights several single points of failure that may occur in real-world environments. A host connection can become degraded or completely fail due to issues in the host, NIC, rail switch, transceiver, or connecting cable. Any of these failures can isolate a GPU. While this might not seem serious in large clusters with thousands of GPUs, as discussed in the previous section, even one isolated or failed GPU can halt the training process.</p><p>Problems with interfaces, transceivers, or cables in inter-switch links can cause congestion and delays. Similar issues arise if a spine switch is malfunctioning. These types of failures typically affect inter-rail traffic but not intra-rail communication. A failure in a rail switch can isolate all GPUs connected to that rail, creating a critical point of failure for a subset of the GPU cluster.</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/challenges-in-ai-fabric/image10-3.png alt=img|320x271></p><p><strong>Figure 10-3:</strong> <em>Single-Point Failures.</em></p><h3 id=head-of-line-blocking>Head-of-Line Blocking 
<a class=anchor href=#head-of-line-blocking>#</a></h3><p>In this example GPU clusters, NCCL (NVIDIA Collective Communications Library)  has built a topology where gradients are first sent from GPU-0 to GPU-1 over NVLink, and then forwarded from GPU-1 to other GPU-1s via Rail switch B.</p><p>However, this setup may lead to head-of-line blocking. This happens when GPU-1 is already busy sending its own gradients to the other GPUs, and now it also needs to forward GPU-0’s gradients. Since the PCIe and NIC bandwidth is limited, GPU-0’s traffic may need to wait in line behind GPU-1’s traffic. This queueing delay is called head-of-line blocking, and it can slow down the whole training process. The problem is more likely to happen when many GPUs rely on a single GPU or NIC for forwarding traffic to another rail. Even if only one GPU is overloaded, it can cause delays for others too.</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/challenges-in-ai-fabric/image10-4.png alt=img|320x271></p><p><strong>Figure 10-4:</strong> <em>Head-of-Line Blocking.</em></p><h3 id=hash-polarization-with-ecmp>Hash-Polarization with ECMP
<a class=anchor href=#hash-polarization-with-ecmp>#</a></h3><p>First, when two GPUs open a Queue Pair (QP) between each other, all gradient synchronization traffic is typically sent over that QP. From the network point of view, this looks like one large flow between the GPUs. In deep learning training, gradient data can be hundreds of megabytes or even gigabytes, depending on the model size. So, when it is sent over one QP, the network sees it as a single high-bandwidth flow. This kind of traffic is often called an elephant flow, because it can take a big share of the link bandwidth. This becomes a problem when multiple large flows are hashed to the same uplink or spine port. If that happens, one link can get overloaded while others remain underused. This is one of the reasons we see hash polarization and head-of-line blocking in AI clusters. Hash polarization is a condition where the load-balancing hash algorithm used in ECMP (Equal-Cost Multi-Path) forwarding results in uneven distribution of traffic across multiple available paths.</p><p>For example, in Figure 10-5, GPU-0 in Host-1 and GPU-0 in Host-2 both send traffic to GPU-1 at a rate of 200 Gbps. The ECMP hash function in Rail Switch A selects the link to Spine 1 for both flows. This leads to a situation where one spine link carries 400 Gbps of traffic, while the other remains idle. This is a serious problem in AI clusters because training jobs generate large volumes of east-west traffic between GPUs, often at line rate. When traffic is unevenly distributed due to hash polarization, some links become congested while others are idle. This causes packet delays and retransmissions, which can slow down gradient synchronization and reduce the overall training speed. In large-scale clusters, even a small imbalance can have a noticeable impact on job completion time and resource efficiency.</p><p><img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/challenges-in-ai-fabric/image10-5.png alt=img|320x271></p><p><strong>Figure 10-5:</strong> <em>ECMP Hash-Polarization.</em></p><p>The rest of this book focuses on how these problems can be mitigated or even fully avoided. We will look at design choices, transport optimizations, network-aware scheduling, and alternative topologies that help improve the robustness and efficiency of the AI fabric.</p><p><strong>References:</strong></p><ul><li><a href=https://nwktimes.blogspot.com/2025/04/ai-for-network-engneers-challenges-in.html>https://nwktimes.blogspot.com/2025/04/ai-for-network-engneers-challenges-in.html</a></li></ul></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/prmanna/tech-book/commit/9a7b1ccffd817ca157bfb814b3f163f602f47f1b title='Last modified by Prasenjit Manna | July 6, 2025' target=_blank rel=noopener><img src=/tech-book/svg/calendar.svg class=book-icon alt>
<span>July 6, 2025</span></a></div></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><ul><li><a href=#intro>Intro</a><ul><li><a href=#egress-interface-congestions>Egress Interface Congestions</a></li><li><a href=#single-point-of-failure>Single Point of Failure</a></li><li><a href=#head-of-line-blocking>Head-of-Line Blocking </a></li><li><a href=#hash-polarization-with-ecmp>Hash-Polarization with ECMP</a></li></ul></li></ul></li></ul></nav></div></aside></main></body></html>