<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  Backend Network Topologies for AI Fabrics
  #

Although there are best practices for AI Fabric backend networks, such as Data Center Quantized Congestion Control (DCQCN) for congestion avoidance, rail-optimized routed Clos fabrics, and Layer 2 Rail-Only topologies for small-scale implementations, each vendor offers its own validated design. This approach is beneficial because validated designs are thoroughly tested, and when you build your system based on the vendor’s recommendations, you receive full vendor support and avoid having to reinvent the wheel."><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://prasenjitmanna.com/tech-book/docs/ai-ml-dc/6-backend-network-topologies-for-ai-fabric/"><meta property="og:site_name" content="Technical Book"><meta property="og:title" content="Backend Network Topologies for AI Fabrics"><meta property="og:description" content=" Backend Network Topologies for AI Fabrics # Although there are best practices for AI Fabric backend networks, such as Data Center Quantized Congestion Control (DCQCN) for congestion avoidance, rail-optimized routed Clos fabrics, and Layer 2 Rail-Only topologies for small-scale implementations, each vendor offers its own validated design. This approach is beneficial because validated designs are thoroughly tested, and when you build your system based on the vendor’s recommendations, you receive full vendor support and avoid having to reinvent the wheel."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:modified_time" content="2025-05-05T17:39:19+05:30"><title>Backend Network Topologies for AI Fabrics | Technical Book</title>
<link rel=manifest href=/tech-book/manifest.json><link rel=icon href=/tech-book/favicon.png><link rel=canonical href=https://prasenjitmanna.com/tech-book/docs/ai-ml-dc/6-backend-network-topologies-for-ai-fabric/><link rel=stylesheet href=/tech-book/book.min.a61cdb2979f3c2bece54ef69131fba427dd57d55c232d3bb5fdb62ac41aa8354.css integrity="sha256-phzbKXnzwr7OVO9pEx+6Qn3VfVXCMtO7X9tirEGqg1Q=" crossorigin=anonymous><script defer src=/tech-book/fuse.min.js></script><script defer src=/tech-book/en.search.min.d6fefc4754185a3aa4ecc98e3e066e5fab9fd6381dc023666013a968d238cbfd.js integrity="sha256-1v78R1QYWjqk7MmOPgZuX6uf1jgdwCNmYBOpaNI4y/0=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/tech-book/><img src=/tech-book/logo.png alt=Logo><span>Technical Book</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><input type=checkbox id=section-99f552133860bc21797a47fe73e93434 class=toggle>
<label for=section-99f552133860bc21797a47fe73e93434 class="flex justify-between"><a href=/tech-book/docs/5g/>5G</a></label><ul><li><input type=checkbox id=section-dfc790b73acb0410a0114547cbf5af32 class=toggle>
<label for=section-dfc790b73acb0410a0114547cbf5af32 class="flex justify-between"><a href=/tech-book/docs/5g/5g-intro/>An Overview of 5G Networking</a></label></li></ul></li><li><input type=checkbox id=section-7c4a3b16aeeb5b7194b232c1eef2f4fb class=toggle>
<label for=section-7c4a3b16aeeb5b7194b232c1eef2f4fb class="flex justify-between"><a href=/tech-book/docs/algorithms/>Algorithms</a></label><ul><li><input type=checkbox id=section-8956d82fbe6869140758f7e8679174bc class=toggle>
<label for=section-8956d82fbe6869140758f7e8679174bc class="flex justify-between"><a href=/tech-book/docs/algorithms/breadth-first-search/>Breadth First Search</a></label></li><li><input type=checkbox id=section-5a049cfad1740f3fd30565524385fa57 class=toggle>
<label for=section-5a049cfad1740f3fd30565524385fa57 class="flex justify-between"><a href=/tech-book/docs/algorithms/depth-first-search/>Depth First Search</a></label></li><li><input type=checkbox id=section-ebc049f26d82165be8c6f1f9e504e799 class=toggle>
<label for=section-ebc049f26d82165be8c6f1f9e504e799 class="flex justify-between"><a href=/tech-book/docs/algorithms/easy/>Easy Complexity</a></label></li><li><input type=checkbox id=section-1071946392bd1f431993e950147fa054 class=toggle>
<label for=section-1071946392bd1f431993e950147fa054 class="flex justify-between"><a href=/tech-book/docs/algorithms/priority-queue-and-heap/>Priority Queue and Heap</a></label></li><li><input type=checkbox id=section-08afbeb294c43ca4908c1c89a4be9d0a class=toggle>
<label for=section-08afbeb294c43ca4908c1c89a4be9d0a class="flex justify-between"><a href=/tech-book/docs/algorithms/two-pointers/>Two Pointers & Sliding Window</a></label></li><li><input type=checkbox id=section-ef36c7c4f7e0dec6525068c3c409100c class=toggle>
<label for=section-ef36c7c4f7e0dec6525068c3c409100c class="flex justify-between"><a href=/tech-book/docs/algorithms/medium/>Medium Complexity</a></label></li></ul></li><li><input type=checkbox id=section-8c7c5c4a8382299873178820b1d91be1 class=toggle checked>
<label for=section-8c7c5c4a8382299873178820b1d91be1 class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/>Data Center Networking for AI Clusters</a></label><ul><li><input type=checkbox id=section-433ccede4154db01f6c601940d2949e0 class=toggle>
<label for=section-433ccede4154db01f6c601940d2949e0 class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/1-ai-ml-networking/>AI/ML Networking</a></label></li><li><input type=checkbox id=section-65f71f2455a94ea3d1143666d556b0ed class=toggle>
<label for=section-65f71f2455a94ea3d1143666d556b0ed class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/2-ai-deep-learning-basics/>Deep Learning Basics | Artificial Neuron</a></label></li><li><input type=checkbox id=section-f11d3aa2e337730e1e3345f8530fe134 class=toggle>
<label for=section-f11d3aa2e337730e1e3345f8530fe134 class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/3-challenges-in-ai-fabric/>Challenges in AI Fabric Design</a></label></li><li><input type=checkbox id=section-e28420ec7507646128f3695b6f9badbb class=toggle>
<label for=section-e28420ec7507646128f3695b6f9badbb class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/4-congestion-avoidance-in-ai-fabric/>Congestion Avoidance in AI Fabric</a></label></li><li><input type=checkbox id=section-67bb7cbb1dd95e59f8515201219c090f class=toggle>
<label for=section-67bb7cbb1dd95e59f8515201219c090f class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/5-load-balancing-in-ai-fabric/>Load Balancing in AI Fabric</a></label></li><li><input type=checkbox id=section-adb8e29f405ea627a0e41d43e94e3453 class=toggle>
<label for=section-adb8e29f405ea627a0e41d43e94e3453 class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/7-rail-desings-in-gpu-fabric/>Load Balancing in AI Fabric</a></label></li><li><input type=checkbox id=section-5f3e07f6cf7921e5291ff7894e06b19b class=toggle checked>
<label for=section-5f3e07f6cf7921e5291ff7894e06b19b class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/6-backend-network-topologies-for-ai-fabric/ class=active>Backend Network Topologies for AI Fabrics</a></label></li></ul></li><li><input type=checkbox id=section-3272b2d28b2b247027bf478619ca416f class=toggle>
<label for=section-3272b2d28b2b247027bf478619ca416f class="flex justify-between"><a href=/tech-book/docs/data-center/>Data Center Tips</a></label><ul><li><input type=checkbox id=section-984f932c7aba4f0a1841e8413165c947 class=toggle>
<label for=section-984f932c7aba4f0a1841e8413165c947 class="flex justify-between"><a href=/tech-book/docs/data-center/data-center-ethernet/>Data Center Ethernet</a></label></li><li><input type=checkbox id=section-bc5ac88940153a700e63e3be886c63cc class=toggle>
<label for=section-bc5ac88940153a700e63e3be886c63cc class="flex justify-between"><a href=/tech-book/docs/data-center/data-center-technologies/>Data Center Technologies</a></label></li><li><input type=checkbox id=section-86fde1ddf43700ef8191e11c59a82cf1 class=toggle>
<label for=section-86fde1ddf43700ef8191e11c59a82cf1 class="flex justify-between"><a href=/tech-book/docs/data-center/data-center-network-virtualization/>Network Virtualization in Cloud Data Centers</a></label></li></ul></li><li><input type=checkbox id=section-ddf784688e0c6d5abb681d2c57851559 class=toggle>
<label for=section-ddf784688e0c6d5abb681d2c57851559 class="flex justify-between"><a href=/tech-book/docs/manageability/>Manageability</a></label><ul><li><input type=checkbox id=section-33ac730897af6feca81c1fdb7869c57e class=toggle>
<label for=section-33ac730897af6feca81c1fdb7869c57e class="flex justify-between"><a href=/tech-book/docs/manageability/why-grpc-on-http2/>gRPC on HTTP/2</a></label></li></ul></li><li><input type=checkbox id=section-c14ae944424668ef125e10cd791a3d3d class=toggle>
<label for=section-c14ae944424668ef125e10cd791a3d3d class="flex justify-between"><a href=/tech-book/docs/networking-tips/>Networking Tips</a></label><ul><li><input type=checkbox id=section-78fdf21c03c55935d3146441b06faf3e class=toggle>
<label for=section-78fdf21c03c55935d3146441b06faf3e class="flex justify-between"><a href=/tech-book/docs/networking-tips/dns/>DNS Overview</a></label></li><li><input type=checkbox id=section-bbcb027a658dc7a2333d59078ee507f9 class=toggle>
<label for=section-bbcb027a658dc7a2333d59078ee507f9 class="flex justify-between"><a href=/tech-book/docs/networking-tips/ecmp/>ECMP Load Balancing</a></label></li><li><input type=checkbox id=section-3b39b86f18b3451e5b8a1b81c369549c class=toggle>
<label for=section-3b39b86f18b3451e5b8a1b81c369549c class="flex justify-between"><a href=/tech-book/docs/networking-tips/ip-fragmentation/>IP Fragmentation - IPv4 & IPv6</a></label></li><li><input type=checkbox id=section-c6d06c54cc91b0bc3f948d33b437fa8b class=toggle>
<label for=section-c6d06c54cc91b0bc3f948d33b437fa8b class="flex justify-between"><a href=/tech-book/docs/networking-tips/ip-tos-dscp/>IP Precedence And TOS | DSCP</a></label></li><li><input type=checkbox id=section-5f3260c76dd37177e3f89057bfe520ae class=toggle>
<label for=section-5f3260c76dd37177e3f89057bfe520ae class="flex justify-between"><a href=/tech-book/docs/networking-tips/traceroute/>Linux traceroute tool</a></label></li><li><input type=checkbox id=section-a26cc3fafb36cbc55527adf39ec83849 class=toggle>
<label for=section-a26cc3fafb36cbc55527adf39ec83849 class="flex justify-between"><a href=/tech-book/docs/networking-tips/mlag/>Multi Chassis Link Aggregation Basics</a></label></li><li><input type=checkbox id=section-36409a0abcb2d4d4baf2e0b682d1a5dd class=toggle>
<label for=section-36409a0abcb2d4d4baf2e0b682d1a5dd class="flex justify-between"><a href=/tech-book/docs/networking-tips/qos/>QoS</a></label></li><li><input type=checkbox id=section-db41e3547d316b01acf9a0ce9c04ef34 class=toggle>
<label for=section-db41e3547d316b01acf9a0ce9c04ef34 class="flex justify-between"><a href=/tech-book/docs/networking-tips/spine-leaf-arch/>Spine-leaf Architecture Basics</a></label></li><li><input type=checkbox id=section-0eb0d409074a76b8cb02624655e2434d class=toggle>
<label for=section-0eb0d409074a76b8cb02624655e2434d class="flex justify-between"><a href=/tech-book/docs/networking-tips/tcp-congestion/>TCP Congestion Control</a></label></li><li><input type=checkbox id=section-3d1710947b3c5be1a1cc33d88fcc6f54 class=toggle>
<label for=section-3d1710947b3c5be1a1cc33d88fcc6f54 class="flex justify-between"><a href=/tech-book/docs/networking-tips/tcp-data-transfer/>TCP Data Transfer</a></label></li></ul></li><li><input type=checkbox id=section-f0a392f2f083f28a4991336773716a63 class=toggle>
<label for=section-f0a392f2f083f28a4991336773716a63 class="flex justify-between"><a href=/tech-book/docs/optical-knowledge/>Optical Knowledge</a></label><ul><li><input type=checkbox id=section-18261b95a92d4fa86116243edca4e9fb class=toggle>
<label for=section-18261b95a92d4fa86116243edca4e9fb class="flex justify-between"><a href=/tech-book/docs/optical-knowledge/optical-breakout/>Optical Transceiver(Grey) & Breakout Model</a></label></li></ul></li><li><input type=checkbox id=section-a55840d746138b3d1fedb81acbccdded class=toggle>
<label for=section-a55840d746138b3d1fedb81acbccdded class="flex justify-between"><a href=/tech-book/docs/programming-tips/>Programming Tips</a></label><ul><li><input type=checkbox id=section-f929f1fe13cb5d6cc96ca3e98f9d9777 class=toggle>
<label for=section-f929f1fe13cb5d6cc96ca3e98f9d9777 class="flex justify-between"><a href=/tech-book/docs/programming-tips/c++/>C++ Tips</a></label></li></ul></li><li><input type=checkbox id=section-b35dbf5ebcd4c19926cf5a9aab6c7655 class=toggle>
<label for=section-b35dbf5ebcd4c19926cf5a9aab6c7655 class="flex justify-between"><a href=/tech-book/docs/systemdesign-tips/>SystemDesign-Tips</a></label><ul><li><input type=checkbox id=section-4a618bc3b0b30107f0cec6d3bd6c025f class=toggle>
<label for=section-4a618bc3b0b30107f0cec6d3bd6c025f class="flex justify-between"><a href=/tech-book/docs/systemdesign-tips/code-deployment-system/>Design A Code-Deployment System</a></label></li><li><input type=checkbox id=section-e600754306aa95ce9c5a72b5efec6d7a class=toggle>
<label for=section-e600754306aa95ce9c5a72b5efec6d7a class="flex justify-between"><a href=/tech-book/docs/systemdesign-tips/stock-broker/>Design A Stock-Broker System</a></label></li><li><input type=checkbox id=section-bfa7a29878f65cfbb179e491c1211fa8 class=toggle>
<label for=section-bfa7a29878f65cfbb179e491c1211fa8 class="flex justify-between"><a href=/tech-book/docs/systemdesign-tips/design-amazon/>Design Amazon</a></label></li><li><input type=checkbox id=section-5456837e25872f6352d861a9b5662cb1 class=toggle>
<label for=section-5456837e25872f6352d861a9b5662cb1 class="flex justify-between"><a href=/tech-book/docs/systemdesign-tips/design-slack/>Design Slack</a></label></li><li><input type=checkbox id=section-5a78d16f54536a400b654f17f917bce1 class=toggle>
<label for=section-5a78d16f54536a400b654f17f917bce1 class="flex justify-between"><a href=/tech-book/docs/systemdesign-tips/google-drive/>Google Drive - Design</a></label></li></ul></li></ul><ul><li><a href=/tech-book/posts/>Blog</a></li><li><a href=https://prasenjitmanna.com/ target=_blank rel=noopener>Prasenjit's Blog</a></li><li><a href=https://prasenjitmanna.com/tech-book/ target=_blank rel=noopener>Prasenjit - Tech Book</a></li><li><a href=https://prasenjitmanna.com/upskills/ target=_blank rel=noopener>Prasenjit - Upskills</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/tech-book/svg/menu.svg class=book-icon alt=Menu>
</label><strong>Backend Network Topologies for AI Fabrics</strong>
<label for=toc-control><img src=/tech-book/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><ul><li><a href=#backend-network-topologies-for-ai-fabrics>Backend Network Topologies for AI Fabrics</a><ul><li><a href=#shared-nic>Shared NIC</a></li><li><a href=#nic-per-gpu>NIC per GPU</a></li><li><a href=#design-scenarios>Design Scenarios</a><ul><li><a href=#single-rail-switch-design-with-dedicated-single-port-nics-per-gpu>Single Rail Switch Design with Dedicated, Single-Port NICs per GPU</a><ul><li><a href=#benefits>Benefits</a></li><li><a href=#drawbacks>Drawbacks</a></li></ul></li><li><a href=#dual-rail-switch-topology-with-dedicated-dual-port-nics-per-gpu>Dual-Rail Switch Topology with Dedicated, Dual-Port NICs per GPU</a><ul><li><a href=#benefits-1>Benefits</a></li><li><a href=#drawbacks-1>Drawbacks</a></li></ul></li><li><a href=#challenges-of-multi-chassis-link-aggregation-mlag>Challenges of Multi-Chassis Link Aggregation (MLAG)</a></li><li><a href=#vendor-specific-mlag-solutions>Vendor-Specific MLAG Solutions</a></li><li><a href=#standards-based-alternative-evpn-esi-multihoming>Standards-Based Alternative: EVPN ESI Multihoming</a></li><li><a href=#cross-rail-communication-over-nvlink-in-rail-only-topologies>Cross-Rail Communication over NVLink in Rail-Only Topologies</a></li></ul></li></ul></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h2 id=backend-network-topologies-for-ai-fabrics>Backend Network Topologies for AI Fabrics
<a class=anchor href=#backend-network-topologies-for-ai-fabrics>#</a></h2><p>Although there are best practices for AI Fabric backend networks, such as Data Center Quantized Congestion Control (DCQCN) for congestion avoidance, rail-optimized routed Clos fabrics, and Layer 2 Rail-Only topologies for small-scale implementations, each vendor offers its own validated design. This approach is beneficial because validated designs are thoroughly tested, and when you build your system based on the vendor’s recommendations, you receive full vendor support and avoid having to reinvent the wheel.</p><p>However, instead of focusing on any specific vendor’s design, this chapter explains general design principles for building a resilient, non-blocking, and lossless Ethernet backend network for AI workloads.</p><p>Before diving into backend network design, this chapter first provides a high-level overview of a GPU server based on NVIDIA H100 GPUs. The first section introduces a shared NIC architecture, where 8 GPUs share two NICs. The second section covers an architecture where each of the 8 GPUs has a dedicated NIC.</p><h3 id=shared-nic>Shared NIC
<a class=anchor href=#shared-nic>#</a></h3><p>Figure 13-1 illustrates a shared NIC approach. In this example setup, NVIDIA H100 GPUs 0–3 are connected to NVSwitch chips 1-1, 1-2, 1-3, and 1-4 on baseboard-1, while GPUs 4–7 are connected to NVSwitch chips 2-1, 2-2, 2-3, and 2-4 on baseboard-2. Each GPU connects to all four NVSwitch chips on its respective baseboard using a total of 18 NVLink 4 connections: 5 links to chip 1-1, 4 links to chip 1-2, 4 links to chip 1-3, and 5 links to chip 1-4.</p><p>The NVSwitch chips themselves are paired between the two baseboards. For example, chip 1-1 on baseboard-1 connects to chip 2-1 on baseboard-2 with four NVLink connections, chip 1-2 connects to chip 2-2, and so on. This design forms a fully connected crossbar topology across the entire system.</p><p>Thanks to this balanced pairing, GPU-to-GPU communication is very efficient whether the GPUs are located on the same baseboard or on different baseboards. Each GPU can achieve up to 900 GB/s of total GPU-to-GPU bandwidth at full NVLink 4 speed.</p><p>For inter-GPU server connection, GPUs are also connected to a shared NVIDIA ConnectX-7 200 GbE NIC through a PEX89144 PCIe Gen5 switch. Each GPU has a dedicated PCIe Gen5 x16 link to the switch, providing up to 64 GB/s of bidirectional bandwidth (32 GB/s in each direction) between the GPU and the switch. The ConnectX-7 (200Gbps) NIC is also connected to the same PCIe switch, enabling high-speed data transfers between remote GPUs and the NIC through the PCIe fabric.</p><p>While each GPU benefits from a high-bandwidth, low-latency PCIe connection to the switch, the NIC itself has a maximum network bandwidth of 200 GbE, which corresponds to roughly 25 GB/s. Therefore, the PCIe switch is not a bottleneck; instead, the NIC’s available bandwidth must be shared among all eight GPUs. In scenarios where multiple GPUs are sending or receiving data simultaneously, the NIC becomes the limiting factor, and the bandwidth is divided between the GPUs.</p><p>In real-world AI workloads, however, GPUs rarely saturate both the PCIe interface and the NIC at the same time. Data transfers between the GPUs and the NIC are often bursty and asynchronous, depending on the training or inference pipeline stage. For example, during deep learning training, large gradients might be exchanged periodically, but not every GPU constantly sends data at full speed. Additionally, many optimizations like gradient compression, pipeline parallelism, and overlapping computation with communication further reduce the likelihood of sustained full-speed congestion.</p><p>As a result, even though the NIC bandwidth must be shared, the shared ConnectX-7 design generally provides sufficient network performance for typical AI workloads without significantly impacting training or inference times.</p><p>In high-performance environments, such as large-scale training workloads or GPU communication across nodes, this shared setup can become a bottleneck. Latency may increase under load, and data transfer speeds can slow down. </p><p>Despite these challenges, the design is still useful in many cases. It is well-suited for development environments, smaller models, or setups where cost is a primary concern. If the workload does not require maximum GPU-to-network performance, sharing a NIC across GPUs can be a reasonable and efficient solution. However, for optimal performance and full support for technologies like GPUDirect RDMA, it is better to use a dedicated NIC for each GPU. </p><p><a href=https://blogger.googleusercontent.com/img/a/AVvXsEg-WAuUm6Ar60MotlJsZAvVVXOeRcMRS6QYbwsimqEVjGEUz2S-Y2rYfj2PSObUNMcuDMbfEOgj0jFYaWMjWn6gGd9eUFFpWwQwTliI4CzcCItgE7_JFfER4tbzac84NaefCn8mvaBj2oTcLY1V3GljH0n3bqCGK9jJsrsuC2a5NKggQvd6X2nY7Nw6tY0><img src="https://blogger.googleusercontent.com/img/a/AVvXsEg-WAuUm6Ar60MotlJsZAvVVXOeRcMRS6QYbwsimqEVjGEUz2S-Y2rYfj2PSObUNMcuDMbfEOgj0jFYaWMjWn6gGd9eUFFpWwQwTliI4CzcCItgE7_JFfER4tbzac84NaefCn8mvaBj2oTcLY1V3GljH0n3bqCGK9jJsrsuC2a5NKggQvd6X2nY7Nw6tY0=w640-h356" alt></a></p><p><strong>Figure 13-1:</strong> <em>Shared NIC GPU Server.</em></p><h3 id=nic-per-gpu>NIC per GPU
<a class=anchor href=#nic-per-gpu>#</a></h3><p>Figure 13-2 builds on the shared NIC design from Figure 13-1 but takes a different approach. In this setup, each GPU has its own dedicated ConnectX-7 200 GbE NIC. All NICs are connected to the PCIe Gen5 switch, just like in the earlier setup, but now each GPU uses its own PCIe Gen5 x16 connection to a dedicated NIC. This design eliminates the need for NIC sharing and allows every GPU to use the full 64 GB/s PCIe bandwidth independently.</p><p>The biggest advantage of this design is in GPU-to-NIC communication. There is no bandwidth contention at the PCIe level, and each GPU can fully utilize RDMA and GPUDirect features with its own NIC. This setup improves network throughput and reduces latency, especially in multi-node training workloads where GPUs frequently send and receive large amounts of data over Ethernet. </p><p>The main drawback of this setup is cost. Adding one NIC per GPU increases both hardware costs and power consumption. It also requires more switch ports and cabling, which may affect system design. Still, these trade-offs are often acceptable in performance-critical environments.</p><p>This overall design reflects NVIDIA’s DGX and HGX architecture, where GPUs are fully interconnected using NVLink and NVSwitch and each GPU is typically paired with a dedicated ConnectX or BlueField NIC to maximize network performance. In addition, this configuration is well suited for rail-optimized backend networks, where consistent per-GPU network bandwidth and predictable east-west traffic patterns are important.</p><p><a href=https://blogger.googleusercontent.com/img/a/AVvXsEjm_EoGQewoQyUuVgoeQXrHVovZh-TLH1SDDfBeXWEpjhpFvTZN9JNqzUpoLOMbcqj4g57pQllY1gUI7l2Os19RXtfK1WNB9Lvu2pk5TNgOIoQK1Dn93dPx306Cb7VYp_zDdDo0U4Bgg3DC1I6FoZuZe_lZzrblxrASz7x-1yW5DpXZy26kU7Trpiqbjro><img src="https://blogger.googleusercontent.com/img/a/AVvXsEjm_EoGQewoQyUuVgoeQXrHVovZh-TLH1SDDfBeXWEpjhpFvTZN9JNqzUpoLOMbcqj4g57pQllY1gUI7l2Os19RXtfK1WNB9Lvu2pk5TNgOIoQK1Dn93dPx306Cb7VYp_zDdDo0U4Bgg3DC1I6FoZuZe_lZzrblxrASz7x-1yW5DpXZy26kU7Trpiqbjro=w640-h330" alt></a></p><p><strong>Figure 13-2:</strong> <em>Dedicated NIC per GPU.</em></p><p>Before moving to the design sections, it is worth mentioning that the need for a high-performance backend network, and how it is designed, is closely related to the size of the neural networks being used. Larger models require more GPU memory and often must be split across multiple GPUs or even servers. This increases the need for fast, low-latency communication between GPUs, which puts more pressure on the backend network.</p><p>Figure 13-3 shows a GPU server with 8 GPUs. Each GPU has 80 GB of memory, giving a total of 640 GB GPU memory. This kind of setup is common in high-performance AI clusters.</p><p>The figure also shows three examples of running large language models (LLMs) with different parameter sizes:</p><ul><li>8B model: This model has 8 billion parameters and needs only approximately 16 GB of memory. It fits on a single GPU if model parallelism is not required. </li><li>70B model: This larger model has 70 billion parameters and needs approximately 140 GB of memory. It cannot fit into one GPU, so it must use at least two GPUs. In this case, the GPUs communicate using intra-host GPU connections across NVLink.</li><li>405B model: This large model has 405 billion parameters and needs approximately 810 GB of memory. It does not fit into one server. Running this model requires at least 10 GPUs across multiple servers. The GPUs must use both intra-GPU connections inside a server and inter-GPU connections between servers.</li></ul><p>This figure highlights how model size directly affects memory needs, and the number of GPUs required. As models grow, parallelism and fast GPU interconnects become essential.</p><p><a href=https://blogger.googleusercontent.com/img/a/AVvXsEg8mcSSt4nvRNkIUoABij0pGaINQpeqv_q4P8xHWIXLTJQu4B12sAghIdx1QK2RNzKIYD2pAB6lWIZZC_lCLC3E-j0HiWfz1kU8z98ec9LSUnxj1FCo94V40CWRuHoV1PPrey9o81qsyOH9R7Dnk6qbdc3LsSJP_IqFmdsDAqqijJkW89nRKU6Qn4kilFU><img src="https://blogger.googleusercontent.com/img/a/AVvXsEg8mcSSt4nvRNkIUoABij0pGaINQpeqv_q4P8xHWIXLTJQu4B12sAghIdx1QK2RNzKIYD2pAB6lWIZZC_lCLC3E-j0HiWfz1kU8z98ec9LSUnxj1FCo94V40CWRuHoV1PPrey9o81qsyOH9R7Dnk6qbdc3LsSJP_IqFmdsDAqqijJkW89nRKU6Qn4kilFU=w640-h310" alt></a></p><p><strong>Figure 13-3:</strong> <em>Model Size and Required GPUs.</em></p><h3 id=design-scenarios>Design Scenarios
<a class=anchor href=#design-scenarios>#</a></h3><h4 id=single-rail-switch-design-with-dedicated-single-port-nics-per-gpu>Single Rail Switch Design with Dedicated, Single-Port NICs per GPU
<a class=anchor href=#single-rail-switch-design-with-dedicated-single-port-nics-per-gpu>#</a></h4><p>Figure 13-4 illustrates a single rail switch design. The switch interfaces are divided into three groups of eight 200 Gbps interface each. The first group of eight ports is reserved for Host-1, the second group for Host-2, and the third group for Host-3. Each host has eight GPUs, and each GPU is equipped with a dedicated, single-port NIC.</p><p>Within each group, ports are assigned to different VLANs to separate traffic into different logical rails. Specifically, the first port of each group belongs to the VLAN representing Rail-1, the second port belongs to Rail-2, and so on. This pattern continues across all three host groups.</p><h5 id=benefits>Benefits
<a class=anchor href=#benefits>#</a></h5><ul><li>Simplicity: The architecture is very easy to design, configure, and troubleshoot. A single switch and straightforward VLAN assignment simplify management.</li><li>Cost-Effectiveness: Only one switch is needed, reducing capital expenditure (CapEx) compared to dual-rail or redundant designs. Less hardware also means lower operational expenditure (OpEx), including reduced power, cooling, and maintenance costs. Additionally, fewer devices translate to lower subscription-based licensing fees and service contract costs, further improving the total cost of ownership.</li><li>Efficient Use of Resources: Ports are used efficiently by directly mapping each GPU’s NIC to a specific port on the switch, minimizing wasted capacity.</li><li>Low Latency within the Rail: Since all communications stay within the same switch, latency is minimized, benefiting tightly-coupled GPU workloads.</li><li>Sufficient for Smaller Deployments: In smaller clusters or test environments where absolute redundancy is not critical, this design is perfectly sufficient.</li></ul><h5 id=drawbacks>Drawbacks
<a class=anchor href=#drawbacks>#</a></h5><ul><li>No Redundancy: A single switch creates a single point of failure. If the switch fails, all GPU communications are lost.</li><li>Limited Scalability: Expanding beyond the available switch ports can be challenging. Adding more hosts or GPUs might require replacing the switch or redesigning the network.</li><li>Potential Oversubscription: With all GPUs sending and receiving traffic through the same switch, there’s a risk of oversubscription, especially under heavy AI workload patterns where network traffic bursts are common.</li><li>Difficult Maintenance: Software upgrades or hardware maintenance on the switch impact all connected hosts, making planned downtime more disruptive.</li><li>Not Suitable for High Availability (HA) Requirements: Critical AI workloads, especially in production environments, often require dual-rail (redundant) networking to meet high availability requirements. This design would not meet such standards.</li></ul><p>Single rail designs are cost-efficient and simple but lack redundancy and scalability, making them best suited for small or non-critical AI deployments.</p><p><a href=https://blogger.googleusercontent.com/img/a/AVvXsEgmz9x5QJyyHI33rcSf5lduAzPZAqBgsNpUGbahHXM2U56wCjVMQT9-boWY2M9G9_4x8VWJJUblYeWnGzh2nY0Qhy0esRB_I_mmEAbcZz19uGoMwd0n_fhwm5HVjIF5Yc74iY_JMuhDciXay8Ys7h1Xnc5Rvid2iZfv11g5PbwGvGi5qG_LIBMgFJd406g><img src="https://blogger.googleusercontent.com/img/a/AVvXsEgmz9x5QJyyHI33rcSf5lduAzPZAqBgsNpUGbahHXM2U56wCjVMQT9-boWY2M9G9_4x8VWJJUblYeWnGzh2nY0Qhy0esRB_I_mmEAbcZz19uGoMwd0n_fhwm5HVjIF5Yc74iY_JMuhDciXay8Ys7h1Xnc5Rvid2iZfv11g5PbwGvGi5qG_LIBMgFJd406g=w640-h334" alt></a></p><p><strong>Figure 13-4:</strong> <em>Single Rail Switch Design: GPU with Single Port NIC.</em></p><h4 id=dual-rail-switch-topology-with-dedicated-dual-port-nics-per-gpu>Dual-Rail Switch Topology with Dedicated, Dual-Port NICs per GPU
<a class=anchor href=#dual-rail-switch-topology-with-dedicated-dual-port-nics-per-gpu>#</a></h4><p>In this topology, each host contains 8 GPUs, and each GPU has a dedicated dual-port NIC. The NICs are connected across two independent Rail switches equipped with 200 Gbps interfaces. This design ensures that every GPU has redundant network connectivity through separate switches, maximizing performance, resiliency, and failover capabilities.</p><p>Each Rail switch independently connects to one port of each NIC, creating a dual-homed connection per GPU. To ensure seamless operations and redundancy, the two switches must logically appear as a single device to the host NICs, even though they are physically distinct systems.</p><h5 id=benefits-1>Benefits
<a class=anchor href=#benefits-1>#</a></h5><ul><li>High Availability: The failure of a single switch, link, or NIC port does not isolate any GPU, maintaining system uptime.</li><li>Load Balancing: Traffic can be distributed across both switches, maximizing bandwidth utilization and reducing bottlenecks.</li><li>Scalability: Dual-rail architectures can be extended easily to larger deployments while maintaining predictable performance and redundancy.</li><li>Operational Flexibility: Maintenance can often be performed on one switch without service disruption.</li></ul><h5 id=drawbacks-1>Drawbacks
<a class=anchor href=#drawbacks-1>#</a></h5><ul><li>Higher Cost: Requires two switches, twice the number of cables, and dual-port NICs, increasing CapEx and OpEx.</li><li>Complexity: Managing a dual-rail environment introduces more design complexity due to Multi-Chassis Link Aggregation (MLAG).</li><li>Increased Power and Space Requirements: Two switches and more cabling demand more rack space, power, and cooling.</li></ul><h4 id=challenges-of-multi-chassis-link-aggregation-mlag>Challenges of Multi-Chassis Link Aggregation (MLAG)
<a class=anchor href=#challenges-of-multi-chassis-link-aggregation-mlag>#</a></h4><p>To create a logical channel between dual-port NICs and two switches, the switches must be presented as a single logical device to each NIC. Multi-Chassis Link Aggregation (MLAG) is often used for this purpose. MLAG allows a host to see both switch uplinks as part of the same LAG (Link Aggregation Group).</p><p>Another solution is to assign the two NIC ports to different VLANs without bundling them into a LAG, though this approach may limit bandwidth utilization and redundancy benefits compared to MLAG.</p><p>MLAG introduces several challenges:</p><ul><li><strong>MAC Address Synchronization:</strong> Both switches must advertise the same MAC address to the host NICs, allowing the two switches to appear as a single device.</li><li><strong>Port Identification:</strong> A common approach to building MLAG is to use the same interface numbers on both switches. Therefore, the system must be capable of uniquely identifying each member link internally.</li><li><strong>Control Plane Synchronization:</strong> The two switches must exchange state information (e.g., MAC learning, link status) to maintain a consistent and synchronized view of the network.</li><li><strong>Failover Handling:</strong> The switches must detect failures quickly and handle them gracefully without disrupting existing sessions, requiring robust failure detection and recovery mechanisms.</li></ul><h4 id=vendor-specific-mlag-solutions>Vendor-Specific MLAG Solutions
<a class=anchor href=#vendor-specific-mlag-solutions>#</a></h4><p>The following list shows some of the vendor proprietary MLAG:</p><ul><li>Cisco Virtual Port Channel (vPC): Cisco&rsquo;s vPC allows two Nexus switches to appear as one logical switch to connected devices, synchronizing MAC addresses and forwarding state.</li><li>Juniper Virtual Chassis / MC-LAG: Juniper offers Virtual Chassis and MC-LAG solutions, where two or more switches operate with a shared control plane, presenting themselves as a single switch to the host.</li><li>Arista MLAG: Arista Networks implements MLAG with a simple peer-link architecture, supporting independent control planes while synchronizing forwarding state.</li><li>NVIDIA/Mellanox MLAG: Mellanox switches also offer MLAG solutions, often optimized for HPC and AI workloads.</li></ul><h4 id=standards-based-alternative-evpn-esi-multihoming>Standards-Based Alternative: EVPN ESI Multihoming
<a class=anchor href=#standards-based-alternative-evpn-esi-multihoming>#</a></h4><p>Instead of vendor-specific MLAG, a standards-based approach using Ethernet Segment Identifier (ESI) Multihoming under BGP EVPN can be used. In this model:</p><ul><li>Switches advertise shared Ethernet segments (ESIs) to the host over BGP EVPN.</li><li>Hosts see multiple physical links but treat them as part of a logical redundant connection.</li><li>EVPN ESI Multihoming allows for interoperable solutions across vendors, but typically adds more complexity to the control plane compared to simple MLAG setups.</li></ul><p><a href=https://blogger.googleusercontent.com/img/a/AVvXsEjngPlj9kSLMNLpKnv0zGaKFrSI1mnmkplNERxcX2DJr5HJMfAnjszy6eIicaSoRvzJuFVz5DXJBqHHjZSXfdeYcsTAB5HhWyYwMGL-ZgMZ7HTQRKOIoJc8S3O4Hp_H6TlN-YAAK67DWd7k1n-mvYZbnhzmczXXpGgB-3de2h3MU0WsgfdpnslhMj61Ygw><img src="https://blogger.googleusercontent.com/img/a/AVvXsEjngPlj9kSLMNLpKnv0zGaKFrSI1mnmkplNERxcX2DJr5HJMfAnjszy6eIicaSoRvzJuFVz5DXJBqHHjZSXfdeYcsTAB5HhWyYwMGL-ZgMZ7HTQRKOIoJc8S3O4Hp_H6TlN-YAAK67DWd7k1n-mvYZbnhzmczXXpGgB-3de2h3MU0WsgfdpnslhMj61Ygw=w640-h332" alt></a></p><p><strong>Figure 13-5:</strong> <em>Dual Rail Switch Design: GPU with Dual-Port NIC.</em></p><h4 id=cross-rail-communication-over-nvlink-in-rail-only-topologies>Cross-Rail Communication over NVLink in Rail-Only Topologies
<a class=anchor href=#cross-rail-communication-over-nvlink-in-rail-only-topologies>#</a></h4><p>In the introduced single- and dual-rail topologies (Figures 13-4 and 13-5), each GPU is connected to a dedicated NIC, and each NIC connects to a specific Rail switch. However, there is no direct cross-rail connection between the switches themselves — no additional spine layer interconnecting the rails. As a result, if a GPU needs to send data to a destination GPU that belongs to a different rail, special handling is required within the host before the data can exit over the network.</p><p>For example, consider a memory copy operation where GPU-2 (connected to Rail 3) on Host-1 needs to send data to GPU-3 (connected to Rail 4) on Host-2. Since GPU-2’s NIC is associated with Rail 3 and GPU-3 expects data arriving over Rail 4, the communication path must traverse multiple stages:</p><ol><li>Intra-Host Transfer: The data is first copied locally over NVLink from GPU-2 to GPU-3 within Host-1. NVLink provides a high-bandwidth, low-latency connection between GPUs inside the same server.</li><li>NIC Transmission: Once the data resides in GPU-3’s memory, it can be sent out through GPU-3’s NIC, which connects to Rail 4.</li><li>Inter-Host Transfer: The packet travels over Rail 4 through one of the Rail switches to reach Host-2.</li><li>Destination Reception: Finally, the data is delivered to GPU-3 on Host-2.</li></ol><p>This method ensures that each network link (and corresponding NIC) is used according to its assigned rail without needing direct switch-to-switch rail interconnects.</p><p>To coordinate and optimize such multi-step communication, NVIDIA Collective Communications Library (NCCL) plays a critical role. NCCL automatically handles GPU-to-GPU communication across multiple nodes and rails, selecting the appropriate path, initiating memory copies over NVLink, and scheduling transmissions over the correct NICs — all while maximizing bandwidth and minimizing latency. The upcoming chapter will explore NCCL in greater detail.</p><p>Figure 13-6 illustrates how the upcoming topology in Figure 13-7 maps NIC-to-Rail connections, transitioning from a switch interface-based view to a rail-based view. Figure 13-6 shows a partial interface layout of a Cisco Nexus 9348D-GX2A switch and how its interfaces are grouped into different rails as follows:</p><ul><li>Rail-1 Interfaces: 1, 4, 7, 10</li><li>Rail-2 Interfaces: 13, 16, 19, 22</li><li>Rail-3 Interfaces: 25, 28, 31, 34</li><li>Rail-4 Interfaces: 37, 40, 43, 46</li><li>Rail-5 Interfaces: 2, 5, 8, 11</li><li>Rail-6 Interfaces: 14, 17, 20, 23</li><li>Rail-7 Interfaces: 26, 29, 32, 35</li><li>Rail-8 Interfaces: 38, 41, 44, 47</li></ul><p>However, a port-based layout becomes extremely messy when describing larger implementations. Therefore, the common practice is to reference the rail number instead of individual switch interface identifiers.</p><p><a href=https://blogger.googleusercontent.com/img/a/AVvXsEhplkUJDuU7yYzXLi1HlP_2kAmn7Yx4JZOPuoT2wRpHKTx2qNsPUmphVzMxQUsXhuzLdRsgF4SZsnr0CHb8K4AIfsF9yS91IxZ4zi8u_Djokux9K5puWgf2EjzWVXWTbej2XRzc_5ssvw8VfHW86mAbY2q6BLzmyl7lYlCh_Icp93dwMoAuLmqZ5thxTZY><img src="https://blogger.googleusercontent.com/img/a/AVvXsEhplkUJDuU7yYzXLi1HlP_2kAmn7Yx4JZOPuoT2wRpHKTx2qNsPUmphVzMxQUsXhuzLdRsgF4SZsnr0CHb8K4AIfsF9yS91IxZ4zi8u_Djokux9K5puWgf2EjzWVXWTbej2XRzc_5ssvw8VfHW86mAbY2q6BLzmyl7lYlCh_Icp93dwMoAuLmqZ5thxTZY=w640-h362" alt></a></p><p><strong>Figure 13-6:</strong> <em>Interface Block to Rail Mapping.</em></p><p>Figure 13-7 provides an example showing how each NIC is now connected to a rail instead of being directly mapped to a specific physical interface. In this approach, each rail represents a logical group of physical interfaces, simplifying the overall design and making larger deployments easier to visualize and document.</p><p>In our example &ldquo;Host-Segment&rdquo; (an unofficial name), we have four hosts, each equipped with eight GPUs — 32 GPUs in total. Each GPU has a dedicated 200 Gbps dual-port NIC. All GPUs are connected to two rail switches over a 2 × 200 Gbps MLAG, providing 400 Gbps of transmission speed per GPU.</p><p><a href=https://blogger.googleusercontent.com/img/a/AVvXsEhUyQ0OtKPQ-J-AjurIM97tTu3MCso6Ciz_UP7CL-8SQ69_UM8JEgka0BwjlLyoyStOe30hJiIbnZHfcQ8RVAvyqTCfIEU9NUCyFoCtjkqvIzH6C91HwkBEZzPAgLiCaqT53aEZdAhb2w3F-VjiwGgJMG2KyDmnjaMwDlhshvbflndXd-hV8M4eVRtJwKs><img src=https://blogger.googleusercontent.com/img/a/AVvXsEhUyQ0OtKPQ-J-AjurIM97tTu3MCso6Ciz_UP7CL-8SQ69_UM8JEgka0BwjlLyoyStOe30hJiIbnZHfcQ8RVAvyqTCfIEU9NUCyFoCtjkqvIzH6C91HwkBEZzPAgLiCaqT53aEZdAhb2w3F-VjiwGgJMG2KyDmnjaMwDlhshvbflndXd-hV8M4eVRtJwKs alt></a></p><p><strong>Figure 13-7:</strong> <em>Example Figure of Connecting 32 Dual-Port NICs 8 Rails on 2 Switches.</em></p><p>Figure 13-8 shows how multiple Host-Segments can be connected. The figure illustrates a simplified two-tier, three-stage Clos fabric topology, where full-mesh Layer 3 links are established between the four Rail switches (leaf switches) and the Spine switches. The figure also presents the link capacity calculations. Each Rail switch has 32 × 100 Gbps connections to the hosts, providing a total downlink capacity of 3.2 Tbps.</p><p>Since oversubscription is generally not preferred in GPU clusters — to maintain high performance and low latency — the uplink capacity from each Rail switch to the Spine layer must also match 3.2 Tbps. To achieve this, each Rail switch must have uplinks capable of an aggregate transfer rate of 3.2 Tbps. This can be implemented either by using native 800 Gbps interfaces or by forming a logical Layer 3 port channel composed of two 400 Gbps links per Spine connection. Additionally, Inter-Switch capacity can be increased by adding more switches in the Spine layer. This is one of the benefits of a Clos fabric: the capacity can be scaled without the need to replace 400 Gbps interfaces with 800 Gbps interfaces, for example.</p><p>This topology forms a Pod and supports 64 GPUs in total and provides a non-blocking architecture, ensuring optimal east-west traffic performance between GPUs across different Host-Segments.</p><p>In network design, the terms &ldquo;two-tier&rdquo; and &ldquo;three-stage&rdquo; Clos fabric describe different aspects of the same overall topology. &ldquo;Two-tier&rdquo; focuses on the physical switch layers (typically Leaf and Spine) and describes the depth of the topology, offering a hierarchy view of the architecture. Essentially, it&rsquo;s concerned with how many switching layers are present. On the other hand, three-stage Clos describes the logical data path a packet follows when moving between endpoints: Leaf–Spine–Leaf. It focuses on how data moves through the network and the stages traffic flows through. Therefore, while a two-tier topology refers to the physical switch structure, a three-stage Clos describes the logical path taken by packets, which crosses through three stages: Leaf, Spine, and Leaf. These two perspectives are complementary, not contradictory, and together they provide a complete view of the Clos network design.</p><p><a href=https://blogger.googleusercontent.com/img/a/AVvXsEjmuI-t16WjNMAH-U35zRBNLFrFztZZsxeIKk22N_AwvAlzDqtm5OAopKjBcUuXMJq19H8g63v22QxGsDvlqultATYylR_3wolQ5-P_HaB4GkYyBJGF1JznYj49pDt9anMhThes74bORPexfM2P3VijocoGI9iOE-w4K6GrWBnkTqlMG9aghP_5PeFzXBs><img src=https://blogger.googleusercontent.com/img/a/AVvXsEjmuI-t16WjNMAH-U35zRBNLFrFztZZsxeIKk22N_AwvAlzDqtm5OAopKjBcUuXMJq19H8g63v22QxGsDvlqultATYylR_3wolQ5-P_HaB4GkYyBJGF1JznYj49pDt9anMhThes74bORPexfM2P3VijocoGI9iOE-w4K6GrWBnkTqlMG9aghP_5PeFzXBs alt></a></p><p><strong>Figure 13-8:</strong> <em>AI fabric – Pod Design.</em></p><p>Figure 13-9 extends the previous example by adding a second 64-GPU Pod, creating a larger multi-Pod architecture. To interconnect the two Pods, four Super-Spine switches are introduced, forming an additional aggregation layer above the Spine layer. Each Pod retains its internal two-tier Clos fabric structure, with Rail switches fully meshed to the Spine switches as described earlier. The Spine switches from both Pods are then connected northbound to the Super-Spine switches over Layer 3 links.</p><p>Due to the introduction of the Super-Spine layer, the complete system now forms a three-tier, five-stage Clos topology. This design supports scalable expansion while maintaining predictable latency and high bandwidth between GPUs across different Pods. Similar to the Rail-to-Spine design, maintaining a non-blocking architecture between the Spine and Super-Spine layers is critical. Each Spine switch aggregates 3.2 Tbps of traffic from its Rail switches; therefore, the uplink capacity from each Spine to the Super-Spine layer must also be 3.2 Tbps.</p><p>This can be achieved either by using native 800 Gbps links or logical Layer 3 port channels composed of two 400 Gbps links per Super-Spine connection. All Spine switches are fully meshed with all Super-Spine switches to ensure high availability and consistent bandwidth. This architecture enables seamless east-west traffic between GPUs located in different Pods, ensuring that inter-Pod communication maintains the same non-blocking performance as intra-Pod traffic.</p><p><a href=https://blogger.googleusercontent.com/img/a/AVvXsEjg85GATQTVBTrrSPCPBKoI05YwttuKvXDlQgve8zsIBQS9sts_znSuStCGXktakwu1ODPqdLe3ROXA-U0v4JRngDvrIclLtkdf-tqWMLetu4nys8Jr7786mZHjGGa4OMPtJo4jSxo-fD83P6c1MMF_CMOqPbW-8V0Oer1GmdGrb3CxiCMP8I7p7q_s5Cw><img src="https://blogger.googleusercontent.com/img/a/AVvXsEjg85GATQTVBTrrSPCPBKoI05YwttuKvXDlQgve8zsIBQS9sts_znSuStCGXktakwu1ODPqdLe3ROXA-U0v4JRngDvrIclLtkdf-tqWMLetu4nys8Jr7786mZHjGGa4OMPtJo4jSxo-fD83P6c1MMF_CMOqPbW-8V0Oer1GmdGrb3CxiCMP8I7p7q_s5Cw=w640-h362" alt></a></p><p><strong>Figure 13-9:</strong> <em>AI fabric – Multi-Pod Design.</em></p><p>In this chapter, we focus mainly on different topology options, such as Single Rail with Single-Port GPU NIC, Dual Rail Switch with Dual-Port GPU NIC, Cross-Rail Over Layer 3 Clos fabric, and finally, Inter-Pod architecture. The next chapter will delve more in-depth into the technical solutions and challenges.</p><p><strong>References:</strong></p><ul><li><a href=https://nwktimes.blogspot.com/2025/04/ai-fabric-backend-network-topologies.html>https://nwktimes.blogspot.com/2025/04/ai-fabric-backend-network-topologies.html</a></li></ul></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/prmanna/tech-book/commit/709b3461484c2c69d7e211a3794b24bb41141f06 title='Last modified by Prasenjit Manna | May 5, 2025' target=_blank rel=noopener><img src=/tech-book/svg/calendar.svg class=book-icon alt>
<span>May 5, 2025</span></a></div></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><ul><li><a href=#backend-network-topologies-for-ai-fabrics>Backend Network Topologies for AI Fabrics</a><ul><li><a href=#shared-nic>Shared NIC</a></li><li><a href=#nic-per-gpu>NIC per GPU</a></li><li><a href=#design-scenarios>Design Scenarios</a><ul><li><a href=#single-rail-switch-design-with-dedicated-single-port-nics-per-gpu>Single Rail Switch Design with Dedicated, Single-Port NICs per GPU</a><ul><li><a href=#benefits>Benefits</a></li><li><a href=#drawbacks>Drawbacks</a></li></ul></li><li><a href=#dual-rail-switch-topology-with-dedicated-dual-port-nics-per-gpu>Dual-Rail Switch Topology with Dedicated, Dual-Port NICs per GPU</a><ul><li><a href=#benefits-1>Benefits</a></li><li><a href=#drawbacks-1>Drawbacks</a></li></ul></li><li><a href=#challenges-of-multi-chassis-link-aggregation-mlag>Challenges of Multi-Chassis Link Aggregation (MLAG)</a></li><li><a href=#vendor-specific-mlag-solutions>Vendor-Specific MLAG Solutions</a></li><li><a href=#standards-based-alternative-evpn-esi-multihoming>Standards-Based Alternative: EVPN ESI Multihoming</a></li><li><a href=#cross-rail-communication-over-nvlink-in-rail-only-topologies>Cross-Rail Communication over NVLink in Rail-Only Topologies</a></li></ul></li></ul></li></ul></li></ul></nav></div></aside></main></body></html>