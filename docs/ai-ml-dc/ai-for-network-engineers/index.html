<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  Content
  #

Introduction 
Artificial Neuron  
Weighted Sum for Pre-Activation Value  
ReLU Activation Function for Post-Activation  
Bias Term 
S-Shaped Functions – TANH and SIGMOID
Network Impact
Summary
References

  Introduction
  #

Artificial Intelligence (AI) is a broad term for solutions that aim to mimic the functions of the human brain. Machine Learning (ML), in turn, is a subset of AI, suitable for tasks like simple pattern recognition and prediction. Deep Learning (DL), the focus of this section, is a subset of ML that leverages algorithms to extract meaningful patterns from data. Unlike ML, DL does not necessarily require human intervention, such as providing structured, labeled datasets (e.g., 1,000 bird images labeled as “bird” and 1,000 cat images labeled as “cat”). "><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://prasenjitmanna.com/tech-book/docs/ai-ml-dc/ai-for-network-engineers/"><meta property="og:site_name" content="Technical Book"><meta property="og:title" content="Deep Learning Basics | Artificial Neuron"><meta property="og:description" content=" Content # Introduction Artificial Neuron Weighted Sum for Pre-Activation Value ReLU Activation Function for Post-Activation Bias Term S-Shaped Functions – TANH and SIGMOID Network Impact
Summary
References
Introduction # Artificial Intelligence (AI) is a broad term for solutions that aim to mimic the functions of the human brain. Machine Learning (ML), in turn, is a subset of AI, suitable for tasks like simple pattern recognition and prediction. Deep Learning (DL), the focus of this section, is a subset of ML that leverages algorithms to extract meaningful patterns from data. Unlike ML, DL does not necessarily require human intervention, such as providing structured, labeled datasets (e.g., 1,000 bird images labeled as “bird” and 1,000 cat images labeled as “cat”). "><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:modified_time" content="2025-04-19T23:09:23+05:30"><title>Deep Learning Basics | Artificial Neuron | Technical Book</title>
<link rel=manifest href=/tech-book/manifest.json><link rel=icon href=/tech-book/favicon.png><link rel=canonical href=https://prasenjitmanna.com/tech-book/docs/ai-ml-dc/ai-for-network-engineers/><link rel=stylesheet href=/tech-book/book.min.a61cdb2979f3c2bece54ef69131fba427dd57d55c232d3bb5fdb62ac41aa8354.css integrity="sha256-phzbKXnzwr7OVO9pEx+6Qn3VfVXCMtO7X9tirEGqg1Q=" crossorigin=anonymous><script defer src=/tech-book/fuse.min.js></script><script defer src=/tech-book/en.search.min.7bcaf40247fa29eb7e517d52f50393f510f8065698f60a88a4417a637f0b3ad3.js integrity="sha256-e8r0Akf6Ket+UX1S9QOT9RD4BlaY9gqIpEF6Y38LOtM=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/tech-book/><img src=/tech-book/logo.png alt=Logo><span>Technical Book</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><input type=checkbox id=section-99f552133860bc21797a47fe73e93434 class=toggle>
<label for=section-99f552133860bc21797a47fe73e93434 class="flex justify-between"><a href=/tech-book/docs/5g/>5G</a></label><ul><li><input type=checkbox id=section-dfc790b73acb0410a0114547cbf5af32 class=toggle>
<label for=section-dfc790b73acb0410a0114547cbf5af32 class="flex justify-between"><a href=/tech-book/docs/5g/5g-intro/>An Overview of 5G Networking</a></label></li></ul></li><li><input type=checkbox id=section-7c4a3b16aeeb5b7194b232c1eef2f4fb class=toggle>
<label for=section-7c4a3b16aeeb5b7194b232c1eef2f4fb class="flex justify-between"><a href=/tech-book/docs/algorithms/>Algorithms</a></label><ul><li><input type=checkbox id=section-8956d82fbe6869140758f7e8679174bc class=toggle>
<label for=section-8956d82fbe6869140758f7e8679174bc class="flex justify-between"><a href=/tech-book/docs/algorithms/breadth-first-search/>Breadth First Search</a></label></li><li><input type=checkbox id=section-5a049cfad1740f3fd30565524385fa57 class=toggle>
<label for=section-5a049cfad1740f3fd30565524385fa57 class="flex justify-between"><a href=/tech-book/docs/algorithms/depth-first-search/>Depth First Search</a></label></li><li><input type=checkbox id=section-ebc049f26d82165be8c6f1f9e504e799 class=toggle>
<label for=section-ebc049f26d82165be8c6f1f9e504e799 class="flex justify-between"><a href=/tech-book/docs/algorithms/easy/>Easy Complexity</a></label></li><li><input type=checkbox id=section-1071946392bd1f431993e950147fa054 class=toggle>
<label for=section-1071946392bd1f431993e950147fa054 class="flex justify-between"><a href=/tech-book/docs/algorithms/priority-queue-and-heap/>Priority Queue and Heap</a></label></li><li><input type=checkbox id=section-08afbeb294c43ca4908c1c89a4be9d0a class=toggle>
<label for=section-08afbeb294c43ca4908c1c89a4be9d0a class="flex justify-between"><a href=/tech-book/docs/algorithms/two-pointers/>Two Pointers & Sliding Window</a></label></li><li><input type=checkbox id=section-ef36c7c4f7e0dec6525068c3c409100c class=toggle>
<label for=section-ef36c7c4f7e0dec6525068c3c409100c class="flex justify-between"><a href=/tech-book/docs/algorithms/medium/>Medium Complexity</a></label></li></ul></li><li><input type=checkbox id=section-8c7c5c4a8382299873178820b1d91be1 class=toggle checked>
<label for=section-8c7c5c4a8382299873178820b1d91be1 class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/>Data Center Networking for AI Clusters</a></label><ul><li><input type=checkbox id=section-cd3a69c68b09887f63fced84e24740c3 class=toggle>
<label for=section-cd3a69c68b09887f63fced84e24740c3 class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/ai-ml-networking/>AI/ML Networking</a></label></li><li><input type=checkbox id=section-ff36d5b7070fdf6652042949a0c53536 class=toggle checked>
<label for=section-ff36d5b7070fdf6652042949a0c53536 class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/ai-for-network-engineers/ class=active>Deep Learning Basics | Artificial Neuron</a></label></li></ul></li><li><input type=checkbox id=section-3272b2d28b2b247027bf478619ca416f class=toggle>
<label for=section-3272b2d28b2b247027bf478619ca416f class="flex justify-between"><a href=/tech-book/docs/data-center/>Data Center Tips</a></label><ul><li><input type=checkbox id=section-984f932c7aba4f0a1841e8413165c947 class=toggle>
<label for=section-984f932c7aba4f0a1841e8413165c947 class="flex justify-between"><a href=/tech-book/docs/data-center/data-center-ethernet/>Data Center Ethernet</a></label></li><li><input type=checkbox id=section-bc5ac88940153a700e63e3be886c63cc class=toggle>
<label for=section-bc5ac88940153a700e63e3be886c63cc class="flex justify-between"><a href=/tech-book/docs/data-center/data-center-technologies/>Data Center Technologies</a></label></li><li><input type=checkbox id=section-86fde1ddf43700ef8191e11c59a82cf1 class=toggle>
<label for=section-86fde1ddf43700ef8191e11c59a82cf1 class="flex justify-between"><a href=/tech-book/docs/data-center/data-center-network-virtualization/>Network Virtualization in Cloud Data Centers</a></label></li></ul></li><li><input type=checkbox id=section-ddf784688e0c6d5abb681d2c57851559 class=toggle>
<label for=section-ddf784688e0c6d5abb681d2c57851559 class="flex justify-between"><a href=/tech-book/docs/manageability/>Manageability</a></label><ul><li><input type=checkbox id=section-33ac730897af6feca81c1fdb7869c57e class=toggle>
<label for=section-33ac730897af6feca81c1fdb7869c57e class="flex justify-between"><a href=/tech-book/docs/manageability/why-grpc-on-http2/>gRPC on HTTP/2</a></label></li></ul></li><li><input type=checkbox id=section-c14ae944424668ef125e10cd791a3d3d class=toggle>
<label for=section-c14ae944424668ef125e10cd791a3d3d class="flex justify-between"><a href=/tech-book/docs/networking-tips/>Networking Tips</a></label><ul><li><input type=checkbox id=section-78fdf21c03c55935d3146441b06faf3e class=toggle>
<label for=section-78fdf21c03c55935d3146441b06faf3e class="flex justify-between"><a href=/tech-book/docs/networking-tips/dns/>DNS Overview</a></label></li><li><input type=checkbox id=section-bbcb027a658dc7a2333d59078ee507f9 class=toggle>
<label for=section-bbcb027a658dc7a2333d59078ee507f9 class="flex justify-between"><a href=/tech-book/docs/networking-tips/ecmp/>ECMP Load Balancing</a></label></li><li><input type=checkbox id=section-3b39b86f18b3451e5b8a1b81c369549c class=toggle>
<label for=section-3b39b86f18b3451e5b8a1b81c369549c class="flex justify-between"><a href=/tech-book/docs/networking-tips/ip-fragmentation/>IP Fragmentation - IPv4 & IPv6</a></label></li><li><input type=checkbox id=section-c6d06c54cc91b0bc3f948d33b437fa8b class=toggle>
<label for=section-c6d06c54cc91b0bc3f948d33b437fa8b class="flex justify-between"><a href=/tech-book/docs/networking-tips/ip-tos-dscp/>IP Precedence And TOS | DSCP</a></label></li><li><input type=checkbox id=section-5f3260c76dd37177e3f89057bfe520ae class=toggle>
<label for=section-5f3260c76dd37177e3f89057bfe520ae class="flex justify-between"><a href=/tech-book/docs/networking-tips/traceroute/>Linux traceroute tool</a></label></li><li><input type=checkbox id=section-a26cc3fafb36cbc55527adf39ec83849 class=toggle>
<label for=section-a26cc3fafb36cbc55527adf39ec83849 class="flex justify-between"><a href=/tech-book/docs/networking-tips/mlag/>Multi Chassis Link Aggregation Basics</a></label></li><li><input type=checkbox id=section-36409a0abcb2d4d4baf2e0b682d1a5dd class=toggle>
<label for=section-36409a0abcb2d4d4baf2e0b682d1a5dd class="flex justify-between"><a href=/tech-book/docs/networking-tips/qos/>QoS</a></label></li><li><input type=checkbox id=section-db41e3547d316b01acf9a0ce9c04ef34 class=toggle>
<label for=section-db41e3547d316b01acf9a0ce9c04ef34 class="flex justify-between"><a href=/tech-book/docs/networking-tips/spine-leaf-arch/>Spine-leaf Architecture Basics</a></label></li><li><input type=checkbox id=section-0eb0d409074a76b8cb02624655e2434d class=toggle>
<label for=section-0eb0d409074a76b8cb02624655e2434d class="flex justify-between"><a href=/tech-book/docs/networking-tips/tcp-congestion/>TCP Congestion Control</a></label></li><li><input type=checkbox id=section-3d1710947b3c5be1a1cc33d88fcc6f54 class=toggle>
<label for=section-3d1710947b3c5be1a1cc33d88fcc6f54 class="flex justify-between"><a href=/tech-book/docs/networking-tips/tcp-data-transfer/>TCP Data Transfer</a></label></li></ul></li><li><input type=checkbox id=section-f0a392f2f083f28a4991336773716a63 class=toggle>
<label for=section-f0a392f2f083f28a4991336773716a63 class="flex justify-between"><a href=/tech-book/docs/optical-knowledge/>Optical Knowledge</a></label><ul><li><input type=checkbox id=section-18261b95a92d4fa86116243edca4e9fb class=toggle>
<label for=section-18261b95a92d4fa86116243edca4e9fb class="flex justify-between"><a href=/tech-book/docs/optical-knowledge/optical-breakout/>Optical Transceiver(Grey) & Breakout Model</a></label></li></ul></li><li><input type=checkbox id=section-a55840d746138b3d1fedb81acbccdded class=toggle>
<label for=section-a55840d746138b3d1fedb81acbccdded class="flex justify-between"><a href=/tech-book/docs/programming-tips/>Programming Tips</a></label><ul><li><input type=checkbox id=section-f929f1fe13cb5d6cc96ca3e98f9d9777 class=toggle>
<label for=section-f929f1fe13cb5d6cc96ca3e98f9d9777 class="flex justify-between"><a href=/tech-book/docs/programming-tips/c++/>C++ Tips</a></label></li></ul></li><li><input type=checkbox id=section-b35dbf5ebcd4c19926cf5a9aab6c7655 class=toggle>
<label for=section-b35dbf5ebcd4c19926cf5a9aab6c7655 class="flex justify-between"><a href=/tech-book/docs/systemdesign-tips/>SystemDesign-Tips</a></label><ul><li><input type=checkbox id=section-4a618bc3b0b30107f0cec6d3bd6c025f class=toggle>
<label for=section-4a618bc3b0b30107f0cec6d3bd6c025f class="flex justify-between"><a href=/tech-book/docs/systemdesign-tips/code-deployment-system/>Design A Code-Deployment System</a></label></li><li><input type=checkbox id=section-e600754306aa95ce9c5a72b5efec6d7a class=toggle>
<label for=section-e600754306aa95ce9c5a72b5efec6d7a class="flex justify-between"><a href=/tech-book/docs/systemdesign-tips/stock-broker/>Design A Stock-Broker System</a></label></li><li><input type=checkbox id=section-bfa7a29878f65cfbb179e491c1211fa8 class=toggle>
<label for=section-bfa7a29878f65cfbb179e491c1211fa8 class="flex justify-between"><a href=/tech-book/docs/systemdesign-tips/design-amazon/>Design Amazon</a></label></li><li><input type=checkbox id=section-5456837e25872f6352d861a9b5662cb1 class=toggle>
<label for=section-5456837e25872f6352d861a9b5662cb1 class="flex justify-between"><a href=/tech-book/docs/systemdesign-tips/design-slack/>Design Slack</a></label></li><li><input type=checkbox id=section-5a78d16f54536a400b654f17f917bce1 class=toggle>
<label for=section-5a78d16f54536a400b654f17f917bce1 class="flex justify-between"><a href=/tech-book/docs/systemdesign-tips/google-drive/>Google Drive - Design</a></label></li></ul></li></ul><ul><li><a href=/tech-book/posts/>Blog</a></li><li><a href=https://prasenjitmanna.com/ target=_blank rel=noopener>Prasenjit's Blog</a></li><li><a href=https://prasenjitmanna.com/tech-book/ target=_blank rel=noopener>Prasenjit - Tech Book</a></li><li><a href=https://prasenjitmanna.com/upskills/ target=_blank rel=noopener>Prasenjit - Upskills</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/tech-book/svg/menu.svg class=book-icon alt=Menu>
</label><strong>Deep Learning Basics | Artificial Neuron</strong>
<label for=toc-control><img src=/tech-book/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><ul><li><ul><li></li><li><a href=#introduction>Introduction</a></li><li><a href=#artificial-neuron>Artificial Neuron</a></li><li><a href=#network-impact>Network Impact</a></li><li><a href=#summary>Summary</a></li></ul></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h4 id=content>Content
<a class=anchor href=#content>#</a></h4><p>Introduction 
Artificial Neuron  
Weighted Sum for Pre-Activation Value  
ReLU Activation Function for Post-Activation  
Bias Term 
S-Shaped Functions – TANH and SIGMOID
Network Impact<br>Summary<br>References</p><h3 id=introduction>Introduction
<a class=anchor href=#introduction>#</a></h3><p>Artificial Intelligence (AI) is a broad term for solutions that aim to mimic the functions of the human brain. Machine Learning (ML), in turn, is a subset of AI, suitable for tasks like simple pattern recognition and prediction. Deep Learning (DL), the focus of this section, is a subset of ML that leverages algorithms to extract meaningful patterns from data. Unlike ML, DL does not necessarily require human intervention, such as providing structured, labeled datasets (e.g., 1,000 bird images labeled as “bird” and 1,000 cat images labeled as “cat”). </p><p>DL utilizes layered, hierarchical Deep Neural Networks (DNNs), where hidden and output layers consist of computational units, artificial neurons, which individually process input data. The nodes in the input layer pass the input data to the first hidden layer without performing any computations, which is why they are not considered neurons or computational units. Each neuron calculates a pre-activation value (z) based on the input received from the previous layer and then applies an activation function to this value, producing a post-activation output (ŷ) value. There are various DNN models, such as Feed-Forward Neural Networks (FNN), Convolutional Neural Networks (CNN), and Recurrent Neural Networks (RNN), each designed for different use cases. For example, FNNs are suitable for simple, structured tasks like handwritten digit recognition using the MNIST dataset [1], CNNs are effective for larger image recognition tasks such as with the CIFAR-10 dataset [2], and RNNs are commonly used for time-series forecasting, like predicting future sales based on historical sales data. </p><p>To provide accurate predictions based on input data, neural networks are trained using labeled datasets. The MNIST (Modified National Institute of Standards and Technology) dataset [1] contains 60,000 training and 10,000 test images of handwritten digits (grayscale, 28x28 pixels). The CIFAR-10 [2] dataset consists of 60,000 color images (32x32 pixels), with 50,000 training images and 10,000 test images, divided into 10 classes. The CIFAR-100 dataset [3], as the name implies, has 100 image classes, with each class containing 600 images (500 training and 100 test images per class). Once the test results reach the desired level, the neural network can be deployed to production.</p><p><a href=https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgylzz8OQkWjjhyBpYDBNRNezdgowWbuzeSXr_I5uSf6XnWq2y2ZNXLKJtTcrFTyWsxR_TGMJE1-88DTfNcfYktTOCx7J87VxTkRYTEVBpxuFFRiIXisS1Qw9KjKqgOW8bmAT8_4lJgaRabWZh8b5e9T0fDrQIgghzFlJgwKn2Dvj8HJiDQ2pNLhRKKKlM/s4577/1-1.jpg><img src=https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgylzz8OQkWjjhyBpYDBNRNezdgowWbuzeSXr_I5uSf6XnWq2y2ZNXLKJtTcrFTyWsxR_TGMJE1-88DTfNcfYktTOCx7J87VxTkRYTEVBpxuFFRiIXisS1Qw9KjKqgOW8bmAT8_4lJgaRabWZh8b5e9T0fDrQIgghzFlJgwKn2Dvj8HJiDQ2pNLhRKKKlM/w640-h416/1-1.jpg alt></a></p><p><strong>Figure 1-1:</strong> <em>Deep Learning Introduction.</em></p><h3 id=artificial-neuron>Artificial Neuron
<a class=anchor href=#artificial-neuron>#</a></h3><p>As we saw in the previous section, DNNs leverage a hierarchical, layered, role-based (input layer, n hidden layers, and output layer) network model, where each layer consists of artificial neurons. Neurons in different layers may be fully or partially connected to neurons in the other layers, depending on the DNN’s network model. However, neurons within the same layer are not connected.</p><p>The logical structure and functionality of an artificial neuron aim to emulate those of a biological neuron. A biological neuron transmits electrical signals to its peer neuron via the output axon to the axon terminal, which connects to the dendrites of the peer neuron at a connection point called a synapse. A biological neuron may receive signals from multiple dendrites, and if the signal is strong enough, it activates the neuron, causing the cell body to trigger a signal through its output axon, which connects to another neuron, and the process continues.</p><p>An artificial neuron, as a computational unit, calculates the weighted sum (z = pre-activation value) of the input (x) received from the previous layer, adds a bias value (b), and applies an activation function to produce the output (ŷ = post-activation value). </p><p>The weight value for the input can be loosely compared to a synapse since it represents a connection, input values are assigned with weight. The weights-to-neuron association, in turn, can be seen as analogous to dendrites. The computational processes (weighted sum of inputs, bias addition, and activation functions) represent the cell body, while the connections to other neurons can be compared to output axons. In Figure 1-2, bn refers to a biological neuron. From now on, &ldquo;neuron&rdquo; will refer to an artificial neuron.</p><p><a href=https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjOrkcskrlY_TpMwWARozhWCz6-0xDdUhr_765rrYx_1sloD7s1Kpwq_9o3S7CO9tg2TQBZ30MjNnUJBFD5BplD_iWUB-N51okGxDKZPpZYwsURDniDeQIpxpSOAngWnYaGj4jRG5rKq_UH9fEDGdvVQjOBnni-qji-uJLggmrEfpMG0FDYcAMNme4O8Pg/s4158/1-2.jpg><img src=https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjOrkcskrlY_TpMwWARozhWCz6-0xDdUhr_765rrYx_1sloD7s1Kpwq_9o3S7CO9tg2TQBZ30MjNnUJBFD5BplD_iWUB-N51okGxDKZPpZYwsURDniDeQIpxpSOAngWnYaGj4jRG5rKq_UH9fEDGdvVQjOBnni-qji-uJLggmrEfpMG0FDYcAMNme4O8Pg/w640-h366/1-2.jpg alt></a></p><p><strong>Figure 1-2:</strong> <em>Construct of an Artificial Neuron.</em></p><h4 id=weighted-sum-for-pre-activation-value>Weighted Sum for Pre-Activation Value
<a class=anchor href=#weighted-sum-for-pre-activation-value>#</a></h4><p>The lower part of Figure 1-2 depicts the mathematical formulas of a neuron. The <em>pre-activation</em> value z is the weighted sum of the inputs. Although the bias is not part of the input data, a neuron treats it as an input variable when calculating the weighted sum. Each input x has a corresponding weight w. The calculation process is straightforward: each input value is multiplied by its corresponding weight, and the results are summed to obtain the weighted sum z.</p><p>The capital Greek letter sigma ∑ in the formula indicates that we are summing a series of terms, which, in this case, are the input values multiplied by their respective weights. The letter n specifies how many terms are being summed (four pairs of inputs and weights), while the letter iii denotes the starting point of the summation (bias b0 and weight w0). The equation for the weighted sum is:</p><p>z = b0w0 + x1w1 + x2w2 + x3w3. </p><h4 id=relu-activation-function-for-post-activation>ReLU Activation Function for Post-Activation
<a class=anchor href=#relu-activation-function-for-post-activation>#</a></h4><p>Next, the process applies an activation function, ReLU (Rectified Linear Unit) [4] in our example, to the weighted sum z obtain the post-activation value ŷ. The output of the ReLU function is z if z is greater than zero (0); otherwise, the result is zero (0). This can be written as: </p><p>ŷ = MAX (0, z) </p><p>which selects the larger value between 0 and variable z.  The figure 1-3 depicts the ReLU activation function.</p><p><a href=https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhuyOdyPBd4vJC9Q5d3kKRz_eGaHlbX5s0vc6mbdOwf6i2qz55wRX3IxTZ5jenuY1ZtR_UX16iEVrzFZu_W2QsWonDLEcZwQAmwt1_hEFKmmb2J54TjXB5cIMM480I4LNzy7MKNGMSd7ub4TNbeAsF1FRPEhkonxa6rxPqj0Kg_S5qoR_EnKW_-axuq9NI/s2879/1-3.jpg><img src=https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhuyOdyPBd4vJC9Q5d3kKRz_eGaHlbX5s0vc6mbdOwf6i2qz55wRX3IxTZ5jenuY1ZtR_UX16iEVrzFZu_W2QsWonDLEcZwQAmwt1_hEFKmmb2J54TjXB5cIMM480I4LNzy7MKNGMSd7ub4TNbeAsF1FRPEhkonxa6rxPqj0Kg_S5qoR_EnKW_-axuq9NI/w640-h494/1-3.jpg alt></a></p><p><strong>Figure 1-3:</strong> <em>Construct of an Artificial Neuron.</em></p><p>Based on the figure 1-3 we can use the mathematical definition below for ReLU:</p><p><a href=https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjZOd_pTlu8aKviu_WlIqUx4wvywYR3pgvLDuHKOIiAX0AmzrSTopNH1nbcpSzC2frox8I0N9ZQwai1jdvLIii8HHH9EFjKUkXn_nWNPIZipIe00M1anltJ-uiFaVSg7NL3awe1ThXMh_AbxB75nwtSJ5jvEmxtPQXmimnDXPxj4M8QxKVagtSC2Hu-I_4/s1051/1-3-kaava.jpg><img src=https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjZOd_pTlu8aKviu_WlIqUx4wvywYR3pgvLDuHKOIiAX0AmzrSTopNH1nbcpSzC2frox8I0N9ZQwai1jdvLIii8HHH9EFjKUkXn_nWNPIZipIe00M1anltJ-uiFaVSg7NL3awe1ThXMh_AbxB75nwtSJ5jvEmxtPQXmimnDXPxj4M8QxKVagtSC2Hu-I_4/w640-h342/1-3-kaava.jpg alt></a></p><h4 id=bias-term>Bias Term
<a class=anchor href=#bias-term>#</a></h4><p>In the example calculation above, imagine that all input values are zero. Without a bias term, the activation value will be zero, regardless of how large the weight parameters are. Therefore, the bias term allows the neuron to produce non-zero outputs, even when all input values are zero.</p><h4 id=s-shaped-functions--tanh-and-sigmoid>S-Shaped Functions – TANH and SIGMOID
<a class=anchor href=#s-shaped-functions--tanh-and-sigmoid>#</a></h4><p>The ReLU function is a non-linear activation function. Naturally, there are other activation functions as well. The Hyperbolic Tangent (tanh) [5] and the logistic Sigmoid [6] functions are examples of S-shaped functions that are symmetric around zero. Figure 1-4 illustrates that as the positive z value increases, the tanh function approaches one (1), while as the negative z value decreases, it approaches -1. Thus, the range of the tanh function is from -1 to 1. Similarly, the sigmoid function&rsquo;s S-curve is also symmetric around zero, but its range is from 0 to 1.</p><p><a href=https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjeIqQJqOcO1a5K7mi9pky0yQbyZET0nciIFl_NlnylFLsZJyDy1RAgB0nqJteGQhPmshYXS5ozCjddEvfP_0w4Q6UXivhqK1TAge7_jolCTqx059LHQs9raYAK54P6op1uOQm_jtO5yvFOl1Ms8MiEHIcExscjFlZJ-Iyzus026-6PFDhlLSocVqfIHDM/s4430/1-5.jpg><img src=https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjeIqQJqOcO1a5K7mi9pky0yQbyZET0nciIFl_NlnylFLsZJyDy1RAgB0nqJteGQhPmshYXS5ozCjddEvfP_0w4Q6UXivhqK1TAge7_jolCTqx059LHQs9raYAK54P6op1uOQm_jtO5yvFOl1Ms8MiEHIcExscjFlZJ-Iyzus026-6PFDhlLSocVqfIHDM/w640-h324/1-5.jpg alt></a></p><p><strong>Figure 1-4:</strong> <em>Tanh and Sigmoid functions.</em></p><p>Here are examples of both functions using the same pre-activation value of 3.02, as in the ReLU activation function example.</p><p>Note, the ⅇ represents <strong>Euler’s Number ⅇ≈ 2.718.</strong> The symbol σ represents the sigmoid function.</p><p>The formula for tanh function is:</p><p><a href=https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4R5gSyFjTT5RmxaKhzg7p75b62HJ3MnSqlp4dcDuf_csfY2h1-i4HV_lnY77guRnt6a0Sy7xxmD-y16jkVXjNHSzCTamDN4VNqExSGoy8nDXDP8fttjk0sZwT0xj_s8LSuK1FzVbcpJgo9nIxLBYZTWmv57HjYA0q0IJLZyCLJWF4xDaotW3boxwtEtQ/s919/1-4-tanh-1.jpg><img src=https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4R5gSyFjTT5RmxaKhzg7p75b62HJ3MnSqlp4dcDuf_csfY2h1-i4HV_lnY77guRnt6a0Sy7xxmD-y16jkVXjNHSzCTamDN4VNqExSGoy8nDXDP8fttjk0sZwT0xj_s8LSuK1FzVbcpJgo9nIxLBYZTWmv57HjYA0q0IJLZyCLJWF4xDaotW3boxwtEtQ/w640-h68/1-4-tanh-1.jpg alt></a></p><p>The tanh function for z = 3,02</p><p><a href=https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg4ZwR_uymBs_3vyj_xIXTP9bj9FSD4sQCyyIi487ZMHR3x2O4jud4_9lHWnonMDgnSn-CvPVyvLRquGP9fGyVWQGN1-7DsCbVmtsokvg-ZXiZKkUEI1cZCKTIjkl9Ou2hPA6g8b9Zb8v5uGLmK_lWYwNCbul6_6dJH1IVAlZnJipMqX37oHw60kZRmY8s/s818/1-4-tanh-2.jpg><img src=https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg4ZwR_uymBs_3vyj_xIXTP9bj9FSD4sQCyyIi487ZMHR3x2O4jud4_9lHWnonMDgnSn-CvPVyvLRquGP9fGyVWQGN1-7DsCbVmtsokvg-ZXiZKkUEI1cZCKTIjkl9Ou2hPA6g8b9Zb8v5uGLmK_lWYwNCbul6_6dJH1IVAlZnJipMqX37oHw60kZRmY8s/w640-h80/1-4-tanh-2.jpg alt></a></p><p>The formula for sigmoid function is:</p><p><a href=https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiAs-QqW36L1knHMyQJUqhHKLkvaFjzM2v1Cf4EgqMPdHhD5C3ly2oDS_PStNIjaxBZykMwaEUrXNZuOWYBkl_FZpvNyw2eUZ0r5ky60p5z_taOy6BejK1_vb6WpJUEQ8TbeA95vSOljAkZAR8TszrY0f-hpKhGrIe4NwJqVuE27hdtag770YkVgkg-5Qs/s790/1-4-sigmoid-1.jpg><img src=https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiAs-QqW36L1knHMyQJUqhHKLkvaFjzM2v1Cf4EgqMPdHhD5C3ly2oDS_PStNIjaxBZykMwaEUrXNZuOWYBkl_FZpvNyw2eUZ0r5ky60p5z_taOy6BejK1_vb6WpJUEQ8TbeA95vSOljAkZAR8TszrY0f-hpKhGrIe4NwJqVuE27hdtag770YkVgkg-5Qs/w640-h84/1-4-sigmoid-1.jpg alt></a></p><p>The sigmoid function for z = 3,02</p><p>The symbol σ represents sigmoid function.</p><p><a href=https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg037DKdEdrenzQdfB6nDXJqdE7tQ1VjivAQniM5rr8VtdjTfTS4c8tzkqzWTe6Swml9zR-UCqyJLHBXsENQ0Feo2mFUU9s_JAfagJXQbBhGDxIODeD0WgzR_3cJnkJm4UE9OjQinb7e4zH5ogMT-oC3hKstJdShhQwUxm7g8q1GTl_QrYkXkemI6ZlId0/s819/1-4-sigmoid-2.jpg><img src=https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg037DKdEdrenzQdfB6nDXJqdE7tQ1VjivAQniM5rr8VtdjTfTS4c8tzkqzWTe6Swml9zR-UCqyJLHBXsENQ0Feo2mFUU9s_JAfagJXQbBhGDxIODeD0WgzR_3cJnkJm4UE9OjQinb7e4zH5ogMT-oC3hKstJdShhQwUxm7g8q1GTl_QrYkXkemI6ZlId0/w640-h66/1-4-sigmoid-2.jpg alt></a></p><p>For z=3.02, both the sigmoid and tanh functions return values close to 1, but the tanh function is slightly higher, approaching its maximum of 1 more quickly than the sigmoid.</p><p>Which activation function should we use? The ReLU function has largely replaced the tanh and sigmoid functions due to its simplicity, which reduces the required computation cycles. However, if tanh and sigmoid are used, tanh is typically applied in the hidden layers, while sigmoid is used in the output layer.</p><h3 id=network-impact>Network Impact
<a class=anchor href=#network-impact>#</a></h3><p>A single artificial neuron is the smallest unit of a neural network. The size of the neuron depends on its connections to input nodes. Every connection has an associated weight parameter, which is typically a 32-bit value. In our example, with 4 connections, the size of the neuron is 4 x 32 bits = 128 bits.</p><p>Although we haven’t defined the size of the input in this example, let’s assume that each input (x) is an 8-bit value, giving us 4 x 8 bits = 32 bits for the input data. Thus, our single neuron &ldquo;model&rdquo; requires 128 bits for the weights plus 32 bits for the input data, totaling 160 bits of memory. This is small enough to not require parallelization.</p><p>However, if the memory requirement of the neural network model combined with the input data (the &ldquo;job&rdquo;) exceeds the memory capacity of a GPU, a parallelization strategy is needed. The job can be split across multiple GPUs within a single server, with synchronization happening over high-speed NVLink. If the job must be divided between multiple GPU servers, synchronization occurs over the backend network, which must provide lossless, high-speed packet forwarding.</p><p>Parallelization strategies will be discussed in the next chapter, which introduces a Feedforward Neural Network using the Backpropagation algorithm, and in later chapters dedicated to Parallelization.</p><h3 id=summary>Summary
<a class=anchor href=#summary>#</a></h3><p>Deep Learning leverages Neural Networks, which consist of artificial neurons. An artificial neuron mimics the structure and operation of a biological neuron. Input data is fed to the neuron through connections, each with its own weight parameter. The neuron uses these weights to calculate a weighted sum of the inputs, known as the pre-activation value. This result is then passed through an activation function, which provides the post-activation value, or the actual output of the neuron. The activation functions discussed in this chapter are the non-linear ReLU (Rectified Linear Unit), Hyperbolic Tangent (tanh), and logistic Sigmoid functions.</p><p><strong>References:</strong></p><ul><li><a href=https://nwktimes.blogspot.com/2024/09/ai4ne-ch1-artificial-neuron.html>https://nwktimes.blogspot.com/2024/09/ai4ne-ch1-artificial-neuron.html</a></li></ul></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/prmanna/tech-book/commit/ef54debd533c51201059f99e204fc24d4a6d3e22 title='Last modified by Prasenjit Manna | April 19, 2025' target=_blank rel=noopener><img src=/tech-book/svg/calendar.svg class=book-icon alt>
<span>April 19, 2025</span></a></div></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><ul><li><ul><li></li><li><a href=#introduction>Introduction</a></li><li><a href=#artificial-neuron>Artificial Neuron</a></li><li><a href=#network-impact>Network Impact</a></li><li><a href=#summary>Summary</a></li></ul></li></ul></li></ul></nav></div></aside></main></body></html>