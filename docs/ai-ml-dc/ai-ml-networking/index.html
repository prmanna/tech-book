<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  Intro
  #


  AI/ML Networking Part I: RDMA Basics
  #

TBD

  AI/ML Networking: Part-II: Introduction of Deep Neural Networks
  #

Machine Learning (ML) is a subset of Artificial Intelligence (AI). ML is based on algorithms that allow learning, predicting, and making decisions based on data rather than pre-programmed tasks. ML leverages Deep Neural Networks (DNNs), which have multiple layers, each consisting of neurons that process information from sub-layers as part of the training process. Large Language Models (LLMs), such as OpenAI’s GPT (Generative Pre-trained Transformers), utilize ML and Deep Neural Networks."><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://prasenjitmanna.com/tech-book/docs/ai-ml-dc/ai-ml-networking/"><meta property="og:site_name" content="Technical Book"><meta property="og:title" content="AI/ML Networking"><meta property="og:description" content=" Intro # AI/ML Networking Part I: RDMA Basics # TBD
AI/ML Networking: Part-II: Introduction of Deep Neural Networks # Machine Learning (ML) is a subset of Artificial Intelligence (AI). ML is based on algorithms that allow learning, predicting, and making decisions based on data rather than pre-programmed tasks. ML leverages Deep Neural Networks (DNNs), which have multiple layers, each consisting of neurons that process information from sub-layers as part of the training process. Large Language Models (LLMs), such as OpenAI’s GPT (Generative Pre-trained Transformers), utilize ML and Deep Neural Networks."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:modified_time" content="2025-04-19T00:19:27+05:30"><title>AI/ML Networking | Technical Book</title>
<link rel=manifest href=/tech-book/manifest.json><link rel=icon href=/tech-book/favicon.png><link rel=canonical href=https://prasenjitmanna.com/tech-book/docs/ai-ml-dc/ai-ml-networking/><link rel=stylesheet href=/tech-book/book.min.a61cdb2979f3c2bece54ef69131fba427dd57d55c232d3bb5fdb62ac41aa8354.css integrity="sha256-phzbKXnzwr7OVO9pEx+6Qn3VfVXCMtO7X9tirEGqg1Q=" crossorigin=anonymous><script defer src=/tech-book/fuse.min.js></script><script defer src=/tech-book/en.search.min.4b7afe11992360c2af4fdaa4ff0822692940948a281ca5326ca34e31e41f52aa.js integrity="sha256-S3r+EZkjYMKvT9qk/wgiaSlAlIooHKUybKNOMeQfUqo=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/tech-book/><img src=/tech-book/logo.png alt=Logo><span>Technical Book</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><input type=checkbox id=section-99f552133860bc21797a47fe73e93434 class=toggle>
<label for=section-99f552133860bc21797a47fe73e93434 class="flex justify-between"><a href=/tech-book/docs/5g/>5G</a></label><ul><li><input type=checkbox id=section-dfc790b73acb0410a0114547cbf5af32 class=toggle>
<label for=section-dfc790b73acb0410a0114547cbf5af32 class="flex justify-between"><a href=/tech-book/docs/5g/5g-intro/>An Overview of 5G Networking</a></label></li></ul></li><li><input type=checkbox id=section-7c4a3b16aeeb5b7194b232c1eef2f4fb class=toggle>
<label for=section-7c4a3b16aeeb5b7194b232c1eef2f4fb class="flex justify-between"><a href=/tech-book/docs/algorithms/>Algorithms</a></label><ul><li><input type=checkbox id=section-8956d82fbe6869140758f7e8679174bc class=toggle>
<label for=section-8956d82fbe6869140758f7e8679174bc class="flex justify-between"><a href=/tech-book/docs/algorithms/breadth-first-search/>Breadth First Search</a></label></li><li><input type=checkbox id=section-5a049cfad1740f3fd30565524385fa57 class=toggle>
<label for=section-5a049cfad1740f3fd30565524385fa57 class="flex justify-between"><a href=/tech-book/docs/algorithms/depth-first-search/>Depth First Search</a></label></li><li><input type=checkbox id=section-ebc049f26d82165be8c6f1f9e504e799 class=toggle>
<label for=section-ebc049f26d82165be8c6f1f9e504e799 class="flex justify-between"><a href=/tech-book/docs/algorithms/easy/>Easy Complexity</a></label></li><li><input type=checkbox id=section-1071946392bd1f431993e950147fa054 class=toggle>
<label for=section-1071946392bd1f431993e950147fa054 class="flex justify-between"><a href=/tech-book/docs/algorithms/priority-queue-and-heap/>Priority Queue and Heap</a></label></li><li><input type=checkbox id=section-08afbeb294c43ca4908c1c89a4be9d0a class=toggle>
<label for=section-08afbeb294c43ca4908c1c89a4be9d0a class="flex justify-between"><a href=/tech-book/docs/algorithms/two-pointers/>Two Pointers & Sliding Window</a></label></li><li><input type=checkbox id=section-ef36c7c4f7e0dec6525068c3c409100c class=toggle>
<label for=section-ef36c7c4f7e0dec6525068c3c409100c class="flex justify-between"><a href=/tech-book/docs/algorithms/medium/>Medium Complexity</a></label></li></ul></li><li><input type=checkbox id=section-8c7c5c4a8382299873178820b1d91be1 class=toggle checked>
<label for=section-8c7c5c4a8382299873178820b1d91be1 class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/>Data Center Networking for AI Clusters</a></label><ul><li><input type=checkbox id=section-cd3a69c68b09887f63fced84e24740c3 class=toggle checked>
<label for=section-cd3a69c68b09887f63fced84e24740c3 class="flex justify-between"><a href=/tech-book/docs/ai-ml-dc/ai-ml-networking/ class=active>AI/ML Networking</a></label></li></ul></li><li><input type=checkbox id=section-3272b2d28b2b247027bf478619ca416f class=toggle>
<label for=section-3272b2d28b2b247027bf478619ca416f class="flex justify-between"><a href=/tech-book/docs/data-center/>Data Center Tips</a></label><ul><li><input type=checkbox id=section-984f932c7aba4f0a1841e8413165c947 class=toggle>
<label for=section-984f932c7aba4f0a1841e8413165c947 class="flex justify-between"><a href=/tech-book/docs/data-center/data-center-ethernet/>Data Center Ethernet</a></label></li><li><input type=checkbox id=section-bc5ac88940153a700e63e3be886c63cc class=toggle>
<label for=section-bc5ac88940153a700e63e3be886c63cc class="flex justify-between"><a href=/tech-book/docs/data-center/data-center-technologies/>Data Center Technologies</a></label></li><li><input type=checkbox id=section-86fde1ddf43700ef8191e11c59a82cf1 class=toggle>
<label for=section-86fde1ddf43700ef8191e11c59a82cf1 class="flex justify-between"><a href=/tech-book/docs/data-center/data-center-network-virtualization/>Network Virtualization in Cloud Data Centers</a></label></li></ul></li><li><input type=checkbox id=section-ddf784688e0c6d5abb681d2c57851559 class=toggle>
<label for=section-ddf784688e0c6d5abb681d2c57851559 class="flex justify-between"><a href=/tech-book/docs/manageability/>Manageability</a></label><ul><li><input type=checkbox id=section-33ac730897af6feca81c1fdb7869c57e class=toggle>
<label for=section-33ac730897af6feca81c1fdb7869c57e class="flex justify-between"><a href=/tech-book/docs/manageability/why-grpc-on-http2/>gRPC on HTTP/2</a></label></li></ul></li><li><input type=checkbox id=section-c14ae944424668ef125e10cd791a3d3d class=toggle>
<label for=section-c14ae944424668ef125e10cd791a3d3d class="flex justify-between"><a href=/tech-book/docs/networking-tips/>Networking Tips</a></label><ul><li><input type=checkbox id=section-78fdf21c03c55935d3146441b06faf3e class=toggle>
<label for=section-78fdf21c03c55935d3146441b06faf3e class="flex justify-between"><a href=/tech-book/docs/networking-tips/dns/>DNS Overview</a></label></li><li><input type=checkbox id=section-bbcb027a658dc7a2333d59078ee507f9 class=toggle>
<label for=section-bbcb027a658dc7a2333d59078ee507f9 class="flex justify-between"><a href=/tech-book/docs/networking-tips/ecmp/>ECMP Load Balancing</a></label></li><li><input type=checkbox id=section-3b39b86f18b3451e5b8a1b81c369549c class=toggle>
<label for=section-3b39b86f18b3451e5b8a1b81c369549c class="flex justify-between"><a href=/tech-book/docs/networking-tips/ip-fragmentation/>IP Fragmentation - IPv4 & IPv6</a></label></li><li><input type=checkbox id=section-c6d06c54cc91b0bc3f948d33b437fa8b class=toggle>
<label for=section-c6d06c54cc91b0bc3f948d33b437fa8b class="flex justify-between"><a href=/tech-book/docs/networking-tips/ip-tos-dscp/>IP Precedence And TOS | DSCP</a></label></li><li><input type=checkbox id=section-5f3260c76dd37177e3f89057bfe520ae class=toggle>
<label for=section-5f3260c76dd37177e3f89057bfe520ae class="flex justify-between"><a href=/tech-book/docs/networking-tips/traceroute/>Linux traceroute tool</a></label></li><li><input type=checkbox id=section-a26cc3fafb36cbc55527adf39ec83849 class=toggle>
<label for=section-a26cc3fafb36cbc55527adf39ec83849 class="flex justify-between"><a href=/tech-book/docs/networking-tips/mlag/>Multi Chassis Link Aggregation Basics</a></label></li><li><input type=checkbox id=section-36409a0abcb2d4d4baf2e0b682d1a5dd class=toggle>
<label for=section-36409a0abcb2d4d4baf2e0b682d1a5dd class="flex justify-between"><a href=/tech-book/docs/networking-tips/qos/>QoS</a></label></li><li><input type=checkbox id=section-db41e3547d316b01acf9a0ce9c04ef34 class=toggle>
<label for=section-db41e3547d316b01acf9a0ce9c04ef34 class="flex justify-between"><a href=/tech-book/docs/networking-tips/spine-leaf-arch/>Spine-leaf Architecture Basics</a></label></li><li><input type=checkbox id=section-0eb0d409074a76b8cb02624655e2434d class=toggle>
<label for=section-0eb0d409074a76b8cb02624655e2434d class="flex justify-between"><a href=/tech-book/docs/networking-tips/tcp-congestion/>TCP Congestion Control</a></label></li><li><input type=checkbox id=section-3d1710947b3c5be1a1cc33d88fcc6f54 class=toggle>
<label for=section-3d1710947b3c5be1a1cc33d88fcc6f54 class="flex justify-between"><a href=/tech-book/docs/networking-tips/tcp-data-transfer/>TCP Data Transfer</a></label></li></ul></li><li><input type=checkbox id=section-f0a392f2f083f28a4991336773716a63 class=toggle>
<label for=section-f0a392f2f083f28a4991336773716a63 class="flex justify-between"><a href=/tech-book/docs/optical-knowledge/>Optical Knowledge</a></label><ul><li><input type=checkbox id=section-18261b95a92d4fa86116243edca4e9fb class=toggle>
<label for=section-18261b95a92d4fa86116243edca4e9fb class="flex justify-between"><a href=/tech-book/docs/optical-knowledge/optical-breakout/>Optical Transceiver(Grey) & Breakout Model</a></label></li></ul></li><li><input type=checkbox id=section-a55840d746138b3d1fedb81acbccdded class=toggle>
<label for=section-a55840d746138b3d1fedb81acbccdded class="flex justify-between"><a href=/tech-book/docs/programming-tips/>Programming Tips</a></label><ul><li><input type=checkbox id=section-f929f1fe13cb5d6cc96ca3e98f9d9777 class=toggle>
<label for=section-f929f1fe13cb5d6cc96ca3e98f9d9777 class="flex justify-between"><a href=/tech-book/docs/programming-tips/c++/>C++ Tips</a></label></li></ul></li><li><input type=checkbox id=section-b35dbf5ebcd4c19926cf5a9aab6c7655 class=toggle>
<label for=section-b35dbf5ebcd4c19926cf5a9aab6c7655 class="flex justify-between"><a href=/tech-book/docs/systemdesign-tips/>SystemDesign-Tips</a></label><ul><li><input type=checkbox id=section-4a618bc3b0b30107f0cec6d3bd6c025f class=toggle>
<label for=section-4a618bc3b0b30107f0cec6d3bd6c025f class="flex justify-between"><a href=/tech-book/docs/systemdesign-tips/code-deployment-system/>Design A Code-Deployment System</a></label></li><li><input type=checkbox id=section-e600754306aa95ce9c5a72b5efec6d7a class=toggle>
<label for=section-e600754306aa95ce9c5a72b5efec6d7a class="flex justify-between"><a href=/tech-book/docs/systemdesign-tips/stock-broker/>Design A Stock-Broker System</a></label></li><li><input type=checkbox id=section-bfa7a29878f65cfbb179e491c1211fa8 class=toggle>
<label for=section-bfa7a29878f65cfbb179e491c1211fa8 class="flex justify-between"><a href=/tech-book/docs/systemdesign-tips/design-amazon/>Design Amazon</a></label></li><li><input type=checkbox id=section-5456837e25872f6352d861a9b5662cb1 class=toggle>
<label for=section-5456837e25872f6352d861a9b5662cb1 class="flex justify-between"><a href=/tech-book/docs/systemdesign-tips/design-slack/>Design Slack</a></label></li><li><input type=checkbox id=section-5a78d16f54536a400b654f17f917bce1 class=toggle>
<label for=section-5a78d16f54536a400b654f17f917bce1 class="flex justify-between"><a href=/tech-book/docs/systemdesign-tips/google-drive/>Google Drive - Design</a></label></li></ul></li></ul><ul><li><a href=/tech-book/posts/>Blog</a></li><li><a href=https://prasenjitmanna.com/ target=_blank rel=noopener>Prasenjit's Blog</a></li><li><a href=https://prasenjitmanna.com/tech-book/ target=_blank rel=noopener>Prasenjit - Tech Book</a></li><li><a href=https://prasenjitmanna.com/upskills/ target=_blank rel=noopener>Prasenjit - Upskills</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/tech-book/svg/menu.svg class=book-icon alt=Menu>
</label><strong>AI/ML Networking</strong>
<label for=toc-control><img src=/tech-book/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#intro>Intro</a><ul><li><a href=#aiml-networking-part-i-rdma-basics>AI/ML Networking Part I: RDMA Basics</a></li><li><a href=#aiml-networking-part-ii-introduction-of-deep-neural-networks>AI/ML Networking: Part-II: Introduction of Deep Neural Networks</a></li><li><a href=#aiml-networking-part-iii-basics-of-neural-networks-training-process>AI/ML Networking: Part-III: Basics of Neural Networks Training Process</a><ul><li><a href=#neural-network-architecture-overview>Neural Network Architecture Overview</a></li></ul></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=intro>Intro
<a class=anchor href=#intro>#</a></h1><h2 id=aiml-networking-part-i-rdma-basics>AI/ML Networking Part I: RDMA Basics
<a class=anchor href=#aiml-networking-part-i-rdma-basics>#</a></h2><p>TBD</p><h2 id=aiml-networking-part-ii-introduction-of-deep-neural-networks>AI/ML Networking: Part-II: Introduction of Deep Neural Networks
<a class=anchor href=#aiml-networking-part-ii-introduction-of-deep-neural-networks>#</a></h2><p><em>Machine Learning (ML)</em> is a subset of <em>Artificial Intelligence (AI)</em>. ML is based on algorithms that allow learning, predicting, and making decisions based on data rather than pre-programmed tasks. ML leverages <em>Deep Neural Networks (DNNs)</em>, which have multiple layers, each consisting of neurons that process information from sub-layers as part of the training process. <em>Large Language Models (LLMs)</em>, such as OpenAI’s GPT (Generative Pre-trained Transformers), utilize ML and Deep Neural Networks.</p><p>For network engineers, it is crucial to understand the fundamental operations and communication models used in ML training processes. To emphasize the importance of this, I quote the Chinese philosopher and strategist Sun Tzu, who lived around 600 BCE, from his work The Art of War.</p><blockquote><p><em>If you know the enemy and know yourself, you need not fear the result of a hundred battles.</em></p></blockquote><p>We don’t have to be data scientists to design a network for AI/ML, but we must understand the operational fundamentals and communication patterns of ML. Additionally, we must have a deep understanding of network solutions and technologies to build a lossless and cost-effective network for enabling efficient training processes.</p><p>In the upcoming two posts, I will explain the basics of: </p><p><em>a) Data Models:</em> Layers and neurons, forward and backward passes, and algorithms. </p><p><em>b) Parallelization Strategies:</em> How training times can be reduced by dividing the model into smaller entities, batches, and even micro-batches, which are processed by several GPUs simultaneously.</p><p>The number of parameters, the selected data model, and the parallelization strategy affect the network traffic that crosses the data center switch fabric.</p><p>After these two posts, we will be ready to jump into the network part. </p><p>At this stage, you may need to read (or re-read) my previous post about Remote Direct Memory Access (RDMA), a solution that enables GPUs to write data from local memory to remote GPUs&rsquo; memory.
<img src=https://prasenjitmanna.com/tech-book/diagrams/ai-ml-dc/ai-ml-dc-1.jpg alt=img|320x271></p><h2 id=aiml-networking-part-iii-basics-of-neural-networks-training-process>AI/ML Networking: Part-III: Basics of Neural Networks Training Process
<a class=anchor href=#aiml-networking-part-iii-basics-of-neural-networks-training-process>#</a></h2><h3 id=neural-network-architecture-overview>Neural Network Architecture Overview
<a class=anchor href=#neural-network-architecture-overview>#</a></h3><p>Deep Neural Networks (DNN) leverage various architectures for training, with one of the simplest and most fundamental being the Feedforward Neural Network (FNN). Figure 2-1 illustrates our simple, three-layer FNN.</p><h4 id=input-layer>Input Layer: 
<a class=anchor href=#input-layer>#</a></h4><p>The first layer doesn’t have neurons, instead the input data parameters X1, X2, and X3 are in this layer, from where they are fed to first hidden layer. </p><h4 id=hidden-layer>Hidden Layer: 
<a class=anchor href=#hidden-layer>#</a></h4><p>The neurons in the hidden layer calculate a weighted sum of the input data, which is then passed through an activation function. In our example, we are using the Rectified Linear Unit (ReLU) activation function. These calculations produce activation values for neurons. The activation value is modified input data value received from the input layer and published to upper layer.</p><h4 id=output-layer>Output Layer: 
<a class=anchor href=#output-layer>#</a></h4><p>Neurons in this layer calculate the weighted sum in the same manner as neurons in the hidden layer, but the result of the activation function is the final output.</p><p>The process described above is known as the Forwarding pass operation. Once the forward pass process is completed, the result is passed through a loss function, where the received value is compared to the expected value. The difference between these two values triggers the backpropagation process. The Loss calculation is the initial phase of Backpropagation process. During backpropagation, the network fine-tunes the weight values , neuron by neuron, from the output layer through the hidden layers. The neurons in the input layer do not participate in the backpropagation process because they do not have weight values to be adjusted.</p><p>After the backpropagation process, a new iteration of the forward pass begins from the first hidden layer. This loop continues until the received and expected values are close enough to expected value, indicating that the training is complete.</p><p><a href=https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgwQEpZVHZtvYz66gfK5OFGHUdixuV8lqUiGoipKqReMEuC1duCCyBt4NatgsKjI6ka3xbZlswzw7vI7CBLGWrz3VKF3Qq0eqnTjbxkLh-3J1iuK9ygTIfKDqyRqMNILGdK5Z9aDxePxmsLVjv0p-YAwqoAngEPUdb72q7eIDjgWO_pyir-zSv5Q-pWkQA/s4245/2-1.jpg><img src=https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgwQEpZVHZtvYz66gfK5OFGHUdixuV8lqUiGoipKqReMEuC1duCCyBt4NatgsKjI6ka3xbZlswzw7vI7CBLGWrz3VKF3Qq0eqnTjbxkLh-3J1iuK9ygTIfKDqyRqMNILGdK5Z9aDxePxmsLVjv0p-YAwqoAngEPUdb72q7eIDjgWO_pyir-zSv5Q-pWkQA/w640-h286/2-1.jpg alt></a></p><p><strong>Figure 2-1:</strong> <em>Deep Neural Network Basic Structure and Operations.</em></p><h4 id=forwarding-pass>Forwarding Pass 
<a class=anchor href=#forwarding-pass>#</a></h4><p>Next, let&rsquo;s examine the operation of a Neural Network in more detail. Figure 2-2 illustrates a simple, three-layer Feedforward Neural Network (FNN) data model. The input layer has two neurons, H1 and H2, each receiving one input data value: a value of one (1) is fed to neuron H1 by input neuron X1, and a value of zero (0) is fed to neuron H2 by input neuron X2. The neurons in the input layer do not calculate a weighted sum or an activation value but instead pass the data to the next layer, which is the first hidden layer.</p><p>The hidden layer in our example consists of two neurons. These neurons use the ReLU activation function to calculate the activation value. During the initialization phase, the weight values for these neurons are assigned using the He Initialization method, which is often used with the ReLU function. The He Initialization method calculates the variance as 2/<em>n</em> where <em>n</em> is the number of neurons in the previous layer. In this example, with two input neurons, this gives a variance of  1 (=2/2). The weights are then drawn from a normal distribution ~<em>N(0,√variance),</em> which in this case is  ~N(0,1). Basically, this means that the randomly generated weight values are centered around zero with a standard deviation of one.</p><p>In Figure 2-2, the weight value for neuron H3 in the hidden layer is 0.5 for both input sources X1 (input data 1) and X2 (input data 0). Similarly, for the hidden layer neuron H4, the weight value is 1 for both input sources X1 (input data 1) and X2 (input data 0). Neurons in the hidden and output layers also have a bias variable. If the input to a neuron is zero, the output would also be zero if there were no bias. The bias ensures that a neuron can still produce a meaningful output even when the input is zero (i.e., the neuron is inactive). Neurons H3 and O5 have a bias value of 0.5, while neuron H4 has a bias value of 0 (I am using zero for simplify the calculation). </p><p>Let’s start the forward pass process from neuron H3 in the hidden layer. First, we calculate the weighted sum using the formula below, where Z3 represents the weighted sum of input. Here, Xn is the actual input data value received from the input layer’s neuron, and Wn  is the weight associated with that particular input neuron.</p><p>The weighted sum calculation (Z3) for neuron H3:</p><pre tabindex=0><code>_Z3 = (X1 ⋅ W31) + (X2 ⋅ W32) + b3_

_Given:_
_Z3 = (1 ⋅ 0.5) + (0 ⋅ 0.5) + 0_
_Z3 = 0.5 + 0 + 0_
_Z3 = 0.5_
</code></pre><p>To get the activation value a3 (shown as H3=0.5 in figure), we apply the ReLU function. The ReLU function outputs zero (0) if the calculated weighted sum Z is less than or equal to zero; otherwise, it outputs the value of the weighted sum Z.</p><p>The activation value a3 for H3 is:</p><blockquote><p><em>ReLU (Z3) = ReLU (0.5) = 0.5</em></p></blockquote><p>The weighted sum calculation for neuron H4:</p><blockquote><p><em>Z4 = (X1 ⋅ W41) + (X2 ⋅ W42) + b4</em></p><p><em>Given:</em>
<em>Z4 = (1 ⋅ 1) + (0 ⋅1) + 0.5</em>
<em>Z4 = 1 + 0 + 0.5</em>
<em>Z4 = 1.5</em></p></blockquote><p>The activation value using ReLU for Z4 is:</p><blockquote><p><em>ReLU (Z4) = ReLU (1.5) = 1.5</em></p></blockquote><p><a href=https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhZlF-uvw1lezXzt0RpaqF4-Xmi91b-4Krhzn9cmxDWswUtdmC8ziCnIH9DXngAg7wwJqRpfN3MingjK2DmPLYOG0utQ0dPeEVi7mhsCtY6PL8WWlUtQmTRv9L22cMtr6ALEYhJ3i5hSw5R1C1pEjjwe53BM5CdiOxjuSUpGRRi3iKPNqKTxUlWUuBbbmU/s4377/2-2.jpg><img src=https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhZlF-uvw1lezXzt0RpaqF4-Xmi91b-4Krhzn9cmxDWswUtdmC8ziCnIH9DXngAg7wwJqRpfN3MingjK2DmPLYOG0utQ0dPeEVi7mhsCtY6PL8WWlUtQmTRv9L22cMtr6ALEYhJ3i5hSw5R1C1pEjjwe53BM5CdiOxjuSUpGRRi3iKPNqKTxUlWUuBbbmU/w640-h296/2-2.jpg alt></a></p><p><strong>Figure 2-2:</strong> <em>Forwarding Pass on Hidden Layer.</em></p><p>After neurons H3 and H4 publish their activation values to neuron O5 in the output layer, O5 calculates the weighted sum Z5 for inputs with weights W53=1and W54=1. Using Z5, it calculates the output using the ReLU function. The difference between the received output value (Yr) and the expected value (Ye) triggers a backpropagation process. In our example, Yr−Ye=0.5.</p><h4 id=backpropagation-process>Backpropagation process
<a class=anchor href=#backpropagation-process>#</a></h4><p>The loss function measures the difference between the predicted output and the actual expected output. The loss function value indicates how well the neural network is performing. A high loss value means the network&rsquo;s predictions are far from the actual values, while a low loss value means the predictions are close.</p><p>After calculating the loss, backpropagation is initiated to minimize this loss. Backpropagation involves calculating the gradient of the loss function with respect to each weight and bias in the network. This step is crucial for adjusting the weights and biases to reduce the loss in subsequent forwarding pass iterations.</p><p>Loss function is calculated using the formula below:</p><blockquote><p><em>Loss (L) = (H3 x W53 + H4 x W54 + b5 – Ye)2</em>
<em>Given:</em>
<em>L = (0.5 x 1 + 1.5 x 1 + 0.5 - 2)2</em>
<em>L = (0.5 + 1.5 + 0.5 - 2)2</em>
<em>L = 0.52</em>
<em>L= 0.25</em></p></blockquote><p><a href=https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjdFOcP9Cn02czGrq25JgHW2iker9yg3-QcllyG360S9j_By-0mgt8JL1avnWINfoSNW6luO9o9vIGXmCYHvWciNc2g6Ioz4ewcbENU2hJMl1Be9uiu6xzxCzthyupv6MC67AiPADVgIYuzIuucpERmd31xHLi5RXcRkkD6b0Jq8PszyDbZ-Q1OWIhJ_6k/s4377/2-3.jpg><img src=https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjdFOcP9Cn02czGrq25JgHW2iker9yg3-QcllyG360S9j_By-0mgt8JL1avnWINfoSNW6luO9o9vIGXmCYHvWciNc2g6Ioz4ewcbENU2hJMl1Be9uiu6xzxCzthyupv6MC67AiPADVgIYuzIuucpERmd31xHLi5RXcRkkD6b0Jq8PszyDbZ-Q1OWIhJ_6k/w640-h346/2-3.jpg alt></a></p><p><strong>Figure 2-3:</strong> <em>Forwarding Pass on Output Layer.</em></p><p>The result of the loss function is then fed into the gradient calculation process, where we compute the gradient of the loss function with respect to each weight and bias in the network. The gradient calculation result is then used to fine-tune the old weight values. The Eta hyper-parameter <em>η</em> (the learning rate) controls the step size during weight updates in the backpropagation process, balancing the speed of convergence with the stability of training. In our example, we are using a learning rate of 1/100 = 0.01. The term hyper-parameters refers to parameters that affect the final result.</p><p>First, we compute the partial derivative of the loss function (gradient calculation) with respect to the old weight values. The following example shows the gradient calculation for weight W53. The same computation applies to W54  and b3.</p><p><strong>Gradient Calculation:</strong></p><blockquote><p><em>∂L   = 2W53 x (Yr – Ye)</em></p><p><em>∂W53</em></p></blockquote><blockquote><p> <em>Given</em>
 <em>= 2 x 0.5 x (2.5 - 2)</em>
 <em>= 1 x 0.5</em>
 <em>= 0.5</em></p></blockquote><p><strong>New weight value calculation.</strong></p><blockquote><p><em>W53 (new) = W53(old) – η x ∂L/∂W53</em></p><p><em>Given:</em>
<em>W53 (new) = 1–0.01 x 0.5</em>
<em>W53 (new) = 0.995</em></p></blockquote><p><a href=https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhIx1tlhe-CTPD9BaZpXDhngVJBd3WpX4y7IdZRHwMC_dEIsvh09C4GdA8dfS3UUJ9RQhYnT50YXHNLja6LM6yJc8mEoDH9WdHfpZ_MjI8byVLk1376ieBDc8gDqoEsmaNhjmVK6y_IMFhRtrOSmfADXg-BFwlPb6eyOsHkjskmX8KIO1xJpJ7xAA6I8pk/s4365/2-4.jpg><img src=https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhIx1tlhe-CTPD9BaZpXDhngVJBd3WpX4y7IdZRHwMC_dEIsvh09C4GdA8dfS3UUJ9RQhYnT50YXHNLja6LM6yJc8mEoDH9WdHfpZ_MjI8byVLk1376ieBDc8gDqoEsmaNhjmVK6y_IMFhRtrOSmfADXg-BFwlPb6eyOsHkjskmX8KIO1xJpJ7xAA6I8pk/w640-h320/2-4.jpg alt></a></p><p><strong>Figure 2-4:</strong> <em>Backpropagation - Gradient Calculation and New Weight Value Computation.</em></p><p>Figure 2-5 shows the formulas for calculating the new bias b3. The process is the same than what was used with updating the weight values.</p><p><a href=https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjgPJMFQHffuBguHEkzw1tyrczatiFp_sxKK7T8BROCP1CHwlH1y04qRUOXhM-3BKKQKERjkD3A1G4YdfZnT1ocptfrbdYPwOYkspb0Uj-YPCPmOQ5fb9aacAcPsKCnJScLHoAs6o9-Ak8eeW-FGKccTKRuoixEsSYAC2_FX7crUbapI0uT5SJop_jxoG8/s4362/2-5.jpg><img src=https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjgPJMFQHffuBguHEkzw1tyrczatiFp_sxKK7T8BROCP1CHwlH1y04qRUOXhM-3BKKQKERjkD3A1G4YdfZnT1ocptfrbdYPwOYkspb0Uj-YPCPmOQ5fb9aacAcPsKCnJScLHoAs6o9-Ak8eeW-FGKccTKRuoixEsSYAC2_FX7crUbapI0uT5SJop_jxoG8/w640-h322/2-5.jpg alt></a></p><p><strong>Figure 2-5:</strong> <em>Backpropagation - Gradient Calculation and New Bias Computation.</em></p><p>After updating the weights and biases, the backpropagation process moves to the hidden layer. Gradient computation in the hidden layer is more complex because the loss function only includes weights from the output layer as you can see from the Loss function formula below:</p><p>Loss (L) = (H3 x W53 + H4 x W54 + b5 – Ye)2</p><p>The formula for computing the weights and biases for neurons in the hidden layers uses the chain rule. The mathematical formula shown below, but the actual computation is beyond the scope of this chapter.</p><p>∂L   =    ∂L  x  ∂H3    </p><p>∂W31   ∂H3    ∂W31    </p><p>After the backpropagation process is completed, the next iteration of the forward pass starts. This loop continues until the received result is close enough to the expected result.</p><p>If the size of the input data exceeds the GPU’s memory capacity or if the computing power of one GPU is insufficient for the data model, we need to decide on a parallelization strategy. This strategy defines how the training workload is distributed across several GPUs. Parallelization impacts network load if we need more GPUs than are available on one server. Dividing the workload among GPUs within a single GPU-server or between multiple GPU-servers triggers synchronization of calculated gradients between GPUs. When the gradient is calculated, the GPUs synchronize the results and compute the average gradient, which is then used to update the weight values.</p><p>The upcoming chapter introduces pipeline parallelization and synchronization processes in detail. We will also discuss why lossless connection is required for AI/ML.</p><p><strong>References:</strong></p></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/prmanna/tech-book/commit/e037aad9bf2add338f95c13e8e7ddfdc21acfd6f title='Last modified by Prasenjit Manna | April 18, 2025' target=_blank rel=noopener><img src=/tech-book/svg/calendar.svg class=book-icon alt>
<span>April 18, 2025</span></a></div></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#intro>Intro</a><ul><li><a href=#aiml-networking-part-i-rdma-basics>AI/ML Networking Part I: RDMA Basics</a></li><li><a href=#aiml-networking-part-ii-introduction-of-deep-neural-networks>AI/ML Networking: Part-II: Introduction of Deep Neural Networks</a></li><li><a href=#aiml-networking-part-iii-basics-of-neural-networks-training-process>AI/ML Networking: Part-III: Basics of Neural Networks Training Process</a><ul><li><a href=#neural-network-architecture-overview>Neural Network Architecture Overview</a></li></ul></li></ul></li></ul></nav></div></aside></main></body></html>