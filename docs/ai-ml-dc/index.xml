<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Data Center Networking for AI Clusters on Technical Book</title><link>https://prasenjitmanna.com/tech-book/docs/ai-ml-dc/</link><description>Recent content in Data Center Networking for AI Clusters on Technical Book</description><generator>Hugo</generator><language>en-us</language><atom:link href="https://prasenjitmanna.com/tech-book/docs/ai-ml-dc/index.xml" rel="self" type="application/rss+xml"/><item><title>1. AI/ML Networking</title><link>https://prasenjitmanna.com/tech-book/docs/ai-ml-dc/1-ai-ml-networking/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://prasenjitmanna.com/tech-book/docs/ai-ml-dc/1-ai-ml-networking/</guid><description>&lt;h1 id="intro">
 Intro
 &lt;a class="anchor" href="#intro">#&lt;/a>
&lt;/h1>
&lt;h2 id="aiml-networking-part-i-rdma-basics">
 AI/ML Networking Part I: RDMA Basics
 &lt;a class="anchor" href="#aiml-networking-part-i-rdma-basics">#&lt;/a>
&lt;/h2>
&lt;p>TBD&lt;/p>
&lt;h2 id="aiml-networking-part-ii-introduction-of-deep-neural-networks">
 AI/ML Networking: Part-II: Introduction of Deep Neural Networks
 &lt;a class="anchor" href="#aiml-networking-part-ii-introduction-of-deep-neural-networks">#&lt;/a>
&lt;/h2>
&lt;p>&lt;em>Machine Learning (ML)&lt;/em> is a subset of &lt;em>Artificial Intelligence (AI)&lt;/em>. ML is based on algorithms that allow learning, predicting, and making decisions based on data rather than pre-programmed tasks. ML leverages &lt;em>Deep Neural Networks (DNNs)&lt;/em>, which have multiple layers, each consisting of neurons that process information from sub-layers as part of the training process. &lt;em>Large Language Models (LLMs)&lt;/em>, such as OpenAI’s GPT (Generative Pre-trained Transformers), utilize ML and Deep Neural Networks.&lt;/p></description></item><item><title>2. Deep Learning Basics | Artificial Neuron</title><link>https://prasenjitmanna.com/tech-book/docs/ai-ml-dc/2-ai-deep-learning-basics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://prasenjitmanna.com/tech-book/docs/ai-ml-dc/2-ai-deep-learning-basics/</guid><description>&lt;h4 id="content">
 Content
 &lt;a class="anchor" href="#content">#&lt;/a>
&lt;/h4>
&lt;ul>
&lt;li>Introduction &lt;/li>
&lt;li>Artificial Neuron  
&lt;ul>
&lt;li>Weighted Sum for Pre-Activation Value  &lt;/li>
&lt;li>ReLU Activation Function for Post-Activation  &lt;/li>
&lt;li>Bias Term &lt;/li>
&lt;li>S-Shaped Functions – TANH and SIGMOID&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Network Impact&lt;/li>
&lt;li>Summary&lt;/li>
&lt;/ul>
&lt;h2 id="introduction">
 Introduction
 &lt;a class="anchor" href="#introduction">#&lt;/a>
&lt;/h2>
&lt;p>Artificial Intelligence (AI) is a broad term for solutions that aim to mimic the functions of the human brain. Machine Learning (ML), in turn, is a subset of AI, suitable for tasks like simple pattern recognition and prediction. Deep Learning (DL), the focus of this section, is a subset of ML that leverages algorithms to extract meaningful patterns from data. Unlike ML, DL does not necessarily require human intervention, such as providing structured, labeled datasets (e.g., 1,000 bird images labeled as “bird” and 1,000 cat images labeled as “cat”). &lt;/p></description></item><item><title>3. Challenges in AI Fabric Design</title><link>https://prasenjitmanna.com/tech-book/docs/ai-ml-dc/3-challenges-in-ai-fabric/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://prasenjitmanna.com/tech-book/docs/ai-ml-dc/3-challenges-in-ai-fabric/</guid><description>&lt;h2 id="intro">
 Intro
 &lt;a class="anchor" href="#intro">#&lt;/a>
&lt;/h2>
&lt;p>Figure 10-1 illustrates a simple distributed GPU cluster consisting of three GPU hosts. Each host has two GPUs and a Network Interface Card (NIC) with two interfaces. Intra-host GPU communication uses high-speed NVLink interfaces, while inter-host communication takes place via NICs over slower PCIe buses.&lt;/p>
&lt;p>GPU-0 on each host is connected to Rail Switch A through interface E1. GPU-1 uses interface E2 and connects to Rail Switch B. In this setup, inter-host communication between GPUs connected to the same rail passes through a single switch. However, communication between GPUs on different rails goes over three hops  Rail–Spine–Rail switches.&lt;/p></description></item><item><title>4. Congestion Avoidance in AI Fabric</title><link>https://prasenjitmanna.com/tech-book/docs/ai-ml-dc/4-congestion-avoidance-in-ai-fabric/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://prasenjitmanna.com/tech-book/docs/ai-ml-dc/4-congestion-avoidance-in-ai-fabric/</guid><description>&lt;h2 id="congestion-avoidance-in-ai-fabric---part-i-explicit-congestion-notification-ecn">
 Congestion Avoidance in AI Fabric - Part I: Explicit Congestion Notification (ECN)
 &lt;a class="anchor" href="#congestion-avoidance-in-ai-fabric---part-i-explicit-congestion-notification-ecn">#&lt;/a>
&lt;/h2>
&lt;p>As explained in the preceding chapter, “Egress Interface Congestions,” both the Rail switch links to GPU servers and the inter-switch links can become congested during gradient synchronization. It is essential to implement congestion control mechanisms specifically designed for RDMA workloads in AI fabric back-end networks because congestion slows down the learning process and even a single packet loss may restart the whole training process.&lt;/p></description></item><item><title>5. Load Balancing in AI Fabric</title><link>https://prasenjitmanna.com/tech-book/docs/ai-ml-dc/5-load-balancing-in-ai-fabric/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://prasenjitmanna.com/tech-book/docs/ai-ml-dc/5-load-balancing-in-ai-fabric/</guid><description>&lt;h2 id="ai-for-network-engineers-understanding-flow-flowlet-and-packet-based-load-balancing">
 AI for Network Engineers: Understanding Flow, Flowlet, and Packet-Based Load Balancing
 &lt;a class="anchor" href="#ai-for-network-engineers-understanding-flow-flowlet-and-packet-based-load-balancing">#&lt;/a>
&lt;/h2>
&lt;p>Though BGP supports the traditional Flow-based Layer 3 Equal Cost Multi-Pathing (ECMP) traffic load balancing method, it is not the best fit for a RoCEv2-based AI backend network. This is because GPU-to-GPU communication creates massive elephant flows, which RDMA-capable NICs transmit at line rate. These flows can easily cause congestion in the backend network.&lt;/p></description></item><item><title>6. Backend Network Topologies for AI Fabrics</title><link>https://prasenjitmanna.com/tech-book/docs/ai-ml-dc/6-backend-network-topologies-for-ai-fabric/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://prasenjitmanna.com/tech-book/docs/ai-ml-dc/6-backend-network-topologies-for-ai-fabric/</guid><description>&lt;h1 id="intro">
 Intro
 &lt;a class="anchor" href="#intro">#&lt;/a>
&lt;/h1>
&lt;p>&lt;strong>References:&lt;/strong>&lt;/p></description></item></channel></rss>