[{"id":0,"href":"/tech-book/docs/","title":"Example Site","section":"Introduction","content":"Introduction #  Ferre hinnitibus erat accipitrem dixi Troiae tollens #  Lorem markdownum, a quoque nutu est quodcumque mandasset veluti. Passim inportuna totidemque nympha fert; repetens pendent, poenarum guttura sed vacet non, mortali undas. Omnis pharetramque gramen portentificisque membris servatum novabis fallit de nubibus atque silvas mihi. Dixit repetitaque Quid; verrit longa; sententia mandat quascumque nescio solebat litore; noctes. Hostem haerentem circuit plenaque tamen.\n Pedum ne indigenae finire invergens carpebat Velit posses summoque De fumos illa foret  Est simul fameque tauri qua ad #  Locum nullus nisi vomentes. Ab Persea sermone vela, miratur aratro; eandem Argolicas gener.\nMe sol #  Nec dis certa fuit socer, Nonacria dies manet tacitaque sibi? Sucis est iactata Castrumque iudex, et iactato quoque terraeque es tandem et maternos vittis. Lumina litus bene poenamque animos callem ne tuas in leones illam dea cadunt genus, et pleno nunc in quod. Anumque crescentesque sanguinis progenies nuribus rustica tinguet. Pater omnes liquido creditis noctem.\nif (mirrored(icmp_dvd_pim, 3, smbMirroredHard) != lion(clickImportQueue, viralItunesBalancing, bankruptcy_file_pptp)) { file += ip_cybercrime_suffix; } if (runtimeSmartRom == netMarketingWord) { virusBalancingWin *= scriptPromptBespoke + raster(post_drive, windowsSli); cd = address_hertz_trojan; soap_ccd.pcbServerGigahertz(asp_hardware_isa, offlinePeopleware, nui); } else { megabyte.api = modem_flowchart - web + syntaxHalftoneAddress; } if (3 \u0026lt; mebibyteNetworkAnimated) { pharming_regular_error *= jsp_ribbon + algorithm * recycleMediaKindle( dvrSyntax, cdma); adf_sla *= hoverCropDrive; templateNtfs = -1 - vertical; } else { expressionCompressionVariable.bootMulti = white_eup_javascript( table_suffix); guidPpiPram.tracerouteLinux += rtfTerabyteQuicktime(1, managementRosetta(webcamActivex), 740874); } var virusTweetSsl = nullGigo;  Trepident sitimque #  Sentiet et ferali errorem fessam, coercet superbus, Ascaniumque in pennis mediis; dolor? Vidit imi Aeacon perfida propositos adde, tua Somni Fluctibus errante lustrat non.\nTamen inde, vos videt e flammis Scythica parantem rupisque pectora umbras. Haec ficta canistris repercusso simul ego aris Dixit! Esse Fama trepidare hunc crescendo vigor ululasse vertice exspatiantur celer tepidique petita aversata oculis iussa est me ferro.\n"},{"id":1,"href":"/tech-book/docs/algorithms/easy/","title":"Easy Complexity","section":"Algorithms","content":"Easy Complexity #   "},{"id":2,"href":"/tech-book/docs/systemdesign-tips/google-drive/","title":"Google Drive - Design","section":"SystemDesign-Tips","content":"1. Gathering System Requirements #  As with any systems design interview question, the first thing that we want to do is to gather system requirements; we need to figure out what system we\u0026rsquo;re building exactly. We\u0026rsquo;re designing the core user flow of the Google Drive web application. This consists of storing two main entities: folders and files. More specifically, the system should allow users to create folders, upload and download files, and rename and move entities once they\u0026rsquo;re stored. We don\u0026rsquo;t have to worry about ACLs, sharing entities, or any other auxiliary Google Drive features.\nWe\u0026rsquo;re going to be building this system at a very large scale, assuming 1 billion users, each with 15GB of data stored in Google Drive on average. This adds up to approximately 15,000 PB of data in total, without counting any metadata that we might store for each entity, like its name or its type.\nWe need this service to be Highly Available and also very redundant. No data that\u0026rsquo;s successfully stored in Google Drive can ever be lost, even through catastrophic failures in an entire region of the world.\n2. Coming Up With A Plan #  It\u0026rsquo;s important to organize ourselves and to lay out a clear plan regarding how we\u0026rsquo;re going to tackle our design. What are the major, distinguishable components of our how system?\nFirst of all, we\u0026rsquo;ll need to support the following operations:\nFor Files UploadFile DownloadFile DeleteFile RenameFile MoveFile For Folders CreateFolder GetFolder DeleteFolder RenameFolder MoveFolder Secondly, we\u0026rsquo;ll have to come up with a proper storage solution for two types of data:\nFile Contents: The contents of the files uploaded to Google Drive. These are opaque bytes with no particular structure or format. Entity Info: The metadata for each entity. This might include fields like entityID, ownerID, lastModified, entityName, entityType. This list is non-exhaustive, and we\u0026rsquo;ll most likely add to it later on. Let\u0026rsquo;s start by going over the storage solutions that we want to use, and then we\u0026rsquo;ll go through what happens when each of the operations outlined above is performed.\n3. Storing Entity Info #  To store entity information, we can use key-value stores. Since we need high availability and data replication, we need to use something like Etcd, Zookeeper, or Google Cloud Spanner (as a K-V store) that gives us both of those guarantees as well as consistency (as opposed to DynamoDB, for instance, which would give us only eventual consistency).\nSince we\u0026rsquo;re going to be dealing with many gigabytes of entity information (given that we\u0026rsquo;re serving a billion users), we\u0026rsquo;ll need to shard this data across multiple clusters of these K-V stores. Sharding on entityID means that we\u0026rsquo;ll lose the ability to perform batch operations, which these key-value stores give us out of the box and which we\u0026rsquo;ll need when we move entities around (for instance, moving a file from one folder to another would involve editing the metadata of 3 entities; if they were located in 3 different shards that wouldn\u0026rsquo;t be great). Instead, we can shard based on the ownerID of the entity, which means that we can edit the metadata of multiple entities atomically with a transaction, so long as the entities belong to the same user.\nGiven the traffic that this website needs to serve, we can have a layer of proxies for entity information, load balanced on a hash of the ownerID. The proxies could have some caching, as well as perform ACL checks when we eventually decide to support them. The proxies would live at the regional level, whereas the source-of-truth key-value stores would be accessed globally.\n4. Storing File Data #  When dealing with potentially very large uploads and data storage, it\u0026rsquo;s often advantageous to split up data into blobs that can be pieced back together to form the original data. When uploading a file, the request will be load balanced across multiple servers that we\u0026rsquo;ll call \u0026ldquo;blob splitters\u0026rdquo;, and these blob splitters will have the job of splitting files into blobs and storing these blobs in some global blob-storage solution like GCS or S3 (since we\u0026rsquo;re designing Google Drive, it might not be a great idea to pick S3 over GCS :P).\nOne thing to keep in mind is that we need a lot of redundancy for the data that we\u0026rsquo;re uploading in order to prevent data loss. So we\u0026rsquo;ll probably want to adopt a strategy like: try pushing to 3 different GCS buckets and consider a write successful only if it went through in at least 2 buckets. This way we always have redundancy without necessarily sacrificing availability. In the background, we can have an extra service in charge of further replicating the data to other buckets in an async manner. For our main 3 buckets, we\u0026rsquo;ll want to pick buckets in 3 different availability zones to avoid having all of our redundant storage get wiped out by potential catastrophic failures in the event of a natural disaster or huge power outage.\nIn order to avoid having multiple identical blobs stored in our blob stores, we\u0026rsquo;ll name the blobs after a hash of their content. This technique is called Content-Addressable Storage, and by using it, we essentially make all blobs immutable in storage. When a file changes, we simply upload the entire new resulting blobs under their new names computed by hashing their new contents.\nThis immutability is very powerful, in part because it means that we can very easily introduce a caching layer between the blob splitters and the buckets, without worrying about keeping caches in sync with the main source of truth when edits are made\u0026ndash;an edit just means that we\u0026rsquo;re dealing with a completely different blob.\n5. Entity Info Structure #  Since folders and files will both have common bits of metadata, we can have them share the same structure. The difference will be that folders will have an is_folder flag set to true and a list of children_ids, which will point to the entity information for the folders and files within the folder in question. Files will have an is_folder flag set to false and a blobs field, which will have the IDs of all of the blobs that make up the data within the relevant file. Both entities can also have a parent_id field, which will point to the entity information of the entity\u0026rsquo;s parent folder. This will help us quickly find parents when moving files and folders.\nFile Info { blobs: [\u0026lsquo;blob_content_hash_0\u0026rsquo;, \u0026lsquo;blob_content_hash_1\u0026rsquo;], id: \u0026lsquo;some_unique_entity_id\u0026rsquo; is_folder: false, name: \u0026lsquo;some_file_name\u0026rsquo;, owner_id: \u0026lsquo;id_of_owner\u0026rsquo;, parent_id: \u0026lsquo;id_of_parent\u0026rsquo;, } Folder Info { children_ids: [\u0026lsquo;id_of_child_0\u0026rsquo;, \u0026lsquo;id_of_child_1\u0026rsquo;], id: \u0026lsquo;some_unique_entity_id\u0026rsquo; is_folder: true, name: \u0026lsquo;some_folder_name\u0026rsquo;, owner_id: \u0026lsquo;id_of_owner\u0026rsquo;, parent_id: \u0026lsquo;id_of_parent\u0026rsquo;, }\n6. Garbage Collection #  Any change to an existing file will create a whole new blob and de-reference the old one. Furthermore, any deleted file will also de-reference the file\u0026rsquo;s blobs. This means that we\u0026rsquo;ll eventually end up with a lot of orphaned blobs that are basically unused and taking up storage for no reason. We\u0026rsquo;ll need a way to get rid of these blobs to free some space.\nWe can have a Garbage Collection service that watches the entity-info K-V stores and keeps counts of the number of times every blob is referenced by files; these counts can be stored in a SQL table.\nReference counts will get updated whenever files are uploaded and deleted. When the reference count for a particular blob reaches 0, the Garbage Collector can mark the blob in question as orphaned in the relevant blob stores, and the blob will be safely deleted after some time if it hasn\u0026rsquo;t been accessed.\n7. End To End API Flow #  Now that we\u0026rsquo;ve designed the entire system, we can walk through what happens when a user performs any of the operations we listed above.\nCreateFolder is simple; since folders don\u0026rsquo;t have a blob-storage component, creating a folder just involves storing some metadata in our key-value stores.\nUploadFile works in two steps. The first is to store the blobs that make up the file in the blob storage. Once the blobs are persisted, we can create the file-info object, store the blob-content hashes inside its blobs field, and write this metadata to our key-value stores.\nDownloadFile fetches the file\u0026rsquo;s metadata from our key-value stores given the file\u0026rsquo;s ID. The metadata contains the hashes of all of the blobs that make up the content of the file, which we can use to fetch all of the blobs from blob storage. We can then assemble them into the file and save it onto local disk.\nAll of the Get, Rename, Move, and Delete operations atomically change the metadata of one or several entities within our key-value stores using the transaction guarantees that they give us.\nSystem Diagram Final Systems Architecture  "},{"id":3,"href":"/tech-book/docs/algorithms/medium/","title":"Medium Complexity","section":"Algorithms","content":"Medium Complexity #     Number Swapper: Write a function to swap a number in place (that is, without temporary variables). Hints - with just addition/substruction arithmatic, XOR.\n  Tic Tac Win: Design an algorithm to figure out if someone has won a game of tic-tac-toe.\n  Hashing #    Two Sum: Find a pair in array whose sum equals to the target input: [10,4,1,3,2] Target: 7 Output: [4,3]\nHint - insert in hash and then start searching from first element, find the difference with the target and find the hash.\n  Logest consecutive sequence: find length of longest consecutive sequence from given array input: [10, 4, 20, 1,3,2] Output: [1,2,3,4]\n  Sliding Window #    Two Sum: find a pair whose sum is equal to target\ninput: [10,4,1,3,2] Target: 7 Output: [4,3]\nHint - sort, start from bengining and end at the same time\n  Traping Water: Given an arrary of height of bars (width = 1) calculate the amount of water trapped.\ninput: 5, 2, 3, 4, 1, 3 Output = 5\nHint - use sliding window.\n  Recursion #    Combination Sum: Return the list of numbers from given array whose sum = target\n  Generate all pairs of valid parenthesis\nInput: 2\nOutput: {\n​\t()(),\n​\t(())\n​\t}\n  Binary Tree #    Height of the binary tree\nHints - use recursion\n  Diameter of the binary tree\nThe diameter of the binary tree is the length of the longest path between any two nodes in the tree.\nHints - use recursion\n  Convert to the Sum Tree: Convert it such that every node\u0026rsquo;s value is equal to sum of its left and right sub tree.\nHints - use recursion\n  Maximum path sum in binary tree\n  Lowest common ancestor in a binary tree\n  "},{"id":4,"href":"/tech-book/posts/2022-01-18-first-doc/","title":"First Blog","section":"Blog","content":"Preface #  This is my black board for my future technical book. There is no structure of this blog posts. Whenever I find a good technical literature, I am planning to add it here.\nFeedback is very important for any development cycle. Please drop a message at prasenjit.manna@gmail.com.\nThanks, Prasenjit Manna\n"},{"id":5,"href":"/tech-book/docs/algorithms/","title":"Algorithms","section":"Example Site","content":"Algorithms #   In this sections, all the interesting algorithms will be classified into simple, medium and hard.\n"},{"id":6,"href":"/tech-book/docs/programming-tips/","title":"Programming Tips","section":"Example Site","content":"Programming Tips #   Bit Manipulation #   XORing a bit with 1 always flips the bit, whereas XO Ring with O will never change it.  Miscellaneous #    Passing a 2D array to a C++ function\nThere are three ways to pass a 2D array to a function:\n  The parameter is a 2D array\nint array[10][10]; void passFunc(int a[][10]) {  // ... } passFunc(array);   The parameter is an array containing pointers\nint *array[10]; for(int i = 0; i \u0026lt; 10; i++)  array[i] = new int[10];  void passFunc(int *a[10]) //Array containing pointers {  // ... } passFunc(array);   The parameter is a pointer to a pointer\nint **array; array = new int *[10]; for(int i = 0; i \u0026lt;10; i++)  array[i] = new int[10]; void passFunc(int **a) {  // ... } passFunc(array);     "},{"id":7,"href":"/tech-book/docs/systemdesign-tips/","title":"SystemDesign-Tips","section":"Example Site","content":"System-Tips #   In this sections, all the essential concents will be described.\nStorage #   Disk - HDD(Hard-disk drive) and SSD(solid state drive). SSD is faster than HDD, hence costlier also. Persistent Storage. Memory - RAM (Random access momory). Volatile storage  Latency and Throughput #    Latency - Time it takes for a certain operation to complete, unit msec or sec.\n  Reading 1 MB from RAM: 250 us (0.25ms)\n  Reading 1 MB from SSD: 1,000 ps (1 ms)\n  Reading 1 MB from HDD: 20,000 is (20 ms)\n  Transfer 1 MB over Network: 10,000 pus (10 ms)\n  Inter-Continental Round Trip: 150,000 ps (150 ms)\n    Throughput - The number of operations that a system can handle properly per time unit. For instance the throughput of a sec measured in requests per second (RPS or QPS).\n  Availability #    Availability - The odds of a particular server or service being up and running at any point in time, usually measured in percentages. A server that has 99% availability will be operational 99% of the time (this would be described as having two nines of availability).\n  High Availability - Used to describe systems that have particularly high levels of availability, typically 5 nines or more; sometimes abbreviated \u0026ldquo;HA\u0026rdquo;,\n  Nines - Typically refers to percentages of uptime. For example, 5 nines of availability means an uptime of 99.999% of the time. Below are the downtimes expected per year depending on those 9s:\n  99% (two 9s): 87.7 hours\n  99.9% (three 9s): 8.8 hours\n  99.99%: 52.6 minutes\n  99.999%: 5.3 minutes\n    Caching #    Cache - A piece of hardware or software that stores data, typically meant to retrieve that data faster than otherwise. Caches are often used to store responses to network requests as well as results of computationally tong operations. Note that data in a cache can become stale if the main source of truth for that data (i.e., the main database behind the cache) gets updated and the cache doesn\u0026rsquo;t.\n  Cache Hit When requested data is found in a cache.\n  Cache Miss When requested data could have been found in a cache but isn\u0026rsquo;t. This is typically used to refer to a negative consequence of a system failure or of a poor design choice. For example: If a server goes down, our load balancer will have to forward requests to a new server, which will result in cache misses.\n  Cache Eviction Policy The policy by which values get evicted or removed from a cache. Popular cache eviction policies include LRU (least-recently used), FIFO (first in first out), and LFU (least-frequently used).\n  Content Delivery Network A CDN is a third-party service that acts like a cache for your servers. Sometimes, web applications can be slow for users in a particular region if your servers are located only in another region. A CDN has servers all around the world, meaning that the latency to a CDN\u0026rsquo;s servers will almost always be far better than the latency to your servers. A CDN\u0026rsquo;s servers are often referred to as PoPs (Points of Presence). Two of the most popular CDNs are Cloudflare and Google Claud CDN.\n  Proxies #    Forward Proxy A server that sits between a client and servers and acts on behalf of the client, typically used to mask the client\u0026rsquo;s identity (IP address), Note that forward proxies are often referred to as just proxies.\n  Reverse Proxy A server that sits between clients and servers and acts on behalf of the servers, typically used for logging, load balancing, or caching. | nginx @ Pronounced \u0026ldquo;engine X°\u0026ndash;not \u0026ldquo;N jinx”, Nginx is a very popular webserver that\u0026rsquo;s often used as a reverse proxy and load balancer. Learn more: https://www.nginx.com/\n  Load Balancer A type of reverse proxy that distributes traffic across servers, Load balancers can be found in many parts of a system, from the DNS layer all the way to the database layer. Learn more: https://www.nginx.com/\n  Server-Selection Strategy How a load balancer chooses servers when distributing traffic amongst multiple servers. Commonly used strategies include roundrobin, random selection, performance-based selection (choosing the server with the best performance metrics, like the fastest response time or the least amount of traffic), and IP-based routing.\n  Databases #    Relational Database A type of structured database in which data is stored following a tabular format; often supports powerful querying using SQL. Learn more: https://www.postgresql.org/\n  Non-Relational Database In contrast with relational database (SQL databases), a type of database that is free of Imposed, tabular-like structure. Non-relational databases are often referred to as NoSQL databases,\n  ACID Transaction\n Atomicity: The operations that constitute the transaction will either all succeed or ail fail. There is no in-between state. Consistency: The transaction cannot bring the database to an invalid state. After the transaction is committed or rolled back, the rules for each record will still apply, and all future transactions will see the effect of the transaction. Also named Strong Consistency. isolation: The execution of multiple transactions concurrently will have the same effect as if they had been executed sequentially. Durability: Any committed transaction is written to non-volatile storage. It will not be undone by a crash, power loss, or network partition.    Strong Consistency Strong Consistency usually refers to the consistency of ACID transactions, as opposed to Eventual Consistency.\n  Eventual Consistency A consistency mode which is unlike Strong Consistency. In this model, reads might return a view of the system that is stale. An eventually consistent datastore will give guarantees that the state of the database will eventuall reflect writes within a time period.\n  No-SQL Databases #    Key-Value Store A Key-Value Store is a flexible NoSQL database that\u0026rsquo;s often used for caching and dynamic configuration. Popular aptions include DynamoDB, Etcd, Redis, and ZooKeeper,\n  etcd Etcd is a strongly consistent and highly available key-value store that\u0026rsquo;s often used to Implement leader election in a system, Learn more: https://etcd.io/\n  Redis An in-memory key-value store. Does offer some persistent storage options but is typically used as a really fast, best-effort caching solution, Redis is also often used to implement rate limiting. Learn more: https://redis.io/\n  Zookeeper Zookeeper Is a strongly consistent, highly available key-value store. It\u0026rsquo;s often used to store important configuration of to perform leader election. Learn more: https://zookeeper.apache.org/\n  DynamoDB An key-value store by AWS, this provides eventual consistency.\n    Blob Storage Widely used kind of storage, in small and large scale systems. They don’t really count as databases per se, partially because they only allow the user to store and retrieve data based on the name of the blob. This is sort of like a key-value store but usually blob stores have different guarantees. They might be slower than KV stores but values can be megabytes large (or sometimes gigabytes large). Usually people use this to store things like large binaries, database snapshots, or images and other static assets that a website might have. Blob storage is rather complicated to have on premise, and only giant companies like Google and Amazon have infrastructure that supports it. So usually in the context of System Design interviews you can assume that you will be able to use GCS or S3. These are blob storage services hosted by Google and Amazon respectively, that cost money depending on how much storage you use and how often you store and retrieve blobs from that storage.\n  Google Cloud Storage(GCS) - is a blob storage service provided by Google. Learn more: https://cloud.google.com/storage\n  S3 - ls a blob storage service provided by Amazon through Amazon Web Services (AWS). Learn more: https://aws.amazon.com/s3/\n    Time Series Database A TSDB is a special kind of database optimized for storing and analyzing time-indexed data: data points that specifically occur at a given moment in time. Examples of TSDBs are InfluxDB, Prometheus, and Graphite.\n  influxDB - popular open-source time series database, Learn more; https://www.influxdata.com/\n  Prometheus - A popular open source time series database, typically used for monitoring purposes. https://prometheus.io\n    Graph Database A type of database that stores data following the graph data model. Data entries in a graph database can have explicitly defined relationships, much like nodes in a graph can have edges. Graph databases take advantage of their underlying graph structure to perform complex queries on deeply connected data very fast. Graph databases are thus often preferred to relational databases when dealing with systems where data points naturally form a graph and have multiple tevels of relationships—for example, social networks.\n Neo4j - a popular grpah DB, consists of nodes, relationships, propreties and labels. https://neo4j.com    Spatial Database A type of database optimized for storing and querying spatial data like locations on a map. Spatial databases rely on spatial indexes fike quadtrees to quickly perform spatial queries like finding all locations in the vicinity of a region.\n  Replication \u0026amp; Shrading #    Replication - The act of duplicating the data from one database server to others. This is sometimes used to increase the redundancy of your system and tolerate regional failures for instance. Other times you can use replication to move data closer to your clients, thus decreasing latency of accessing specific data.\n  Sharding - Sometimes called data partitioning, sharding is the act of splitting a database into two or more pieces called shards and is typically done to increase the throughput of your database. Popular sharding strategies include:\n Sharding based on a client\u0026rsquo;s region. Sharding based on the type of data being stored (e.g: user data gets stored in one shard, payments data gets stored In another shard) Sharding based on the hash of a column (only for structured data)    Peer-To-Peer Networks #    Peer-To-Peer Network A collection of machines referred to as peers that divide a workload between themselves to presumably complete the workload faster than would otherwise be possible. Peer-to-peer networks are often used in file-distribution systems.\n  Gossip Protocol When a set of machines talk to each other in a uncoordinated manner in a cluster to spread information through a system without requiring a central source of data. - ad °\n  Rate Limiting #    Rate Limiting The act of limiting the number of requests sent to or from a system. Rate limiting is most often used to limit the number of incoming requests in order to prevent DoS attacks and can be enforced at the IP-address level, at the user-account level, or at the region level, for example. Rate limiting can also be implemented in tiers; for instance, a type of network request could be limited to 1 per second, 5 per 10 seconds, and 10 per minute.\n  DoS Attack Short for “denial-of-service attack”, a DoS attack is an attack in which a malicious user tries to bring down or damage a system in order to render it unavailable to users. Much of the time, it consists of flooding it with traffic. Some DoS attacks are easily preventable with rate limiting, while others can be far trickier to defend against.\n  DDoS Attack\nShort for “distributed denial-of-service attack\u0026rdquo;, a DDoS attack is a DoS attack in which the traffic flooding the target system comes from many different sources (like thousands of machines), making It much harder to defend against.\n  Redis An in-memory key-value store. Does offer some persistent storage options but Is typically used as a really fast, best-effort caching solution, Redis is also often used to implement rate limiting. Learn more: https //redis.io/\n  Publish/Subscribe Pattern #    Publish/Subscribe Pattern Often shortened as Pub/Sub, the Publish/Subscribe pattern Is a popular messaging model that consists of publishers and subscribers. Publishers publish messages to special topics (sometimes called channels) without caring about or even knowing who will read those messages, and subscribers subscribe to topics and read messages coming through those topics. Pub/Sub systems often come with very powerful guarantees like at-least-once delivery, persistent storage, ordering of messages, and replayability of messages.\n  Apache Kafka A distributed messaging system created by Linkedin. Very useful when using the streaming paradigm as opposed to polling. Learn more: https.//kafka,apache, org/\n  Cloud pub/sub A highly-scalable Pub/Sub messaging service created by Google, Guarantees at-least-once delivery of messages and supports “rewinding” in order to reprocess messages. Learn more: https://cloud.google.com/pubsub/\n  MapReduce #    MapReduce A popular framework for processing very large datasets in a distributed setting efficiently, quickly, and in a fault-tolerant manner. A MapReduce job is comprised of 3 main steps:\n the Map step, which runs a map function on the various chunks of the dataset and transforms these chunks into intermediate key-value pairs. the Shuffle step, which reorganizes the intermediate key-value pairs such that pairs of the same key are routed to the same machine in the final step. the Reduce step, which runs a reduce function on the newly shuffled key-value pairs and transforms them into more meaningful data. The canonical example of a MapReduce use case is counting the number of occurrences of words in a large text file. When dealing with a MapReduce library, engineers and/or systems administrators only need to worry about the map and reduce functions, as well as their inputs and outputs. All other concerns, including the parallelization of tasks and the fault-tolerance of the MapReduce job, are abstracted away and taken care of by the MapReduce implementation.    Distributed File System A Distributed Ale System is an abstraction over a (usually large) cluster of machines that allows them to act like one large file system. The two most popular implementations of a DFS are the Google File System (GFS) and the Hadoop Distributed File System (HDFS). Typically, DFSs take care of the classic availability and replication guarantees that can be tricky to obtain in a distributed-system setting, The overarching idea is that files are split into chunks of a certain size (4MB or 64MB, for instance), and those chunks are sharded across a large cluster of machines. A central control plane is in charge of deciding where each chunk resides, routing reads to the right nodes, and handling communication between machines, Olfferent DFS implementations have slightly different APIs and semantics, but they achieve the same common goal: extremely largescale persistent storage,\n  Hadoop A popular, open-source framework that supports MapReduce jobs and many other kinds of data-processing pipelines. Its central component is HDFS (Hadoop Distributed File System), on top of which other technologies have been developed. Learn more: https://hadoop.apache.org/\n  Security And HTTPS #    Symmetric Encryption A type of encryption that relies on only a single key to both encrypt and decrypt data. The key must be known to all parties involved in the communication and must therefore typically be shared between the parties at one point or another. Symmetric-key algorithms tend to be faster than their asymmetric counterparts. The most widely used symmetric-key algorithms are part of the Advanced Encryption Standard (AES).\n  Asymmetric Encryption Also known as public-key encryption, asymmetric encryption relies on two keys—a public key and a private key—to encrypt and decrypt data. The keys are generated using cryptographic algorithms and are mathematically connected such that data encrypted with the public key can only be decrypted with the private key. While the private key must be kept secure to maintain the fidelity of this encryption paradigm, the public key can be openly shared. Asymmetric-key algorithms tend to be slower than their symmetric counterparts.\n  AES Stands for Advanced Encryption Standard. AES is a widely used encryption standard that has three symmetric-key algorithms (AES-128, AES-192, and AES-256). Of note, AES is considered to be the \u0026ldquo;gold standard\u0026rdquo; in encryption and is even used by the U.S. National Security Agency to encrypt top secret information. .\n  HTTPS The HyperText Transfer Protocol Secure is an extension of HTTP that\u0026rsquo;s used for secure communication online. It requires servers to have trusted certificates (usually SSL certificates) and uses the Transport Layer Security (TLS), a security protocol built on top of TCP, to encrypt data communicated between a client and a server. { TLs The Transport Layer Security is a security protocol over which HTTP runs in order to achieve secure communication online. \u0026ldquo;HTTP over TLS\u0026rdquo; Is also known as HTTPS.\n  SSL Certificate A digital certificate granted to a server by a certificate authority. Contains the server\u0026rsquo;s public key, to be used as part of the TLS handshake process in an HTTPS connection. An SSL certificate effectively confirms that a public key belongs to the server claiming it belongs to them. SSL certificates are a crucial defense against man-in-the-middie attacks,\n  Certificate Authority A trusted entity that signs digital certificates—namely, SSL certificates that are relied on in HTTPS connections.\n  TLS Handshake The process through which a client and a server communicating over HTTPS exchange encryption-related information and establish a secure communication. The typical steps in a TLS handshake are roughly as follows:\n The client sends a client hello string of random bytes—to the server. The server responds with a server hello another string of random bytes—as well as its SSL certificate, which contains its publle key. The client verifies that the certificate was issued by a certificate authority and sends a premaster secret—yet another string of random bytes, this time encrypted with the server\u0026rsquo;s public key—to the server. The client and the server use the client hello, the server helio, and the premaster secret to then generate the same symmetric encryption session keys, to be used to encrypt and decrypt all data communicated during the remainder of the connection.    "}]