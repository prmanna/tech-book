[{"id":0,"href":"/tech-book/docs/","title":"Example Site","section":"Introduction","content":" Introduction # Ferre hinnitibus erat accipitrem dixi Troiae tollens # Lorem markdownum, a quoque nutu est quodcumque mandasset veluti. Passim inportuna totidemque nympha fert; repetens pendent, poenarum guttura sed vacet non, mortali undas. Omnis pharetramque gramen portentificisque membris servatum novabis fallit de nubibus atque silvas mihi. Dixit repetitaque Quid; verrit longa; sententia mandat quascumque nescio solebat litore; noctes. Hostem haerentem circuit plenaque tamen.\nPedum ne indigenae finire invergens carpebat Velit posses summoque De fumos illa foret Est simul fameque tauri qua ad # Locum nullus nisi vomentes. Ab Persea sermone vela, miratur aratro; eandem Argolicas gener.\nMe sol # Nec dis certa fuit socer, Nonacria dies manet tacitaque sibi? Sucis est iactata Castrumque iudex, et iactato quoque terraeque es tandem et maternos vittis. Lumina litus bene poenamque animos callem ne tuas in leones illam dea cadunt genus, et pleno nunc in quod. Anumque crescentesque sanguinis progenies nuribus rustica tinguet. Pater omnes liquido creditis noctem.\nif (mirrored(icmp_dvd_pim, 3, smbMirroredHard) != lion(clickImportQueue, viralItunesBalancing, bankruptcy_file_pptp)) { file += ip_cybercrime_suffix; } if (runtimeSmartRom == netMarketingWord) { virusBalancingWin *= scriptPromptBespoke + raster(post_drive, windowsSli); cd = address_hertz_trojan; soap_ccd.pcbServerGigahertz(asp_hardware_isa, offlinePeopleware, nui); } else { megabyte.api = modem_flowchart - web + syntaxHalftoneAddress; } if (3 \u0026lt; mebibyteNetworkAnimated) { pharming_regular_error *= jsp_ribbon + algorithm * recycleMediaKindle( dvrSyntax, cdma); adf_sla *= hoverCropDrive; templateNtfs = -1 - vertical; } else { expressionCompressionVariable.bootMulti = white_eup_javascript( table_suffix); guidPpiPram.tracerouteLinux += rtfTerabyteQuicktime(1, managementRosetta(webcamActivex), 740874); } var virusTweetSsl = nullGigo; Trepident sitimque # Sentiet et ferali errorem fessam, coercet superbus, Ascaniumque in pennis mediis; dolor? Vidit imi Aeacon perfida propositos adde, tua Somni Fluctibus errante lustrat non.\nTamen inde, vos videt e flammis Scythica parantem rupisque pectora umbras. Haec ficta canistris repercusso simul ego aris Dixit! Esse Fama trepidare hunc crescendo vigor ululasse vertice exspatiantur celer tepidique petita aversata oculis iussa est me ferro.\n"},{"id":1,"href":"/tech-book/docs/algorithms/breadth-first-search/","title":"Breadth First Search","section":"Algorithms","content":" Breadth First Search # Intro # Hopefully, by this time, you\u0026rsquo;ve drunk enough DFS kool-aid to understand its immense power and seen enough visualization to create a call stack in your mind. Now let me introduce the companion spell Breadth First Search (BFS). The names are self-explanatory. While depth first search reaches for depth (child nodes) first before breadth (nodes in the same level/depth), breadth first search visits all nodes in a level before starting to visit the next level. While DFS uses recursion/stack to keep track of progress, BFS uses a queue (First In First Out). When we dequeue a node, we enqueue its children.\nBFS template # from collections import deque def bfs_by_queue(root): queue = deque([root]) # at least one element in the queue to kick start bfs while len(queue) \u0026gt; 0: # as long as there is an element in the queue node = queue.popleft() # dequeue for child in node.children: # enqueue children if OK(child): # early return if problem condition met return FOUND(child) queue.append(child) return NOT_FOUND "},{"id":2,"href":"/tech-book/docs/algorithms/depth-first-search/","title":"Depth First Search","section":"Algorithms","content":" Depth First Search # Intro # The pre-order traversal of a tree is DFS.\nNode\u0026lt;int\u0026gt;* dfs(Node\u0026lt;int\u0026gt;* root, int target) { if (root == nullptr) return nullptr; if (root-\u0026gt;val == target) return root; // return non-null return value from the recursive calls Node\u0026lt;int\u0026gt;* left = dfs(root-\u0026gt;left, target); if (left != nullptr) return left; // at this point, we know left is null, and right could be null or non-null // we return right child\u0026#39;s recursive call result directly because // - if it\u0026#39;s non-null we should return it // - if it\u0026#39;s null, then both left and right are null, we want to return null return dfs(root-\u0026gt;right, target); } Max depth of a binary tree # Max depth of a binary tree is the longest root-to-leaf path. Given a binary tree, find its max depth. Here, we define the length of the path to be the number of edges on that path, not the number of nodes.\nint dfs(Node\u0026lt;int\u0026gt;* root) { // Null node adds no depth if (root == nullptr) return 0; // num nodes in longest path of current subtree = max num nodes of its two subtrees + 1 current node return std::max(dfs(root-\u0026gt;left), dfs(root-\u0026gt;right)) + 1; } int tree_max_depth(Node\u0026lt;int\u0026gt;* root) { return root? dfs(root) - 1 : 0; } Visible Tree Node | Number of Visible Nodes # In a binary tree, a node is labeled as \u0026ldquo;visible\u0026rdquo; if, on the path from the root to that node, there isn\u0026rsquo;t any node with a value higher than this node\u0026rsquo;s value.\nThe root is always \u0026ldquo;visible\u0026rdquo; since there are no other nodes between the root and itself. Given a binary tree, count the number of \u0026ldquo;visible\u0026rdquo; nodes.\nint dfs(Node\u0026lt;int\u0026gt;* root, int max_sofar) { if (!root) return 0; int total = 0; if (root-\u0026gt;val \u0026gt;= max_sofar) total++; total += dfs(root-\u0026gt;left, std::max(max_sofar, root-\u0026gt;val)); total += dfs(root-\u0026gt;right, std::max(max_sofar, root-\u0026gt;val)); return total; } int visible_tree_node(Node\u0026lt;int\u0026gt;* root) { // start max_sofar with smallest number possible so any value root has is greater than it return dfs(root, std::numeric_limits\u0026lt;int\u0026gt;::min()); } "},{"id":3,"href":"/tech-book/docs/algorithms/easy/","title":"Easy Complexity","section":"Algorithms","content":" Easy Complexity # "},{"id":4,"href":"/tech-book/docs/algorithms/two-pointers/","title":"Two Pointers \u0026 Sliding Window","section":"Algorithms","content":" Two Pointers # Valid Palindrome # Determine whether a string is a palindrome, ignoring non-alphanumeric characters and case. Examples:\nInput: Do geese see God? Output: True\nInput: Was it a car or a cat I saw? Output: True\nInput: A brown fox jumping over Output: False\n#include \u0026lt;cctype\u0026gt; // isalnum, tolower #include \u0026lt;iostream\u0026gt; // boolalpha, cin, cout #include \u0026lt;string\u0026gt; // getline bool is_palindrome(std::string s) { int l = 0, r = s.size() - 1; while (l \u0026lt; r) { // Note 1, 2 while (l \u0026lt; r \u0026amp;\u0026amp; !std::isalnum(s[l])) { l++; } while (l \u0026lt; r \u0026amp;\u0026amp; !std::isalnum(s[r])) { r--; } // compare characters ignoring case if (std::tolower(s[l]) != std::tolower(s[r])) return false; l++; r--; } return true; } int main() { std::string s; std::getline(std::cin, s); bool res = is_palindrome(s); std::cout \u0026lt;\u0026lt; std::boolalpha \u0026lt;\u0026lt; res \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; } Remove Duplicates # Given a sorted list of numbers, remove duplicates and return the new length. You must do this in-place and without using extra memory.\nInput: [0, 0, 1, 1, 1, 2, 2].\nOutput: 3.\nYour function should modify the list in place so the first 3 elements becomes 0, 1, 2. Return 3 because the new length is 3.\n#include \u0026lt;algorithm\u0026gt; // copy #include \u0026lt;iostream\u0026gt; // boolalpha, cin, cout #include \u0026lt;iterator\u0026gt; // back_inserter, istream_iterator, ostream_iterator, prev #include \u0026lt;sstream\u0026gt; // istringstream #include \u0026lt;string\u0026gt; // getline, string #include \u0026lt;vector\u0026gt; // vector int remove_duplicates(std::vector\u0026lt;int\u0026gt;\u0026amp; arr) { int slow = 0; for (int fast = 0; fast \u0026lt; arr.size(); fast++) { if (arr.at(fast) != arr.at(slow)) { slow++; arr.at(slow) = arr.at(fast); } } return slow + 1; } template\u0026lt;typename T\u0026gt; std::vector\u0026lt;T\u0026gt; get_words() { std::string line; std::getline(std::cin, line); std::istringstream ss{line}; ss \u0026gt;\u0026gt; std::boolalpha; std::vector\u0026lt;T\u0026gt; v; std::copy(std::istream_iterator\u0026lt;T\u0026gt;{ss}, std::istream_iterator\u0026lt;T\u0026gt;{}, std::back_inserter(v)); return v; } template\u0026lt;typename T\u0026gt; void put_words(const std::vector\u0026lt;T\u0026gt;\u0026amp; v) { if (!v.empty()) { std::copy(v.begin(), std::prev(v.end()), std::ostream_iterator\u0026lt;T\u0026gt;{std::cout, \u0026#34; \u0026#34;}); std::cout \u0026lt;\u0026lt; v.back(); } std::cout \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; } int main() { std::vector\u0026lt;int\u0026gt; arr = get_words\u0026lt;int\u0026gt;(); int res = remove_duplicates(arr); arr.resize(res); put_words(arr); } Sliding Window # Sliding window problems is a variant of the same direction two pointers problems. The function performs on the entire interval between the two pointers instead of only at the two positions. Usually, we keep track of the overall result of the window, and when we \u0026ldquo;slide\u0026rdquo; the window (insert/remove an item), we simply manipulate the result to accomodate the changes to the window. Time complexity wise, this is much more efficient as we do not recalculate the overlapping intervals between two windows over and over again. We try to reduce a nested loop into two passes on the input (one pass with each pointer).\nFixed Size Sliding Window # Given an array (list) nums consisted of only non-negative integers, find the largest sum among all subarrays of length k in nums. For example, if the input is nums = [1, 2, 3, 7, 4, 1], k = 3, then the output would be 14 as the largest length 3 subarray sum is given by [3, 7, 4] which sums to 14.\n#include \u0026lt;algorithm\u0026gt; // copy #include \u0026lt;iostream\u0026gt; // boolalpha, cin, cout, streamsize #include \u0026lt;iterator\u0026gt; // back_inserter, istream_iterator #include \u0026lt;limits\u0026gt; // numeric_limits #include \u0026lt;sstream\u0026gt; // istringstream #include \u0026lt;string\u0026gt; // getline, string #include \u0026lt;vector\u0026gt; // vector int subarray_sum_fixed(std::vector\u0026lt;int\u0026gt; nums, int k) { int window_sum = 0; for (int i = 0; i \u0026lt; k; ++i) { window_sum = window_sum + nums[i]; } int largest = window_sum; for (int right = k; right \u0026lt; nums.size(); ++right) { int left = right - k; window_sum = window_sum - nums[left]; window_sum = window_sum + nums[right]; largest = std::max(largest, window_sum); } return largest; } template\u0026lt;typename T\u0026gt; std::vector\u0026lt;T\u0026gt; get_words() { std::string line; std::getline(std::cin, line); std::istringstream ss{line}; ss \u0026gt;\u0026gt; std::boolalpha; std::vector\u0026lt;T\u0026gt; v; std::copy(std::istream_iterator\u0026lt;T\u0026gt;{ss}, std::istream_iterator\u0026lt;T\u0026gt;{}, std::back_inserter(v)); return v; } void ignore_line() { std::cin.ignore(std::numeric_limits\u0026lt;std::streamsize\u0026gt;::max(), \u0026#39;\\n\u0026#39;); } int main() { std::vector\u0026lt;int\u0026gt; nums = get_words\u0026lt;int\u0026gt;(); int k; std::cin \u0026gt;\u0026gt; k; ignore_line(); int res = subarray_sum_fixed(nums, k); std::cout \u0026lt;\u0026lt; res \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; } Flexible Size Sliding Window - Longest # Recall finding the largest size k subarray sum of an integer array in Largest Subarray Sum. What if we dont need the largest sum among all subarrays of fixed size k, but instead, we want to find the length of the longest subarray with sum smaller than or equal to a target?\nGiven input nums = [1, 6, 3, 1, 2, 4, 5] and target = 10, then the longest subarray that does not exceed 10 is [3, 1, 2, 4], so the output is 4 (length of [3, 1, 2, 4]).\n#include \u0026lt;algorithm\u0026gt; // copy #include \u0026lt;iostream\u0026gt; // boolalpha, cin, cout, streamsize #include \u0026lt;iterator\u0026gt; // back_inserter, istream_iterator #include \u0026lt;limits\u0026gt; // numeric_limits #include \u0026lt;sstream\u0026gt; // istringstream #include \u0026lt;string\u0026gt; // getline, string #include \u0026lt;vector\u0026gt; // vector int subarray_sum_longest(std::vector\u0026lt;int\u0026gt; nums, int target) { int windowSum = 0, length = 0; int left = 0; for (int right = 0; right \u0026lt; nums.size(); ++right) { windowSum = windowSum + nums[right]; while (windowSum \u0026gt; target) { windowSum = windowSum - nums[left]; ++left; } length = std::max(length, right-left+1); } return length; } template\u0026lt;typename T\u0026gt; std::vector\u0026lt;T\u0026gt; get_words() { std::string line; std::getline(std::cin, line); std::istringstream ss{line}; ss \u0026gt;\u0026gt; std::boolalpha; std::vector\u0026lt;T\u0026gt; v; std::copy(std::istream_iterator\u0026lt;T\u0026gt;{ss}, std::istream_iterator\u0026lt;T\u0026gt;{}, std::back_inserter(v)); return v; } void ignore_line() { std::cin.ignore(std::numeric_limits\u0026lt;std::streamsize\u0026gt;::max(), \u0026#39;\\n\u0026#39;); } int main() { std::vector\u0026lt;int\u0026gt; nums = get_words\u0026lt;int\u0026gt;(); int target; std::cin \u0026gt;\u0026gt; target; ignore_line(); int res = subarray_sum_longest(nums, target); std::cout \u0026lt;\u0026lt; res \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; } Flexible Size Sliding Window - Shortest # Let\u0026rsquo;s continue on finding the sum of subarrays. This time given a positive integer array nums, we want to find the length of the shortest subarray such that the subarray sum is at least target. Recall the same example with input nums = [1, 4, 1, 7, 3, 0, 2, 5] and target = 10, then the smallest window with the sum \u0026gt;= 10 is [7, 3] with length 2. So the output is 2.\nWe\u0026rsquo;ll assume for this problem that it\u0026rsquo;s guaranteed target will not exceed the sum of all elements in nums.\n#include \u0026lt;algorithm\u0026gt; // copy #include \u0026lt;iostream\u0026gt; // boolalpha, cin, cout, streamsize #include \u0026lt;iterator\u0026gt; // back_inserter, istream_iterator #include \u0026lt;limits\u0026gt; // numeric_limits #include \u0026lt;sstream\u0026gt; // istringstream #include \u0026lt;string\u0026gt; // getline, string #include \u0026lt;vector\u0026gt; // vector int subarray_sum_shortest(std::vector\u0026lt;int\u0026gt; nums, int target) { int windowSum = 0, length = nums.size(); int left = 0; for (int right = 0; right \u0026lt; nums.size(); ++right) { windowSum = windowSum + nums[right]; while (windowSum \u0026gt;= target) { length = std::min(length, right-left+1); windowSum = windowSum - nums[left]; ++left; } } return length; } template\u0026lt;typename T\u0026gt; std::vector\u0026lt;T\u0026gt; get_words() { std::string line; std::getline(std::cin, line); std::istringstream ss{line}; ss \u0026gt;\u0026gt; std::boolalpha; std::vector\u0026lt;T\u0026gt; v; std::copy(std::istream_iterator\u0026lt;T\u0026gt;{ss}, std::istream_iterator\u0026lt;T\u0026gt;{}, std::back_inserter(v)); return v; } void ignore_line() { std::cin.ignore(std::numeric_limits\u0026lt;std::streamsize\u0026gt;::max(), \u0026#39;\\n\u0026#39;); } int main() { std::vector\u0026lt;int\u0026gt; nums = get_words\u0026lt;int\u0026gt;(); int target; std::cin \u0026gt;\u0026gt; target; ignore_line(); int res = subarray_sum_shortest(nums, target); std::cout \u0026lt;\u0026lt; res \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; } Linked List Cycle # Given a linked list with potentially a loop, determine whether the linked list from the first node contains a cycle in it. For bonus points, do this with constant space.\nbool has_cycle(Node\u0026lt;int\u0026gt;* nodes) { Node\u0026lt;int\u0026gt;* tortoise = next_node(nodes); Node\u0026lt;int\u0026gt;* hare = next_node(next_node(nodes)); while (tortoise != hare \u0026amp;\u0026amp; hare-\u0026gt;next != NULL) { tortoise = next_node(tortoise); hare = next_node(next_node(hare)); } return hare-\u0026gt;next != NULL; } "},{"id":5,"href":"/tech-book/docs/systemdesign-tips/code-deployment-system/","title":"Design A Code-Deployment System","section":"SystemDesign-Tips","content":" 1. Gathering System Requirements # As with any systems design interview question, the first thing that we want to do is to gather system requirements; we need to figure out what system we\u0026rsquo;re building exactly.\nFrom the answers we were given to our clarifying questions (see Prompt Box), we\u0026rsquo;re building a system that involves repeatedly (in the order of thousands of times per day) building and deploying code to hundreds of thousands of machines spread out across 5-10 regions around the world.\nBuilding code will involve grabbing snapshots of source code using commit SHA identifiers; beyond that, we can assume that the actual implementation details of the building action are taken care of. In other words, we don\u0026rsquo;t need to worry about how we would build JavaScript code or C++ code; we just need to design the system that enables the repeated building of code.\nBuilding code will take up to 15 minutes, it\u0026rsquo;ll result in a binary file of up to 10GB, and we want to have the entire deployment process (building and deploying code to our target machines) take at most 30 minutes.\nEach build will need a clear end-state (SUCCESS or FAILURE), and though we care about availability (2 to 3 nines), we don\u0026rsquo;t need to optimize too much on this dimension.\n2. Coming Up With A Plan # It\u0026rsquo;s important to organize ourselves and to lay out a clear plan regarding how we\u0026rsquo;re going to tackle our design. What are the major, distinguishable components of our how system?\nIt seems like this system can actually very simply be divided into two clear subsystems:\nthe Build System that builds code into binaries the Deployment System that deploys binaries to our machines across the world Note that these subsystems will of course have many components themselves, but this is a very straightforward initial way to approach our problem.\n3. Build System \u0026ndash; General Overview # From a high-level perspective, we can call the process of building code into a binary a job, and we can design our build system as a queue of jobs. Jobs get added to the queue, and each job has a commit identifier (the commit SHA) for what version of the code it should build and the name of the artifact that will be created (the name of the resulting binary). Since we\u0026rsquo;re agnostic to the type of the code being built, we can assume that all languages are handled automatically here.\nWe can have a pool of servers (workers) that are going to handle all of these jobs. Each worker will repeatedly take jobs off the queue (in a FIFO manner—no prioritization for now), build the relevant binaries (again, we\u0026rsquo;re assuming that the actual implementation details of building code are given to us), and write the resulting binaries to blob storage (Google Cloud Storage or S3 for instance). Blob storage makes sense here, because binaries are literally blobs of data.\n4. Build System \u0026ndash; Job Queue # A naive design of the job queue would have us implement it in memory (just as we would implement a queue in coding interviews), but this implementation is very problematic; if there\u0026rsquo;s a failure in our servers that hold this queue, we lose the entire state of our jobs: queued jobs and past jobs.\nIt seems like we would be unnecessarily complicating matters by trying to optimize around this in-memory type of storage, so we\u0026rsquo;re likely better off implementing the queue using a SQL database.\n5. Build System \u0026ndash; SQL Job Queue # We can have a jobs table in our SQL database where every record in the database represents a job, and we can use record-creation timestamps as the queue\u0026rsquo;s ordering mechanism.\nOur table will be:\nid: string, the ID of the job, auto-generated created_at: timestamp commit_sha: string name: string, the pointer to the job\u0026rsquo;s eventual binary in blob storage status: string, QUEUED, RUNNING, SUCCEEDED, FAILED We can implement the actual dequeuing mechanism by looking at the oldest creation_timestamp with a QUEUED status. This means that we\u0026rsquo;ll likely want to index our table on both created_at and status.\n6. Build System \u0026ndash; Concurrency # ACID transactions will make it safe for potentially hundreds of workers to grab jobs off the queue without unintentionally running the same job twice (we\u0026rsquo;ll avoid race conditions). Our actual transaction will look like this:\nBEGIN TRANSACTION; SELECT * FROM jobs_table WHERE status = \u0026#39;QUEUED\u0026#39; ORDER BY created_at ASC LIMIT 1; // if there\u0026#39;s none, we ROLLBACK; UPDATE jobs_table SET status = \u0026#39;RUNNING\u0026#39; WHERE id = id from previous query; COMMIT; All of the workers will be running this transaction every so often to dequeue the next job; let\u0026rsquo;s say every 5 seconds. If we arbitrarily assume that we\u0026rsquo;ll have 100 workers sharing the same queue, we\u0026rsquo;ll have 100/5 = 20 reads per second, which is very easy to handle for a SQL database.\n7. Build System \u0026ndash; Lost Jobs # Since we\u0026rsquo;re designing a large-scale system, we have to expect and handle edge cases. Here, what if there\u0026rsquo;s a network partition with our workers or one of our workers dies mid-build? Since builds last around 15 minutes on average, this will very likely happen. In this case, we want to avoid having a \u0026ldquo;lost job\u0026rdquo; that we were never made aware of, and with our current design, the job will remain RUNNING forever. How do we handle this?\nWe could have an extra column on our jobs table called last_heartbeat. This will be updated in a heartbeat fashion by the worker running a particular job, where that worker will update the relevant row in the table every 3-5 minutes to just let us know that it\u0026rsquo;s still running the job.\nWe can then have a completely separate service that polls the table every so often (say, every 5 minutes, depending on how responsive we want this build system to be), checks all of the RUNNING jobs, and if their last_heartbeat was last modified longer than 2 heartbeats ago (we need some margin of error here), then something\u0026rsquo;s likely wrong, and this service can reset the status of the relevant jobs to QUEUED, which would effectively bring them back to the front of the queue.\nThe transaction that this auxiliary service will perform will look something like this:\nUPDATE jobs_table SET status = \u0026#39;QUEUED\u0026#39; WHERE status = \u0026#39;RUNNING\u0026#39; AND last_heartbeat \u0026lt; NOW() - 10 minutes; 8. Build System \u0026ndash; Scale Estimation # We previously arbitrarily assumed that we would have 100 workers, which made our SQL-database queue able to handle the expected load. We should try to estimate if this number of workers is actually realistic.\nWith some back-of-the-envelope math, we can see that, since a build can take up to 15 minutes, a single worker can run 4 jobs per hour, or ~100 (96) jobs per day. Given thousands of builds per day (say, 5000-10000), this means that we would need 50-100 workers (5000 / 100). So our arbitrary figure was accurate.\nEven if the builds aren\u0026rsquo;t uniformly spread out (in other words, they peak during work hours), our system scales horizontally very easily. We can automatically add or remove workers whenever the load warrants it. We can also scale our system vertically by making our workers more powerful, thereby reducing the build time.\n9. Build System \u0026ndash; Storage # We previously mentioned that we would store binaries in blob storage (GCS). Where does this storage fit into our queueing system exactly?\nWhen a worker completes a build, it can store the binary in GCS before updating the relevant row in the jobs table. This will ensure that a binary has been persisted before its relevant job is marked as SUCCEEDED.\nSince we\u0026rsquo;re going to be deploying our binaries to machines spread across the world, it\u0026rsquo;ll likely make sense to have regional storage rather than just a single global blob store.\nWe can design our system based on regional clusters around the world (in our 5-10 global regions). Each region can have a blob store (a regional GCS bucket). Once a worker successfully stores a binary in our main blob store, the worker is released and can run another job, while the main blob store performs some asynchronous replication to store the binary in all of the regional GCS buckets. Given 5-10 regions and 10GB files, this step should take no more than 5-10 minutes, bringing our total build-and-deploy duration so far to roughly 20-25 minutes (15 minutes for a build and 5-10 minutes for global replication of the binary).\n10. Deployment System \u0026ndash; General Overview # From a high-level perspective, our actual deployment system will need to allow for the very fast distribution of 10GB binaries to hundreds of thousands of machines across all of our global regions. We\u0026rsquo;re likely going to want some service that tells us when a binary has been replicated in all regions, another service that can serve as the source of truth for what binary should currently be run on all machines, and finally a peer-to-peer-network design for our actual machines across the world.\n11. Deployment System \u0026ndash; Replication-Status Service # We can have a global service that continuously checks all regional GCS buckets and aggregates the replication status for successful builds (in other words, checks that a given binary in the main blob store has been replicated across all regions). Once a binary has been replicated across all regions, this service updates a separate SQL database with rows containing the name of a binary and a replication_status. Once a binary has a \u0026ldquo;complete\u0026rdquo; replication_status, it\u0026rsquo;s officially deployable.\n12. Deployment System \u0026ndash; Blob Distribution # Since we\u0026rsquo;re going to deploy 10 GBs to hundreds of thousands of machines, even with our regional clusters, having each machine download a 10GB file one after the other from a regional blob store is going to be extremely slow. A peer-to-peer-network approach will be much faster and will allow us to hit our 30-minute time frame for deployments. All of our regional clusters will behave as peer-to-peer networks.\n13. Deployment System \u0026ndash; Trigger # Let\u0026rsquo;s describe what happens when an engineer presses a button on some internal UI that says \u0026ldquo;Deploy build/binary B1 to every machine globally\u0026rdquo;. This is the action that triggers the binary downloads on all the regional peer-to-peer networks.\nTo simplify this process and to support having multiple builds getting deployed concurrently, we can design this in a goal-state oriented manner.\nThe goal-state will be the desired build version at any point in time and will look something like: \u0026ldquo;current_build: B1\u0026rdquo;, and this can be stored in some dynamic configuration service (a key-value store like Etcd or ZooKeeper). We\u0026rsquo;ll have a global goal-state as well as regional goal-states.\nEach regional cluster will have a K-V store that holds configuration for that cluster about what builds should be running on that cluster, and we\u0026rsquo;ll also have a global K-V store.\nWhen an engineer clicks the \u0026ldquo;Deploy build/binary B1\u0026rdquo; button, our global K-V store\u0026rsquo;s build_version will get updated. Regional K-V stores will be continuously polling the global K-V store (say, every 10 seconds) for updates to the build_version and will update themselves accordingly.\nMachines in the clusters/regions will be polling the relevant regional K-V store, and when the build_version changes, they\u0026rsquo;ll try to fetch that build from the P2P network and run the binary.\n14. System Diagram # Final Systems Architecture\n"},{"id":6,"href":"/tech-book/docs/systemdesign-tips/stock-broker/","title":"Design A Stock-Broker System","section":"SystemDesign-Tips","content":" 1. Gathering System Requirements # As with any systems design interview question, the first thing that we want to do is to gather system requirements; we need to figure out what system we\u0026rsquo;re building exactly.\nWe\u0026rsquo;re building a stock-brokerage platform like Robinhood that functions as the intermediary between end-customers and some central stock exchange. The idea is that the central stock exchange is the platform that actually executes stock trades, whereas the stockbroker is just the platform that customers talk to when they want to place a trade\u0026ndash;the stock brokerage is \u0026ldquo;simpler\u0026rdquo; and more \u0026ldquo;human-readable\u0026rdquo;, so to speak.\nWe only care about supporting market trades\u0026ndash;trades that are executed at the current stock price\u0026ndash;and we can assume that our system stores customer balances (i.e., funds that customers may have previously deposited) in a SQL table.\nWe need to design a PlaceTrade API call, and we know that the central exchange\u0026rsquo;s equivalent API method will take in a callback that\u0026rsquo;s guaranteed to be executed upon completion of a call to that API method.\nWe\u0026rsquo;re designing this system to support millions of trades per day coming from millions of customers in a single region (the U.S., for example). We want the system to be highly available.\n2. Coming Up With A Plan # It\u0026rsquo;s important to organize ourselves and to lay out a clear plan regarding how we\u0026rsquo;re going to tackle our design. What are the major, distinguishable components of our how system?\nWe\u0026rsquo;ll approach the design front to back:\nthe PlaceTrade API call that clients will make the API server(s) handling client API calls the system in charge of executing orders for each customer We\u0026rsquo;ll need to make sure that the following hold:\ntrades can never be stuck forever without either succeeding or failing to be executed a single customer\u0026rsquo;s trades have to be executed in the order in which they were placed balances can never go in the negatives 3. API Call # The core API call that we have to implement is PlaceTrade.\nWe\u0026rsquo;ll define its signature as:\nPlaceTrade( customerId: string, stockTicker: string, type: string (BUY/SELL), quantity: integer, ) =\u0026gt; ( tradeId: string, stockTicker: string, type: string (BUY/SELL), quantity: integer, createdAt: timestamp, status: string (PLACED), reason: string, ) The customer ID can be derived from an authentication token that\u0026rsquo;s only known to the user and that\u0026rsquo;s passed into the API call.\nThe status can be one of:\nPLACED IN PROGRESS FILLED REJECTED That being said, PLACED will actually be the defacto status here, because the other statuses will be asynchronously set once the exchange executes our callback. In other words, the trade status will always be PLACED when the PlaceTrade API call returns, but we can imagine that a GetTrade API call could return statuses other than PLACED.\nPotential reasons for a REJECTED trade might be:\ninsufficient funds\nrandom error\npast market hours\n4. API Server(s) # We\u0026rsquo;ll need multiple API servers to handle all of the incoming requests. Since we don\u0026rsquo;t need any caching when making trades, we don\u0026rsquo;t need any server stickiness, and we can just use some round-robin load balancing to distribute incoming requests between our API servers.\nOnce API servers receive a PlaceTrade call, they\u0026rsquo;ll store the trade in a SQL table. This table needs to be in the same SQL database as the one that the balances table is in, because we\u0026rsquo;ll need to use ACID transactions to alter both tables in an atomic way.\nThe SQL table for trades will look like this:\nid: string, a random, auto-generated string customer_id: string, the id of the customer making the trade stockTicker: string, the ticker symbol of the stock being traded type: string, either BUY or SELL quantity: integer (no fractional shares), the number of shares to trade status: string, the status of the trade; starts as PLACED created_at: timestamp, the time when the trade was created reason: string, the human-readable justification of the trade\u0026rsquo;s status The SQL table for balances will look like this:\nid: string, a random, auto-generated string customer_id: string, the id of the customer related to the balance amount: float, the amount of money that the customer has in USD last_modified: timestamp, the time when the balance was last modified 5. Trade-Execution Queue # With hundreds of orders placed every second, the trades table will be pretty massive. We\u0026rsquo;ll need to figure out a robust way to actually execute our trades and to update our table, all the while making sure of a couple of things:\nWe want to make sure that for a single customer, we only process a single BUY trade at any time, because we need to prevent the customer\u0026rsquo;s balance from ever reaching negative values. Given the nature of market orders, we never know the exact dollar value that a trade will get executed at in the exchange until we get a response from the exchange, so we have to speak to the exchange in order to know whether the trade can go through. We can design this part of our system with a Publish/Subscribe pattern. The idea is to use a message queue like Apache Kafka or Google Cloud Pub/Sub and to have a set of topics that customer ids map to. This gives us at-least-once delivery semantics to make sure that we don\u0026rsquo;t miss new trades. When a customer makes a trade, the API server writes a row to the database and also creates a message that gets routed to a topic for that customer (using hashing), notifying the topic\u0026rsquo;s subscriber that there\u0026rsquo;s a new trade.\nThis gives us a guarantee that for a single customer, we only have a single thread trying to execute their trades at any time.\nSubscribers of topics can be rings of 3 workers (clusters of servers, essentially) that use leader election to have 1 master worker do the work for the cluster (this is for our system\u0026rsquo;s high availability)\u0026ndash;the leader grabs messages as they get pushed to the topic and executes the trades for the customers contained in the messages by calling the exchange. As mentioned above, a single customer\u0026rsquo;s trades are only ever handled by the same cluster of workers, which makes our logic and our SQL queries cleaner.\nAs far as how many topics and clusters of workers we\u0026rsquo;ll need, we can do some rough estimation. If we plan to execute millions of trades per day, that comes down to about 10-100 trades per second given open trading hours during a third of a day and non-uniform trading patterns. If we assume that the core execution logic lasts about a second, then we should have roughly 10-100 topics and clusters of workers to process trades in parallel.\n~100,000 seconds per day (3600 * 24) ~1,000,000 trades per day trades bunched in 1/3rd of the day --\u0026gt; (1,000,000 / 100,000) * 3 = ~30 trades per second 6. Trade-Execution Logic # The subscribers (our workers) are streaming / waiting for messages. Imagine the following message were to arrive in the topic queue:\n{\u0026#34;customerId\u0026#34;: \u0026#34;c1\u0026#34;} The following would be pseudo-code for the worker logic:\n// We get the oldest trade that isn\u0026#39;t in a terminal state. trade = SELECT * FROM trades WHERE customer_id = \u0026#39;c1\u0026#39; AND (status = \u0026#39;PLACED\u0026#39; OR status = \u0026#39;IN PROGRESS\u0026#39;) ORDER BY created_at ASC LIMIT 1; // If the trade is PLACED, we know that it\u0026#39;s effectively // ready to be executed. We set it as IN PROGRESS. if trade.status == \u0026#34;PLACED\u0026#34; { UPDATE trades SET status = \u0026#39;IN PROGRESS\u0026#39; WHERE id = trade.id; } // In the event that the trade somehow already exists in the // exchange, the callback will do the work for us. if exchange.TradeExists(trade.id) { return; } // We get the balance for the customer. balance = SELECT amount FROM balances WHERE customer_id = \u0026#39;c1\u0026#39;; // This is the callback that the exchange will execute once // the trade actually completes. We\u0026#39;ll define it further down // in the walkthrough. callback = ... exchange.Execute( trade.stockTicker, trade.type, trade.quantity, max_price = balance, callback, ) 7. Exchange Callback # Below is some pseudo code for the exchange callback:\nfunction exchange_callback(exchange_trade) { if exchange_trade.status == \u0026#39;FILLED\u0026#39; { BEGIN TRANSACTION; trade = SELECT * FROM trades WHERE id = database_trade.id; if trade.status \u0026lt;\u0026gt; \u0026#39;IN PROGRESS\u0026#39; { ROLLBACK; pubsub.send({customer_id: database_trade.customer_id}); return; } UPDATE balances SET amount -= exchange_trade.amount WHERE customer_id = database_trade.customer_id; UPDATE trades SET status = \u0026#39;FILLED\u0026#39; WHERE id = database_trade.id; COMMIT; } else if exchange_trade.status == \u0026#39;REJECTED\u0026#39; { BEGIN TRANSACTION; UPDATE trades SET status = \u0026#39;REJECTED\u0026#39; WHERE id = database_trade.id; UPDATE trades SET reason = exchange_trade.reason WHERE id = database_trade.id; COMMIT; } pubsub.send({customer_id: database_trade.customer_id}); return http.status(200); } 8. System Diagram # Final Systems Architecture\n"},{"id":7,"href":"/tech-book/docs/systemdesign-tips/design-amazon/","title":"Design Amazon","section":"SystemDesign-Tips","content":" 1. Gathering System Requirements # As with any systems design interview question, the first thing that we want to do is to gather system requirements; we need to figure out what system we\u0026rsquo;re building exactly.\nWe\u0026rsquo;re designing the e-commerce side of the Amazon website, and more specifically, the system that supports users searching for items on the Amazon home page, adding items to cart, submitting orders, and those orders being assigned to relevant Amazon warehouses for shipment.\nWe need to handle items going out of stock, and we\u0026rsquo;ve been given some guidelines for a simple \u0026ldquo;stock-reservation\u0026rdquo; system when users begin the checkout process.\nWe have access to two smart services: one that handles user search queries and one that handles warehouse order assignment. It\u0026rsquo;s our job to figure out how these services fit into our larger design.\nWe\u0026rsquo;ll specifically be designing the system that supports amazon.com (i.e., Amazon\u0026rsquo;s U.S. operations), and we\u0026rsquo;ll assume that this system can be replicated for other regional Amazon stores. For the rest of this walkthrough, whenever we refer to \u0026ldquo;Amazon,\u0026rdquo; we\u0026rsquo;ll be referring specifically to Amazon\u0026rsquo;s U.S. store.\nWhile the system should have low latency when searching for items and high availability in general, serving roughly 10 orders per second in the U.S., we\u0026rsquo;ve been told to focus mostly on core functionality.\n2. Coming Up With A Plan # We\u0026rsquo;ll tackle this system by first looking at a high-level overview of how it\u0026rsquo;ll be set up, then diving into its storage components, and finally looking at how the core functionality comes to life. We can divide the core functionality into two main sections:\nThe user side. The warehouse side. We can further divide the user side as follows:\nBrowsing items given a search term. Modifying the cart. Beginning the checkout process. Submitting and canceliing orders. 3. High-Level System Overview # Within a region, user and warehouse requests will get round-robin-load-balanced to respective sets of API servers, and data will be written to and read from a SQL database for that region.\nWe\u0026rsquo;ll go with a SQL database because all of the data that we\u0026rsquo;ll be dealing with (items, carts, orders, etc.) is, by nature, structured and lends itself well to a relational model.\n4. SQL Tables # We\u0026rsquo;ll have six SQL tables to support our entire system\u0026rsquo;s storage needs.\nItems\nThis table will store all of the items on Amazon, with each row representing an item.\nitemId: uuid name: string description: string price: integer currency: enum other\u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; Carts\nThis table will store all of the carts on Amazon, with each row representing a cart. We\u0026rsquo;ve been told that each user can only have a single cart at once.\ncartId: uuid customerId: uuid items: []{itemId, quantity} \u0026hellip; \u0026hellip; \u0026hellip; Orders\nThis table will store all of the orders on Amazon, with each row representing an order.\n| orderId: uuid |\tcustomerId: uuid\t| orderStatus: enum |\titems: []{itemId, quantity} |\tprice: integer |\tpaymentInfo: PaymentInfo\t| shippingAddress: string\t| timestamp: datetime\t| other\u0026hellip; | | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; | | \u0026hellip; |\t\u0026hellip; |\t\u0026hellip; |\t\u0026hellip; |\t\u0026hellip; |\t\u0026hellip; |\t\u0026hellip; |\t\u0026hellip; |\t\u0026hellip;|\nAggregated Stock\nThis table will store all of the item stocks on Amazon that are relevant to users, with each row representing an item. See the Core User Functionality section for more details.\nitemId: uuid stock: integer \u0026hellip; \u0026hellip; Warehouse Orders\nThis table will store all of the orders that Amazon warehouses get, with each row representing a warehouse order. Warehouse orders are either entire normal Amazon orders or subsets of normal Amazon orders.\nwarehouseOrderId: uuid parentOrderId: uuid warehouseId: uuid orderStatus: enum items: []{itemId, quantity} shippingAddress: string \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; Warehouse Stock\nThis table will store all of the item stocks in Amazon warehouses, with each row representing an {item, warehouse} pairing. The physicalStock field represents an item\u0026rsquo;s actual physical stock in the warehouse in question, serving as a source of truth, while the availableStock field represents an item\u0026rsquo;s effective available stock in the relevant warehouse; this stock gets decreased when orders are assigned to warehouses. See the Core Warehouse Functionality section for more details.\nitemId: uuid warehouseId: uuid physicalStock: integer availableStock: integer \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; 5. Core User Functionality # GetItemCatalog(search)\nThis is the endpoint that users call when they\u0026rsquo;re searching for items. The request is routed by API servers to the smart search-results service, which interacts directly with the items table, caches popular item searches, and returns the results.\nThe API servers also fetch the relevant item stocks from the aggregated_stock table.\nUpdateCartItemQuantity(itemId, quantity)\nThis is the endpoint that users call when they\u0026rsquo;re adding or removing items from their cart. The request writes directly to the carts table, and users can only call this endpoint when an item has enough stock in the aggregated_stock table.\nBeginCheckout() \u0026amp; CancelCheckout()\nThese are the endpoints that users call when they\u0026rsquo;re beginning the checkout process and cancelling it. The BeginCheckout request triggers another read of the aggregated_stock table for the relevant items. If some of the items in the cart don\u0026rsquo;t have enough stock anymore, the UI alerts the users accordingly. For items that do have enough stock, the API servers write to the aggregated_stock table and decrease the relevant stocks accordingly, effectively \u0026ldquo;reserving\u0026rdquo; the items during the duration of the checkout. The CancelCheckout request, which also gets automatically called after 10 minutes of being in the checkout process, writes to the aggregated_stock table and increases the relevant stocks accordingly, thereby \u0026ldquo;unreserving\u0026rdquo; the items. Note that all of the writes to the aggregated_stock are ACID transactions, which allows us to comfortably rely on this SQL table as far as stock correctness is concerned.\nSubmitOrder(), CancelOrder(), \u0026amp; GetMyOrders()\nThese are the endpoints that users call when they\u0026rsquo;re submitting and cancelling orders. Both the SubmitOrder and CancelOrder requests write to the orders table, and CancelOrder also writes to the aggregated_stock table, increasing the relevant stocks accordingly (SubmitOrder doesn\u0026rsquo;t need to because the checkout process already has). GetMyOrders simply reads from the orders table. Note that an order can only be cancelled if it hasn\u0026rsquo;t yet been shipped, which is knowable from the orderStatus field.\n6. Core Warehouse Functionality # On the warehouse side of things, we\u0026rsquo;ll have the smart order-assignment service read from the orders table, figure out the best way to split orders up and assign them to warehouses based on shipping addresses, item stocks, and other data points, and write the final warehouse orders to the warehouse_orders table.\nIn order to know which warehouses have what items and how many, the order-assignment service will rely on the availableStock of relevant items in the warehouse_stock table. When the service assigns an order to a warehouse, it decreases the availableStock of the relevant items for the warehouse in question in the warehouse_stock table. These availableStock values are re-increased by the relevant warehouse if its order ends up being cancelled.\nWhen warehouses get new item stock, lose item stock for whatever reason, or physically ship their assigned orders, they\u0026rsquo;ll update the relevant physicalStock values in the warehouse_stock table. If they get new item stock or lose item stock, they\u0026rsquo;ll also write to the aggregated_stock table (they don\u0026rsquo;t need to do this when shipping assigned orders, since the aggregated_stock table already gets updated by the checkout process on the user side of things).\n7. System Diagram # Final Systems Architecture\n"},{"id":8,"href":"/tech-book/docs/systemdesign-tips/design-slack/","title":"Design Slack","section":"SystemDesign-Tips","content":" 1. Gathering System Requirements # As with any systems design interview question, the first thing that we want to do is to gather system requirements; we need to figure out what system we\u0026rsquo;re building exactly.\nWe\u0026rsquo;re designing the core communication system behind Slack, which allows users to send instant messages in Slack channels.\nSpecifically, we\u0026rsquo;ll want to support:\nLoading the most recent messages in a Slack channel when a user clicks on the channel. Immediately seeing which channels have unread messages for a particular user when that user loads Slack. Immediately seeing which channels have unread mentions of a particular user, for that particular user, when that user loads Slack, and more specifically, the number of these unread mentions in each relevant channel. Sending and receiving Slack messages instantly, in real time. Cross-device synchronization: if a user has both the Slack desktop app and the Slack mobile app open, with an unread channel in both, and if they read this channel on one device, the second device should immediately be updated and no longer display the channel as unread. The system should have low latencies and high availability, catering to a single region of roughly 20 million users. The largest Slack organizations will have as many as 50,000 users, with channels of the same size within them.\nThat being said, for the purpose of this design, we should primarily focus on latency and core functionality; availability and regionality can be disregarded, within reason.\n2. Coming Up With A Plan # We\u0026rsquo;ll tackle this system by dividing it into two main sections:\nHandling what happens when a Slack app loads. Handling real-time messaging as well as cross-device synchronization. We can further divide the first section as follows:\nSeeing all of the channels that a user is a part of. Seeing messages in a particular channel. Seeing which channels have unread messages. Seeing which channels have unread mentions and how many they have. 3. Persistent Storage Solution \u0026amp; App Load # While a large component of our design involves real-time communication, another large part of it involves retrieving data (channels, messages, etc.) at any given time when the Slack app loads. To support this, we\u0026rsquo;ll need a persistent storage solution.\nSpecifically, we\u0026rsquo;ll opt for a SQL database since we can expect this data to be structured and to be queried frequently.\nWe can start with a simple table that\u0026rsquo;ll store every Slack channel.\nChannels\nid (channelId): uuid orgId: uuid name: string description: string \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; Then, we can have another simple table representing channel-member pairs: each row in this table will correspond to a particular user who is in a particular channel. We\u0026rsquo;ll use this table, along with the one above, to fetch a user\u0026rsquo;s relevant when the app loads.\nChannel Members\nid: uuid orgId: uuid channelId: uuid userId: uuid \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; We\u0026rsquo;ll naturally need a table to store all historical messages sent on Slack. This will be our largest table, and it\u0026rsquo;ll be queried every time a user fetches messages in a particular channel. The API endpoint that\u0026rsquo;ll interact with this table will return a paginated response, since we\u0026rsquo;ll typically only want the 50 or 100 most recent messages per channel.\nAlso, this table will only be queried when a user clicks on a channel; we don\u0026rsquo;t want to fetch messages for all of a user\u0026rsquo;s channels on app load, since users will likely never look at most of their channels.\nHistorical Messages\nid: uuid orgId: uuid channelId: uuid senderId: uuid sentAt: timestamp body: string mentions: List \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; In order not to fetch recent messages for every channel on app load, all the while supporting the feature of showing which channels have unread messages, we\u0026rsquo;ll need to store two extra tables: one for the latest activity in each channel (this table will be updated whenever a user sends a message in a channel), and one for the last time a particular user has read a channel (this table will be updated whenever a user opens a channel).\nLatest Channel Timestamps\nid: uuid orgId: uuid channelId: uuid lastActive: timestamp \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; Channel Read Receipts\nid: uuid orgId: uuid channelId: uuid userId: uuid lastSeen: timestamp \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; For the number of unread user mentions that we want to display next to channel names, we\u0026rsquo;ll have another table similar to the read-receipts one, except this one will have a count of unread user mentions instead of a timestamp. This count will be updated (incremented) whenever a user tags another user in a channel message, and it\u0026rsquo;ll also be updated (reset to 0) whenever a user opens a channel with unread mentions of themself.\nUnread Channel-User-Mention Counts\nid: uuid orgId: uuid channelId: uuid userId: uuid count: int \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; 4. Load Balancing # For all of the API calls that clients will issue on app load, including writes to our database (when sending a message or marking a channel as read), we\u0026rsquo;re going to want to load balance.\nWe can have a simple round-robin load balancer, forwarding requests to a set of server clusters that will then handle passing requests to our database.\n5. \u0026ldquo;Smart\u0026rdquo; Sharding # Since our tables will be very large, especially the messages table, we\u0026rsquo;ll need to have some sharding in place.\nThe natural approach is to shard based on organization size: we can have the biggest organizations (with the biggest channels) in their individual shards, and we can have smaller organizations grouped together in other shards.\nAn important point to note here is that, over time, organization sizes and Slack activity within organizations will change. Some organizations might double in size overnight, others might experience seemingly random surges of activity, etc.. This means that, despite our relatively sound sharding strategy, we might still run into hot spots, which is very bad considering the fact that we care about latency so much.\nTo handle this, we can add a \u0026ldquo;smart\u0026rdquo; sharding solution: a subsystem of our system that\u0026rsquo;ll asynchronously measure organization activity and \u0026ldquo;rebalance\u0026rdquo; shards accordingly. This service can be a strongly consistent key-value store like Etcd or ZooKeeper, mapping orgIds to shards. Our API servers will communicate with this service to know which shard to route requests to.\n6. Pub/Sub System for Real-Time Behavior # There are two types of real-time behavior that we want to support:\nSending and receiving messages in real time. Cross-device synchronization (instantly marking a channel as read if you have Slack open on two devices and read the channel on one of them). For both of these functionalities, we can rely on a Pub/Sub messaging system, which itself will rely on our previously described \u0026ldquo;smart\u0026rdquo; sharding strategy.\nEvery Slack organization or group of organizations will be assigned to a Kafka topic, and whenever a user sends a message in a channel or marks a channel as read, our previously mentioned API servers, which handle speaking to our database, will also send a Pub/Sub message to the appropriate Kafka topic.\nThe Pub/Sub messages will look like:\n{ \u0026#34;type\u0026#34;: \u0026#34;chat\u0026#34;, \u0026#34;orgId\u0026#34;: \u0026#34;AAA\u0026#34;, \u0026#34;channelId\u0026#34;: \u0026#34;BBB\u0026#34;, \u0026#34;userId\u0026#34;: \u0026#34;CCC\u0026#34;, \u0026#34;messageId\u0026#34;: \u0026#34;DDD\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2020-08-31T01:17:02\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;this is a message\u0026#34;, \u0026#34;mentions\u0026#34;: [\u0026#34;CCC\u0026#34;, \u0026#34;EEE\u0026#34;] }, { \u0026#34;type\u0026#34;: \u0026#34;read-receipt\u0026#34;, \u0026#34;orgId\u0026#34;: \u0026#34;AAA\u0026#34;, \u0026#34;channelId\u0026#34;: \u0026#34;BBB\u0026#34;, \u0026#34;userId\u0026#34;: \u0026#34;CCC\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2020-08-31T01:17:02\u0026#34; } We\u0026rsquo;ll then have a different set of API servers who subscribe to the various Kakfa topics (probably one API server cluster per topic), and our clients (Slack users) will establish long-lived TCP connections with these API server clusters to receive Pub/Sub messages in real time.\nWe\u0026rsquo;ll want a load balancer in between the clients and these API servers, which will also use the \u0026ldquo;smart\u0026rdquo; sharding strategy to match clients with the appropriate API servers, which will be listening to the appropriate Kafka topics.\nWhen clients receive Pub/Sub messages, they\u0026rsquo;ll handle them accordingly (mark a channel as unread, for example), and if the clients refresh their browser or their mobile app, they\u0026rsquo;ll go through the entire \u0026ldquo;on app load\u0026rdquo; system that we described earlier.\nSince each Pub/Sub message comes with a timestamp, and since reading a channel and sending Slack messages involve writing to our persistent storage, the Pub/Sub messages will effectively be idempotent operations.\n7. System Diagram # Final Systems Architecture\n"},{"id":9,"href":"/tech-book/docs/systemdesign-tips/google-drive/","title":"Google Drive - Design","section":"SystemDesign-Tips","content":" 1. Gathering System Requirements # As with any systems design interview question, the first thing that we want to do is to gather system requirements; we need to figure out what system we\u0026rsquo;re building exactly.\nWe\u0026rsquo;re designing the core user flow of the Google Drive web application. This consists of storing two main entities: folders and files. More specifically, the system should allow users to create folders, upload and download files, and rename and move entities once they\u0026rsquo;re stored. We don\u0026rsquo;t have to worry about ACLs, sharing entities, or any other auxiliary Google Drive features.\nWe\u0026rsquo;re going to be building this system at a very large scale, assuming 1 billion users, each with 15GB of data stored in Google Drive on average. This adds up to approximately 15,000 PB of data in total, without counting any metadata that we might store for each entity, like its name or its type.\nWe need this service to be Highly Available and also very redundant. No data that\u0026rsquo;s successfully stored in Google Drive can ever be lost, even through catastrophic failures in an entire region of the world.\n2. Coming Up With A Plan # It\u0026rsquo;s important to organize ourselves and to lay out a clear plan regarding how we\u0026rsquo;re going to tackle our design. What are the major, distinguishable components of our how system?\nFirst of all, we\u0026rsquo;ll need to support the following operations:\nFor Files\nUploadFile DownloadFile DeleteFile RenameFile MoveFile For Folders\nCreateFolder GetFolder DeleteFolder RenameFolder MoveFolder Secondly, we\u0026rsquo;ll have to come up with a proper storage solution for two types of data:\nFile Contents: The contents of the files uploaded to Google Drive. These are opaque bytes with no particular structure or format. Entity Info: The metadata for each entity. This might include fields like entityID, ownerID, lastModified, entityName, entityType. This list is non-exhaustive, and we\u0026rsquo;ll most likely add to it later on. Let\u0026rsquo;s start by going over the storage solutions that we want to use, and then we\u0026rsquo;ll go through what happens when each of the operations outlined above is performed.\n3. Storing Entity Info # To store entity information, we can use key-value stores. Since we need high availability and data replication, we need to use something like Etcd, Zookeeper, or Google Cloud Spanner (as a K-V store) that gives us both of those guarantees as well as consistency (as opposed to DynamoDB, for instance, which would give us only eventual consistency).\nSince we\u0026rsquo;re going to be dealing with many gigabytes of entity information (given that we\u0026rsquo;re serving a billion users), we\u0026rsquo;ll need to shard this data across multiple clusters of these K-V stores. Sharding on entityID means that we\u0026rsquo;ll lose the ability to perform batch operations, which these key-value stores give us out of the box and which we\u0026rsquo;ll need when we move entities around (for instance, moving a file from one folder to another would involve editing the metadata of 3 entities; if they were located in 3 different shards that wouldn\u0026rsquo;t be great). Instead, we can shard based on the ownerID of the entity, which means that we can edit the metadata of multiple entities atomically with a transaction, so long as the entities belong to the same user.\nGiven the traffic that this website needs to serve, we can have a layer of proxies for entity information, load balanced on a hash of the ownerID. The proxies could have some caching, as well as perform ACL checks when we eventually decide to support them. The proxies would live at the regional level, whereas the source-of-truth key-value stores would be accessed globally.\n4. Storing File Data # When dealing with potentially very large uploads and data storage, it\u0026rsquo;s often advantageous to split up data into blobs that can be pieced back together to form the original data. When uploading a file, the request will be load balanced across multiple servers that we\u0026rsquo;ll call \u0026ldquo;blob splitters\u0026rdquo;, and these blob splitters will have the job of splitting files into blobs and storing these blobs in some global blob-storage solution like GCS or S3 (since we\u0026rsquo;re designing Google Drive, it might not be a great idea to pick S3 over GCS :P).\nOne thing to keep in mind is that we need a lot of redundancy for the data that we\u0026rsquo;re uploading in order to prevent data loss. So we\u0026rsquo;ll probably want to adopt a strategy like: try pushing to 3 different GCS buckets and consider a write successful only if it went through in at least 2 buckets. This way we always have redundancy without necessarily sacrificing availability. In the background, we can have an extra service in charge of further replicating the data to other buckets in an async manner. For our main 3 buckets, we\u0026rsquo;ll want to pick buckets in 3 different availability zones to avoid having all of our redundant storage get wiped out by potential catastrophic failures in the event of a natural disaster or huge power outage.\nIn order to avoid having multiple identical blobs stored in our blob stores, we\u0026rsquo;ll name the blobs after a hash of their content. This technique is called Content-Addressable Storage, and by using it, we essentially make all blobs immutable in storage. When a file changes, we simply upload the entire new resulting blobs under their new names computed by hashing their new contents.\nThis immutability is very powerful, in part because it means that we can very easily introduce a caching layer between the blob splitters and the buckets, without worrying about keeping caches in sync with the main source of truth when edits are made\u0026ndash;an edit just means that we\u0026rsquo;re dealing with a completely different blob.\n5. Entity Info Structure # Since folders and files will both have common bits of metadata, we can have them share the same structure. The difference will be that folders will have an is_folder flag set to true and a list of children_ids, which will point to the entity information for the folders and files within the folder in question. Files will have an is_folder flag set to false and a blobs field, which will have the IDs of all of the blobs that make up the data within the relevant file. Both entities can also have a parent_id field, which will point to the entity information of the entity\u0026rsquo;s parent folder. This will help us quickly find parents when moving files and folders.\n`File Info` `{` `blobs: [\u0026#39;blob_content_hash_0\u0026#39;, \u0026#39;blob_content_hash_1\u0026#39;],` `id: \u0026#39;some_unique_entity_id\u0026#39;` `is_folder: false,` `name: \u0026#39;some_file_name\u0026#39;,` `owner_id: \u0026#39;id_of_owner\u0026#39;,` `parent_id: \u0026#39;id_of_parent\u0026#39;,` `}` `Folder Info` `{` `children_ids: [\u0026#39;id_of_child_0\u0026#39;, \u0026#39;id_of_child_1\u0026#39;],` `id: \u0026#39;some_unique_entity_id\u0026#39;` `is_folder: true,` `name: \u0026#39;some_folder_name\u0026#39;,` `owner_id: \u0026#39;id_of_owner\u0026#39;,` `parent_id: \u0026#39;id_of_parent\u0026#39;,` `} `\n6. Garbage Collection # Any change to an existing file will create a whole new blob and de-reference the old one. Furthermore, any deleted file will also de-reference the file\u0026rsquo;s blobs. This means that we\u0026rsquo;ll eventually end up with a lot of orphaned blobs that are basically unused and taking up storage for no reason. We\u0026rsquo;ll need a way to get rid of these blobs to free some space.\nWe can have a Garbage Collection service that watches the entity-info K-V stores and keeps counts of the number of times every blob is referenced by files; these counts can be stored in a SQL table.\nReference counts will get updated whenever files are uploaded and deleted. When the reference count for a particular blob reaches 0, the Garbage Collector can mark the blob in question as orphaned in the relevant blob stores, and the blob will be safely deleted after some time if it hasn\u0026rsquo;t been accessed.\n7. End To End API Flow # Now that we\u0026rsquo;ve designed the entire system, we can walk through what happens when a user performs any of the operations we listed above.\nCreateFolder is simple; since folders don\u0026rsquo;t have a blob-storage component, creating a folder just involves storing some metadata in our key-value stores.\nUploadFile works in two steps. The first is to store the blobs that make up the file in the blob storage. Once the blobs are persisted, we can create the file-info object, store the blob-content hashes inside its blobs field, and write this metadata to our key-value stores.\nDownloadFile fetches the file\u0026rsquo;s metadata from our key-value stores given the file\u0026rsquo;s ID. The metadata contains the hashes of all of the blobs that make up the content of the file, which we can use to fetch all of the blobs from blob storage. We can then assemble them into the file and save it onto local disk.\nAll of the Get, Rename, Move, and Delete operations atomically change the metadata of one or several entities within our key-value stores using the transaction guarantees that they give us.\n8. System Diagram # Final Systems Architecture\n"},{"id":10,"href":"/tech-book/docs/networking-tips/ip-tos-dscp/","title":"IP Precedence And TOS | DSCP","section":"Networking Tips","content":" Intro # 8 Bits of Type of Service in IP Header.\nIP Precedence # RFC791/RFC1349 Interpretation\n** Bits ** 7-5 IP Precedence\n111\tNetwork Control\n110\tInternetwork Control\n101\tCritic/ECP\n100\tFlash Override\n011\tFlash\n010\tImmediate\n001\tPriority\n000\tRoutine\nBits\n4 (1 = Low Delay; 0 = Normal Delay)\n3 (1 = High Throughput; 0 = Normal Throughput)\n2 (1 = High Reliability; 0 = Normal Reliability)\n1 (1 = Minimise monetary cost (RFC 1349))\n0 (Must be 0)\nTOS | DSCP (Differentiated Services Code Point) # RFC 2474 (Differentiated Services) Interpretation\nBits\n7-2\tDSCP\n1-0\tECN (Explicit Congestion Notification)\nDefault Forwarding (DF) PHB # Typically best-effort traffic The recommended DSCP for DF is 0\nExpedited Forwarding (EF) PHB # Dedicated to low-loss, low-latency traffic The recommended DSCP for EF is 101110B (46 or 2E(hex))\nAssured Forwarding (AF) PHB # Gives assurance of delivery under prescribed conditions\nDrop probability Class 1 Class 2 Class 3 Class 4 Low AF11 (DSCP 10) 001010 AF21 (DSCP 18) 010010 AF31 (DSCP 26) 011010 AF41 (DSCP 34) 100010 Medium AF12 (DSCP 12) 001100 AF22 (DSCP 20) 010100 AF32 (DSCP 28) 011100 AF42 (DSCP 36) 100100 High AF13 (DSCP 14) 001110 AF23 (DSCP 22) 010110 AF33 (DSCP 30) 011110 AF43 (DSCP 38) 100110 Class Selector PHBs # This maintains backward compatibility with the IP precedence field.\nService class DSCP Name DSCP Value IP precedence Examples of application Standard CS0 (DF) 0 0 (000) Low-priority data CS1 8 1 (001) File transfer (FTP, SMB) Network operations, administration and management (OAM) CS2 16 2 (010) SNMP, SSH, Ping, Telnet, syslog Broadcast video CS3 24 3 (011) RTSP broadcast TV, treaming of live audio and video events, video surveillance,video-on-demand Real-time interactive CS4 32 4 (100) Gaming, low priority video conferencing Signaling CS5 40 5 (101) Peer-to-peer (SIP, H.323, H.248), NTP Network control CS6 48 6 (110) Routing protocols (OSPF, BGP, ISIS, RIP) Reserved for future use CS7 56 7 (111) DF= Default Forwarding\nPHB == Per-Hop-Behavior\nCS: Class Selector (RFC 2474) AFxy: Assured Forwarding (x=class, y=drop precedence) (RFC2597) EF: Expedited Forwarding (RFC 3246) References:\nhttps://en.wikipedia.org/wiki/Differentiated_services https://bogpeople.com/networking/dscp.shtml "},{"id":11,"href":"/tech-book/docs/algorithms/medium/","title":"Medium Complexity","section":"Algorithms","content":" Medium Complexity # Number Swapper: Write a function to swap a number in place (that is, without temporary variables). Hints - with just addition/substruction arithmatic, XOR.\nTic Tac Win: Design an algorithm to figure out if someone has won a game of tic-tac-toe.\nHashing # Two Sum: Find a pair in array whose sum equals to the target input: [10,4,1,3,2] Target: 7 Output: [4,3]\nHint - insert in hash and then start searching from first element, find the difference with the target and find the hash.\nLogest consecutive sequence: find length of longest consecutive sequence from given array input: [10, 4, 20, 1,3,2] Output: [1,2,3,4]\nSliding Window # Two Sum: find a pair whose sum is equal to target\ninput: [10,4,1,3,2] Target: 7 Output: [4,3]\nHint - sort, start from bengining and end at the same time\nTraping Water: Given an arrary of height of bars (width = 1) calculate the amount of water trapped.\ninput: 5, 2, 3, 4, 1, 3 Output = 5\nHint - use sliding window.\nRecursion # Combination Sum: Return the list of numbers from given array whose sum = target\nGenerate all pairs of valid parenthesis\nInput: 2\nOutput: {\n​\t()(),\n​\t(())\n​\t}\nBinary Tree # Height of the binary tree\nHints - use recursion\nDiameter of the binary tree\nThe diameter of the binary tree is the length of the longest path between any two nodes in the tree.\nHints - use recursion\nConvert to the Sum Tree: Convert it such that every node\u0026rsquo;s value is equal to sum of its left and right sub tree.\nHints - use recursion\nMaximum path sum in binary tree\nLowest common ancestor in a binary tree\n"},{"id":12,"href":"/tech-book/docs/networking-tips/tcp-congestion/","title":"TCP Congestion Control","section":"Networking Tips","content":" Intro # TCP is a protocol that is used to transmit information from one computer on the internet to another, and is the protocol I’ll be focused on in this post. What distinguishes TCP from other networking protocols is that it guarantees 100% transmission. This means that if you send 100kb of data from one computer to another using TCP, all 100kb will make it to the other side. This property of TCP is very powerful and is the reason that many network applications we use, such as the web and email are built on top of it. The way TCP is able to accomplish this goal of trasmitting all the information that is sent over the wire is that for every segment of data that is sent from party A to party B, party B sends an “acknowledgement” segment back to party A indicating that it got that message.\nWhen does congestion happen? # Congestion is problem in computer networks because at the end of the day, information transfer rates are limited by physical channels like ethernet cables or cellular links, and on the internet, many individual devices are connected to these links.\nDetour: What is a link? # Before I dive into what some solutions to this problem are, I want to be a little bit more specific about the properties of links. There are three important details to know about a link:\ndelay (milliseconds) - the time it takes for one packet to get from the beginning to the end of a link bandwidth (megabits/second) - the number of packets that can get through the link in a second queue - the size of the queue for storing packets waiting to be sent out if a link is full, and the strategy for managing that queue when it hits its capacity Using the analogy of the link as a pipe, you can think of the delay as the length of the pipe, and the bandwidth as the circumference of the pipe. An important statistic about a link is the bandwidth-delay product (BDP). If senders are sending more bytes than the BDP, the link’s queue will fill and eventually start dropping packets.\nApproaches # There are two main indicators: packet loss and increased round trip times for packets. When congestion happens, queues on links begin to fill up, and packets get dropped. If a sender notices packet loss, it’s a pretty good indicator that congestion is occuring. Another consequence of queues filling up though is that if packets are spending more time in a queue before making it onto the link, the round trip time, which measures the time from when the sender sends a segment out to the time that it receives an acknowledgement, will increase. While today there are congestion control schemes that take into account both of these indicators, in the original implementations of congestion control, only packet loss was used.\nTherefore, in addition to being able to avoid congestion, congestion control approaches need to be able to “explore” the available bandwidth.\nControl-Based Algorithms # The Congestion Window # A key concept to understand about any congestion control algorithm is the concept of a congestion window. The congestion window refers to the number of segments that a sender can send without having seen an acknowledgment yet. If the congestion window on a sender is set to 2, that means that after the sender sends 2 segments, it must wait to get an acknowledgment from the receiver in order to send any more. The congestion window is often referred to as the “flight size”, because it also corresponds to the number of segments “in flight” at any given point in time.\nThe higher the congestion window, more packets you’ll be able to get across to the receiver in the same time period. To understand this intuitively, if the delay on the network is 88ms, and the congestion window is 10 segments, you’ll be able to send 10 segments for every round trip (88*2 = 176 ms), and if it’s 20 segments, you’ll be able to send 20 segments in the same time period.\nOf course, the risk with raising the congestion window too high is that it will lead to congestion. The goal of a congestion control algorithm, then, is to figure out the right size congestion window to use.\nFrom a theoretical perspective, the right size congestion window to use is the bandwidth-delay product of the link, which as we discussed earlier is the full capacity of the link. The idea here is that if the congestion window is equal to the BDP of the link, it will be fully utilized, and not cause congestion.\nTCP Tahoe # TCP Tahoe is a congestion control scheme that was invented back in the 80s, when congestion was first becoming a problem on the internet. The algorithm itself is fairly simple, and grows the congestion window in two phases.\nPhase 1 # Slow Start: The algorithm begins in a state called “slow start”. In Slow Start, the congestion window grows by 1 every time an acknowledgement is received. This effectively doubles the congestion window on every round trip. If the congestion window is 4, four packets will be in flight at once, and when each of those packets is acknowledged, the congestion window will increase by 1, resulting in a window of size 8. This process continues until the congestion window hits a value called the “Slow Start Threshold” ssthresh. This is a configurable number.\nPhase 2 # Congestion Avoidance: Once the congestion window has hit the ssthresh, it moves from “slow start” into congestion avoidance mode. In congestion avoidance, the congestion window increases by 1 on every round trip. So if the congestion window is 4, the window will increase to 5 after all four of those packets in flight have been acknowledged. This increases the window much more slowly.\nIf Tahoe detects that a packet is lost, it will resend the packet, the slow start threshold is updated to be half the current congestion window, the congestion window is set back to 1, and the algorithm goes back to slow start.\nDetecting Packet Loss \u0026amp; Fast Retransmit # There are two ways that a TCP sender could detect that a packet is lost.\nThe sender “times out”. The sender puts a timeout on every packet that is sent out into the wild, and when that timeout is hit without that packet having been acknowledged, it resends the packet and sets the congestion window to 1.\nThe receiver sends back “duplicate acks”. In TCP, receivers only acknowledge packets that are sent in order. If a packet is sent out of order, it will send out an acknowledgement for the last packet it saw in order. So, if a receiver has received segments 1,2, and 3, and then receives segment #5, it will ack segment #3 again, because #5 came in out of order. In Tahoe, if a sender receives 3 duplicate acks, it considers a packet lost. This is considered “Fast Retransmit”, because it doesn’t wait for the timeout to happen.\nThere are a number of issues with this approach though, which is why it is no longer used today. In particular, it takes a really long time, especially on higher bandwidth networks, for the algorithm to actually take full advantage of the available bandwidth. This is because the window size grows pretty slowly after hitting the slow start threshold. Another issue is that packet loss doesn’t necessarily mean that congestion is occuring–some links, like WiFi, are just inherently lossy. Reacting drastically by cutting the window size to 1 isn’t necessarily always appropriate. A final issue is that this algorithm uses packet loss as the indicator for whether there’s congestion. If the packet loss is happening due to congestion, you are already too late–the window is too high, and you need to let the queues drain.\nTCP CUBIC # This algorithm was implemented in 2005, and is currently the default congestion control algorithm used on Linux systems. Like Tahoe, it relies on packet loss as the indicator of congestion. However, unlike Tahoe, it works far better on high bandwidth networks, since rather than increasing the window by 1 on every round trip, it uses, as the name would suggest, a cubic function to determine what the window size should be, and therefore grows much more quickly.\nAvoidance-Based Algorithms # BBR(Bottleneck Bandwidth and RTT) - (Bufferbloat) # This is a very recent algorithm developed by Google, and unlike Tahoe or CUBIC, uses delay as the indicator of congestion, rather than packet loss. The rough thinking behind this is that delays are a leading indicator of congestion–they occur before packets actually start getting lost. Slowing down the rate of sending before the packets get lost ends up leading to higher throughput.\nActive Queue Management # Random Early Detection # Each router is programmed to monitor its own queue length and, when it detects that congestion is imminent, to notify the source to adjust its congestion window. RED, invented by Sally Floyd and Van Jacobson in the early 1990s.\nRED is most commonly implemented such that it implicitly notifies the source of congestion by dropping one of its packets. The source is, therefore, effectively notified by the subsequent timeout or duplicate ACK. In case you haven’t already guessed, RED is designed to be used in conjunction with TCP, which currently detects congestion by means of timeouts (or some other means of detecting packet loss such as duplicate ACKs). As the “early” part of the RED acronym suggests, the gateway drops the packet earlier than it would have to, so as to notify the source that it should decrease its congestion window sooner than it would normally have. In other words, the router drops a few packets before it has exhausted its buffer space completely, so as to cause the source to slow down, with the hope that this will mean it does not have to drop lots of packets later on.\nhow RED decides when to drop a packet and what packet it decides to drop. To understand the basic idea, consider a simple FIFO queue. Rather than wait for the queue to become completely full and then be forced to drop each arriving packet (the tail drop policy), we could decide to drop each arriving packet with some drop probability whenever the queue length exceeds some drop level. This idea is called early random drop. The RED algorithm defines the details of how to monitor the queue length and when to drop a packet.\nExplicit Congestion Notification || IP \u0026amp; TCP Flags # While TCP’s congestion control mechanism was initially based on packet loss as the primary congestion signal, it has long been recognized that TCP could do a better job if routers were to send a more explicit congestion signal. That is, instead of dropping a packet and assuming TCP will eventually notice (e.g., due to the arrival of a duplicate ACK), any AQM algorithm can potentially do a better job if it instead marks the packet and continues to send it along its way to the destination. This idea was codified in changes to the IP and TCP headers known as Explicit Congestion Notification (ECN), as specified in RFC 3168.\nSpecifically, this feedback is implemented by treating two bits in the IP TOS field as ECN bits. One bit is set by the source to indicate that it is ECN-capable, that is, able to react to a congestion notification. This is called the ECT bit (ECN-Capable Transport). The other bit is set by routers along the end-to-end path when congestion is encountered, as computed by whatever AQM algorithm it is running. This is called the CE bit (Congestion Encountered).\nIn addition to these two bits in the IP header (which are transport-agnostic), ECN also includes the addition of two optional flags to the TCP header. The first, ECE (ECN-Echo), communicates from the receiver to the sender that it has received a packet with the CE bit set. The second, CWR (Congestion Window Reduced) communicates from the sender to the receiver that it has reduced the congestion window.\nBeyond TCP # Datacenters (DCTCP, On-Ramp) # There have been several efforts to optimize TCP for cloud datacenters, where Data Center TCP was one of the first. There are several aspects of the datacenter environment that warrant an approach that differs from more traditional TCP. These include:\nRound trip time for intra-DC traffic are small; Buffers in datacenter switches are also typically small; All the switches are under common administrative control, and thus can be required to meet certain standards; A great deal of traffic has low latency requirements; That traffic competes with high bandwidth flows. It should be noted that DCTCP is not just a version of TCP, but rather, a system design that changes both the switch behavior and the end host response to congestion information received from switches.\ntbd\nHTTP Performance (QUIC) # tbd\nMultipath Transport # tbd\nMobile Cellular Networks # tbd\nReferences:\nhttps://tcpcc.systemsapproach.org/aqm.html "},{"id":13,"href":"/tech-book/posts/2022-01-18-first-doc/","title":"First Blog","section":"Blog","content":" Preface # This is my black board for my future technical book. There is no structure of this blog posts. Whenever I find a good technical literature, I am planning to add it here.\nFeedback is very important for any development cycle. Please drop a message at prasenjit.manna@gmail.com.\nThanks, Prasenjit Manna\n"},{"id":14,"href":"/tech-book/docs/algorithms/","title":"Algorithms","section":"Example Site","content":" Algorithms # In this sections, all the interesting algorithms will be classified into simple, medium and hard.\n"},{"id":15,"href":"/tech-book/docs/networking-tips/","title":"Networking Tips","section":"Example Site","content":" Networking Tips # In this sections, all the interesting networking tips will be classified into simple, medium and hard.\n"},{"id":16,"href":"/tech-book/docs/programming-tips/","title":"Programming Tips","section":"Example Site","content":" Programming Tips # Bit Manipulation # XORing a bit with 1 always flips the bit, whereas XO Ring with O will never change it. Miscellaneous # Passing a 2D array to a C++ function\nThere are three ways to pass a 2D array to a function:\nThe parameter is a 2D array\nint array[10][10]; void passFunc(int a[][10]) { // ... } passFunc(array); The parameter is an array containing pointers\nint *array[10]; for(int i = 0; i \u0026lt; 10; i++) array[i] = new int[10]; void passFunc(int *a[10]) //Array containing pointers { // ... } passFunc(array); The parameter is a pointer to a pointer\nint **array; array = new int *[10]; for(int i = 0; i \u0026lt;10; i++) array[i] = new int[10]; void passFunc(int **a) { // ... } passFunc(array); "},{"id":17,"href":"/tech-book/docs/systemdesign-tips/","title":"SystemDesign-Tips","section":"Example Site","content":" System-Tips # In this sections, all the essential concents will be described.\nStorage # Disk - HDD(Hard-disk drive) and SSD(solid state drive). SSD is faster than HDD, hence costlier also. Persistent Storage. Memory - RAM (Random access momory). Volatile storage Latency and Throughput # Latency - Time it takes for a certain operation to complete, unit msec or sec.\nReading 1 MB from RAM: 250 us (0.25ms)\nReading 1 MB from SSD: 1,000 ps (1 ms)\nReading 1 MB from HDD: 20,000 is (20 ms)\nTransfer 1 MB over Network: 10,000 pus (10 ms)\nInter-Continental Round Trip: 150,000 ps (150 ms)\nThroughput - The number of operations that a system can handle properly per time unit. For instance the throughput of a sec measured in requests per second (RPS or QPS).\nAvailability # Availability - The odds of a particular server or service being up and running at any point in time, usually measured in percentages. A server that has 99% availability will be operational 99% of the time (this would be described as having two nines of availability).\nHigh Availability - Used to describe systems that have particularly high levels of availability, typically 5 nines or more; sometimes abbreviated \u0026ldquo;HA\u0026rdquo;,\nNines - Typically refers to percentages of uptime. For example, 5 nines of availability means an uptime of 99.999% of the time. Below are the downtimes expected per year depending on those 9s:\n99% (two 9s): 87.7 hours\n99.9% (three 9s): 8.8 hours\n99.99%: 52.6 minutes\n99.999%: 5.3 minutes\nCaching # Cache - A piece of hardware or software that stores data, typically meant to retrieve that data faster than otherwise. Caches are often used to store responses to network requests as well as results of computationally tong operations. Note that data in a cache can become stale if the main source of truth for that data (i.e., the main database behind the cache) gets updated and the cache doesn\u0026rsquo;t.\nCache Hit When requested data is found in a cache.\nCache Miss When requested data could have been found in a cache but isn\u0026rsquo;t. This is typically used to refer to a negative consequence of a system failure or of a poor design choice. For example: If a server goes down, our load balancer will have to forward requests to a new server, which will result in cache misses.\nCache Eviction Policy The policy by which values get evicted or removed from a cache. Popular cache eviction policies include LRU (least-recently used), FIFO (first in first out), and LFU (least-frequently used).\nContent Delivery Network A CDN is a third-party service that acts like a cache for your servers. Sometimes, web applications can be slow for users in a particular region if your servers are located only in another region. A CDN has servers all around the world, meaning that the latency to a CDN\u0026rsquo;s servers will almost always be far better than the latency to your servers. A CDN\u0026rsquo;s servers are often referred to as PoPs (Points of Presence). Two of the most popular CDNs are Cloudflare and Google Claud CDN.\nProxies # Forward Proxy A server that sits between a client and servers and acts on behalf of the client, typically used to mask the client\u0026rsquo;s identity (IP address), Note that forward proxies are often referred to as just proxies.\nReverse Proxy A server that sits between clients and servers and acts on behalf of the servers, typically used for logging, load balancing, or caching. | nginx @ Pronounced \u0026ldquo;engine X°\u0026ndash;not \u0026ldquo;N jinx”, Nginx is a very popular webserver that\u0026rsquo;s often used as a reverse proxy and load balancer. Learn more: https://www.nginx.com/\nLoad Balancer A type of reverse proxy that distributes traffic across servers, Load balancers can be found in many parts of a system, from the DNS layer all the way to the database layer. Learn more: https://www.nginx.com/\nServer-Selection Strategy How a load balancer chooses servers when distributing traffic amongst multiple servers. Commonly used strategies include roundrobin, random selection, performance-based selection (choosing the server with the best performance metrics, like the fastest response time or the least amount of traffic), and IP-based routing.\nDatabases # Relational Database A type of structured database in which data is stored following a tabular format; often supports powerful querying using SQL. Learn more: https://www.postgresql.org/\nNon-Relational Database In contrast with relational database (SQL databases), a type of database that is free of Imposed, tabular-like structure. Non-relational databases are often referred to as NoSQL databases,\nACID Transaction\nAtomicity: The operations that constitute the transaction will either all succeed or ail fail. There is no in-between state. Consistency: The transaction cannot bring the database to an invalid state. After the transaction is committed or rolled back, the rules for each record will still apply, and all future transactions will see the effect of the transaction. Also named Strong Consistency. isolation: The execution of multiple transactions concurrently will have the same effect as if they had been executed sequentially. Durability: Any committed transaction is written to non-volatile storage. It will not be undone by a crash, power loss, or network partition. Strong Consistency Strong Consistency usually refers to the consistency of ACID transactions, as opposed to Eventual Consistency.\nEventual Consistency A consistency mode which is unlike Strong Consistency. In this model, reads might return a view of the system that is stale. An eventually consistent datastore will give guarantees that the state of the database will eventuall reflect writes within a time period.\nNo-SQL Databases # Key-Value Store A Key-Value Store is a flexible NoSQL database that\u0026rsquo;s often used for caching and dynamic configuration. Popular aptions include DynamoDB, Etcd, Redis, and ZooKeeper,\netcd Etcd is a strongly consistent and highly available key-value store that\u0026rsquo;s often used to Implement leader election in a system, Learn more: https://etcd.io/\nRedis An in-memory key-value store. Does offer some persistent storage options but is typically used as a really fast, best-effort caching solution, Redis is also often used to implement rate limiting. Learn more: https://redis.io/\nZookeeper Zookeeper Is a strongly consistent, highly available key-value store. It\u0026rsquo;s often used to store important configuration of to perform leader election. Learn more: https://zookeeper.apache.org/\nDynamoDB An key-value store by AWS, this provides eventual consistency.\nBlob Storage Widely used kind of storage, in small and large scale systems. They don’t really count as databases per se, partially because they only allow the user to store and retrieve data based on the name of the blob. This is sort of like a key-value store but usually blob stores have different guarantees. They might be slower than KV stores but values can be megabytes large (or sometimes gigabytes large). Usually people use this to store things like large binaries, database snapshots, or images and other static assets that a website might have. Blob storage is rather complicated to have on premise, and only giant companies like Google and Amazon have infrastructure that supports it. So usually in the context of System Design interviews you can assume that you will be able to use GCS or S3. These are blob storage services hosted by Google and Amazon respectively, that cost money depending on how much storage you use and how often you store and retrieve blobs from that storage.\nGoogle Cloud Storage(GCS) - is a blob storage service provided by Google. Learn more: https://cloud.google.com/storage\nS3 - ls a blob storage service provided by Amazon through Amazon Web Services (AWS). Learn more: https://aws.amazon.com/s3/\nTime Series Database A TSDB is a special kind of database optimized for storing and analyzing time-indexed data: data points that specifically occur at a given moment in time. Examples of TSDBs are InfluxDB, Prometheus, and Graphite.\ninfluxDB - popular open-source time series database, Learn more; https://www.influxdata.com/\nPrometheus - A popular open source time series database, typically used for monitoring purposes. https://prometheus.io\nGraph Database A type of database that stores data following the graph data model. Data entries in a graph database can have explicitly defined relationships, much like nodes in a graph can have edges. Graph databases take advantage of their underlying graph structure to perform complex queries on deeply connected data very fast. Graph databases are thus often preferred to relational databases when dealing with systems where data points naturally form a graph and have multiple tevels of relationships—for example, social networks.\nNeo4j - a popular grpah DB, consists of nodes, relationships, propreties and labels. https://neo4j.com Spatial Database A type of database optimized for storing and querying spatial data like locations on a map. Spatial databases rely on spatial indexes fike quadtrees to quickly perform spatial queries like finding all locations in the vicinity of a region.\nReplication \u0026amp; Shrading # Replication - The act of duplicating the data from one database server to others. This is sometimes used to increase the redundancy of your system and tolerate regional failures for instance. Other times you can use replication to move data closer to your clients, thus decreasing latency of accessing specific data.\nSharding - Sometimes called data partitioning, sharding is the act of splitting a database into two or more pieces called shards and is typically done to increase the throughput of your database. Popular sharding strategies include:\nSharding based on a client\u0026rsquo;s region. Sharding based on the type of data being stored (e.g: user data gets stored in one shard, payments data gets stored In another shard) Sharding based on the hash of a column (only for structured data) Peer-To-Peer Networks # Peer-To-Peer Network A collection of machines referred to as peers that divide a workload between themselves to presumably complete the workload faster than would otherwise be possible. Peer-to-peer networks are often used in file-distribution systems.\nGossip Protocol When a set of machines talk to each other in a uncoordinated manner in a cluster to spread information through a system without requiring a central source of data. - ad °\nRate Limiting # Rate Limiting The act of limiting the number of requests sent to or from a system. Rate limiting is most often used to limit the number of incoming requests in order to prevent DoS attacks and can be enforced at the IP-address level, at the user-account level, or at the region level, for example. Rate limiting can also be implemented in tiers; for instance, a type of network request could be limited to 1 per second, 5 per 10 seconds, and 10 per minute.\nDoS Attack Short for “denial-of-service attack”, a DoS attack is an attack in which a malicious user tries to bring down or damage a system in order to render it unavailable to users. Much of the time, it consists of flooding it with traffic. Some DoS attacks are easily preventable with rate limiting, while others can be far trickier to defend against.\nDDoS Attack\nShort for “distributed denial-of-service attack\u0026rdquo;, a DDoS attack is a DoS attack in which the traffic flooding the target system comes from many different sources (like thousands of machines), making It much harder to defend against.\nRedis An in-memory key-value store. Does offer some persistent storage options but Is typically used as a really fast, best-effort caching solution, Redis is also often used to implement rate limiting. Learn more: https://redis.io/\nPublish/Subscribe Pattern # Publish/Subscribe Pattern Often shortened as Pub/Sub, the Publish/Subscribe pattern Is a popular messaging model that consists of publishers and subscribers. Publishers publish messages to special topics (sometimes called channels) without caring about or even knowing who will read those messages, and subscribers subscribe to topics and read messages coming through those topics. Pub/Sub systems often come with very powerful guarantees like at-least-once delivery, persistent storage, ordering of messages, and replayability of messages.\nApache Kafka A distributed messaging system created by Linkedin. Very useful when using the streaming paradigm as opposed to polling. Learn more: https://kafka.apache.org/\nCloud pub/sub A highly-scalable Pub/Sub messaging service created by Google, Guarantees at-least-once delivery of messages and supports “rewinding” in order to reprocess messages. Learn more: https://cloud.google.com/pubsub/\nMapReduce # MapReduce A popular framework for processing very large datasets in a distributed setting efficiently, quickly, and in a fault-tolerant manner. A MapReduce job is comprised of 3 main steps:\nthe Map step, which runs a map function on the various chunks of the dataset and transforms these chunks into intermediate key-value pairs. the Shuffle step, which reorganizes the intermediate key-value pairs such that pairs of the same key are routed to the same machine in the final step. the Reduce step, which runs a reduce function on the newly shuffled key-value pairs and transforms them into more meaningful data. The canonical example of a MapReduce use case is counting the number of occurrences of words in a large text file. When dealing with a MapReduce library, engineers and/or systems administrators only need to worry about the map and reduce functions, as well as their inputs and outputs. All other concerns, including the parallelization of tasks and the fault-tolerance of the MapReduce job, are abstracted away and taken care of by the MapReduce implementation. Distributed File System A Distributed Ale System is an abstraction over a (usually large) cluster of machines that allows them to act like one large file system. The two most popular implementations of a DFS are the Google File System (GFS) and the Hadoop Distributed File System (HDFS). Typically, DFSs take care of the classic availability and replication guarantees that can be tricky to obtain in a distributed-system setting, The overarching idea is that files are split into chunks of a certain size (4MB or 64MB, for instance), and those chunks are sharded across a large cluster of machines. A central control plane is in charge of deciding where each chunk resides, routing reads to the right nodes, and handling communication between machines, Olfferent DFS implementations have slightly different APIs and semantics, but they achieve the same common goal: extremely largescale persistent storage,\nHadoop A popular, open-source framework that supports MapReduce jobs and many other kinds of data-processing pipelines. Its central component is HDFS (Hadoop Distributed File System), on top of which other technologies have been developed. Learn more: https://hadoop.apache.org/\nSecurity And HTTPS # Symmetric Encryption A type of encryption that relies on only a single key to both encrypt and decrypt data. The key must be known to all parties involved in the communication and must therefore typically be shared between the parties at one point or another. Symmetric-key algorithms tend to be faster than their asymmetric counterparts. The most widely used symmetric-key algorithms are part of the Advanced Encryption Standard (AES).\nAsymmetric Encryption Also known as public-key encryption, asymmetric encryption relies on two keys—a public key and a private key—to encrypt and decrypt data. The keys are generated using cryptographic algorithms and are mathematically connected such that data encrypted with the public key can only be decrypted with the private key. While the private key must be kept secure to maintain the fidelity of this encryption paradigm, the public key can be openly shared. Asymmetric-key algorithms tend to be slower than their symmetric counterparts.\nAES Stands for Advanced Encryption Standard. AES is a widely used encryption standard that has three symmetric-key algorithms (AES-128, AES-192, and AES-256). Of note, AES is considered to be the \u0026ldquo;gold standard\u0026rdquo; in encryption and is even used by the U.S. National Security Agency to encrypt top secret information. .\nHTTPS The HyperText Transfer Protocol Secure is an extension of HTTP that\u0026rsquo;s used for secure communication online. It requires servers to have trusted certificates (usually SSL certificates) and uses the Transport Layer Security (TLS), a security protocol built on top of TCP, to encrypt data communicated between a client and a server. { TLs The Transport Layer Security is a security protocol over which HTTP runs in order to achieve secure communication online. \u0026ldquo;HTTP over TLS\u0026rdquo; Is also known as HTTPS.\nSSL Certificate A digital certificate granted to a server by a certificate authority. Contains the server\u0026rsquo;s public key, to be used as part of the TLS handshake process in an HTTPS connection. An SSL certificate effectively confirms that a public key belongs to the server claiming it belongs to them. SSL certificates are a crucial defense against man-in-the-middie attacks,\nCertificate Authority A trusted entity that signs digital certificates—namely, SSL certificates that are relied on in HTTPS connections.\nTLS Handshake The process through which a client and a server communicating over HTTPS exchange encryption-related information and establish a secure communication. The typical steps in a TLS handshake are roughly as follows:\nThe client sends a client hello string of random bytes—to the server. The server responds with a server hello another string of random bytes—as well as its SSL certificate, which contains its publle key. The client verifies that the certificate was issued by a certificate authority and sends a premaster secret—yet another string of random bytes, this time encrypted with the server\u0026rsquo;s public key—to the server. The client and the server use the client hello, the server helio, and the premaster secret to then generate the same symmetric encryption session keys, to be used to encrypt and decrypt all data communicated during the remainder of the connection. "}]