[{"id":0,"href":"/tech-book/docs/","title":"Example Site","section":"Introduction","content":" Introduction # Ferre hinnitibus erat accipitrem dixi Troiae tollens # Lorem markdownum, a quoque nutu est quodcumque mandasset veluti. Passim inportuna totidemque nympha fert; repetens pendent, poenarum guttura sed vacet non, mortali undas. Omnis pharetramque gramen portentificisque membris servatum novabis fallit de nubibus atque silvas mihi. Dixit repetitaque Quid; verrit longa; sententia mandat quascumque nescio solebat litore; noctes. Hostem haerentem circuit plenaque tamen.\nPedum ne indigenae finire invergens carpebat Velit posses summoque De fumos illa foret Est simul fameque tauri qua ad # Locum nullus nisi vomentes. Ab Persea sermone vela, miratur aratro; eandem Argolicas gener.\nMe sol # Nec dis certa fuit socer, Nonacria dies manet tacitaque sibi? Sucis est iactata Castrumque iudex, et iactato quoque terraeque es tandem et maternos vittis. Lumina litus bene poenamque animos callem ne tuas in leones illam dea cadunt genus, et pleno nunc in quod. Anumque crescentesque sanguinis progenies nuribus rustica tinguet. Pater omnes liquido creditis noctem.\nif (mirrored(icmp_dvd_pim, 3, smbMirroredHard) != lion(clickImportQueue, viralItunesBalancing, bankruptcy_file_pptp)) { file += ip_cybercrime_suffix; } if (runtimeSmartRom == netMarketingWord) { virusBalancingWin *= scriptPromptBespoke + raster(post_drive, windowsSli); cd = address_hertz_trojan; soap_ccd.pcbServerGigahertz(asp_hardware_isa, offlinePeopleware, nui); } else { megabyte.api = modem_flowchart - web + syntaxHalftoneAddress; } if (3 \u0026lt; mebibyteNetworkAnimated) { pharming_regular_error *= jsp_ribbon + algorithm * recycleMediaKindle( dvrSyntax, cdma); adf_sla *= hoverCropDrive; templateNtfs = -1 - vertical; } else { expressionCompressionVariable.bootMulti = white_eup_javascript( table_suffix); guidPpiPram.tracerouteLinux += rtfTerabyteQuicktime(1, managementRosetta(webcamActivex), 740874); } var virusTweetSsl = nullGigo; Trepident sitimque # Sentiet et ferali errorem fessam, coercet superbus, Ascaniumque in pennis mediis; dolor? Vidit imi Aeacon perfida propositos adde, tua Somni Fluctibus errante lustrat non.\nTamen inde, vos videt e flammis Scythica parantem rupisque pectora umbras. Haec ficta canistris repercusso simul ego aris Dixit! Esse Fama trepidare hunc crescendo vigor ululasse vertice exspatiantur celer tepidique petita aversata oculis iussa est me ferro.\n"},{"id":1,"href":"/tech-book/docs/algorithms/breadth-first-search/","title":"Breadth First Search","section":"Algorithms","content":" Breadth First Search # Intro # Hopefully, by this time, you\u0026rsquo;ve drunk enough DFS kool-aid to understand its immense power and seen enough visualization to create a call stack in your mind. Now let me introduce the companion spell Breadth First Search (BFS). The names are self-explanatory. While depth first search reaches for depth (child nodes) first before breadth (nodes in the same level/depth), breadth first search visits all nodes in a level before starting to visit the next level. While DFS uses recursion/stack to keep track of progress, BFS uses a queue (First In First Out). When we dequeue a node, we enqueue its children.\nBFS template # from collections import deque def bfs_by_queue(root): queue = deque([root]) # at least one element in the queue to kick start bfs while len(queue) \u0026gt; 0: # as long as there is an element in the queue node = queue.popleft() # dequeue for child in node.children: # enqueue children if OK(child): # early return if problem condition met return FOUND(child) queue.append(child) return NOT_FOUND "},{"id":2,"href":"/tech-book/docs/programming-tips/c++/","title":"C++ Tips","section":"Programming Tips","content":" C++ Tips # Array # An array in C++ holds a fixed number of elements of a single type in contiguous memory locations.\nstd::string clothes[3]; // declare a string array of size 3 named clothes int numbers[5]; // declare an int array of size 5 named numbers By default, the values in C++ arrays are undetermined at the time of declaration (meaning that the values can be random). An array can be initialized to specific values by enclosing the values in curly braces. The number of values in {} should not exceed the size of the array. If the number of values inside the braces is less than the size of the array, the rest of the array will be set to the default value for their type, such as 0 for int and false for bool. If an array is initialized to empty curly braces, all elements in the array is set to their default value.\nint a[3] = { 3, 4, 5 }; // int array a = [3, 4, 5] int b[5] = { 1 }; // int array b = [1, 0, 0, 0, 0] int c[2] = { }; // int array c = [0, 0] An element in an array is accessed by its index in O(1) time with square brackets.\nstd::string clothes[5]; // declare an array of size 5 clothes[0] = \u0026#34;shirt\u0026#34;; // initialize first element clothes[1] = \u0026#34;dress\u0026#34;; // initialize second element std::cout \u0026lt;\u0026lt; clothes[0]; // print out \u0026#34;shirt\u0026#34; C++ arrays do not have a length attribute that lets you access its size easily. The common way to get the size of an array is by:\nint arr[5] = { 7, 3, 4, 6, 8 } std::cout \u0026lt;\u0026lt; sizeof(arr)/sizeof(arr[0]) \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; // print out 5 Since C++17, we can also retrieve the size of an array with std::size().\nint arr[] = { 6, 7, 8 } std::cout \u0026lt;\u0026lt; std::size(arr) \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; // print out 3 Vector # When you need to create a dynamically resizable array, C++ provides a useful sequence container vector. Like arrays, it provides efficient constant time element access.\nvector_name[i] gets or sets item stored at index i, O(1) emplace_back(item) adds item to the end of the vector, amortized O(1) size() returns the size of the vector, O(1) The emplace_back() operation, which appends a new element to the end of the vector, runs in amortized constant time. A vector will resize itself when needed. A typical resizing implementation is that when the array is full, the array doubles in size, and the old content is copied over to the newer, larger array. The doubling takes O(n) time, but it happens rarely that the overall insertion still takes O(1) time.\nIn general, to iterate through an array, a for loop is easier to reason than while since there\u0026rsquo;s no condition to manage that could skip elements. In C++, there are two types of for loops: the simple for loop and the for-each loop. Use for-each loop if you don\u0026rsquo;t need to access the index since it avoids the possible error of messsing up the index.\nstd::vector\u0026lt;int\u0026gt; numbers{20, 6, 13, 5}; // simple for loop goes through indices so we fetch elements using indices for (int i = 0; i \u0026lt; numbers.size(); i++) { int number = numbers[i]; std::cout \u0026lt;\u0026lt; number \u0026lt;\u0026lt; std::endl; } // for-each loop directly fetches elements for (int number : numbers) { std::cout \u0026lt;\u0026lt; number \u0026lt;\u0026lt; std::endl; } Linked List # C++ doesn\u0026rsquo;t have a built-in singly linked list. Normally at an interview, you\u0026rsquo;ll be given a definition like this:\nstruct LinkedListNode { int val; LinkedListNode* next; } append to end is O(1) finding an element is O(N) Besides problems that specifically ask for linked lists, you don\u0026rsquo;t normally define and use linked list. If you need a list with O(1) append you can use a vector, and if you want O(1) for both prepend and append you can use deque.\nStack # C++ provides a container adaptor std::stack that works on the basis of LIFO (last-in-first-out).\npush: push(item) adds item to end of stack, O(1) pop: pop() removes item at the end of stack, O(1) size: size(), O(1) top: top() returns but doesn\u0026rsquo;t remove item at the end of stack, O(1) Stack is arguably the most magical data structure in computer science. In fact, with two stacks and a finite-state machine you can build a Turing Machine that can solve any problem a computer can solve.\nRecursion and function calls are implemented with stack behind the scene. We\u0026rsquo;ll discuss this in the recursion review module.\nQueue # We normally use std::deque when we need a queue.\nenqueue: push_back(item) inserts item at the end of the queue, O(1) dequeue: pop_front() removes and return item from the head of the queue, O(1) size: size(), O(1) peek: front() returns but don\u0026rsquo;t remove item at the head of the queue, O(1) In coding interviews, we see queues most often in breadth-first search. We\u0026rsquo;ll also cover monotonic deque where elements are sorted inside the deque that is useful in solving some advanced coding problems.\nHash Table # std::unordered_map in C++ implements hash table.\nat(key) gets item mapped to key if present, O(1) map[key] gets item mapped to key if present (returns 0 if not present), and inserts the key if not, O(N) map[key] = item sets item mapped to key, O(1) count(key) finds out if key is present or not, O(1) erase(key) removes item mapped to key if present, O(1) It\u0026rsquo;s worth mentioning that these are average case time complexity. A hash table\u0026rsquo;s worst time complexity is actually O(N) due to hash collision and other things. For the vast majority of the cases and certainly most coding interviews, the assumption of constant time lookup/insert/delete is valid.\nUse a hash table if you want to create a mapping from A to B. Many starter interview problems can be solved with hash tables.\nHash Set # std::unordered_set in C++ is useful in answering existence queries in constant time.\ncount(item) checks if item is in a set, O(1) insert(item) adds item to a set if it\u0026rsquo;s not already present, O(1) erase(item) removes item from a set if it\u0026rsquo;s present, O(1) size() gets the size of the set, O(1) Hash set is useful when you only need to know existence of a key. Example use cases include DFS and BFS on graphs.\nTree # Normally at an interview, you\u0026rsquo;d be given the following implementation for a binary tree:\nstruct Node { int val; Node* left; Node* right; Node(T val, Node* left = nullptr, Node* right = nullptr) : val{val}, left{left}, right{right} {} }; For n-nary trees:\n#include \u0026lt;vector\u0026gt; struct Node { int val; std::vector\u0026lt;Node*\u0026gt; children; Node(int val, std::vector\u0026lt;Node*\u0026gt; children = {}) : val{val}, children{children} {} }; Infinity # Infinity is useful when you want to initialize a variable that is greater or smaller than any value that your algorithm may want to compare with. std::numeric_limits\u0026lt;T\u0026gt; provides the maximum and minimum values of fundamental arithmatic types in C++.\n#include \u0026lt;limits\u0026gt; int max = std::numeric_limits\u0026lt;int\u0026gt;::max(); // 2147483648 int min = std::numeric_limits\u0026lt;int\u0026gt;::min(); // -2147483648 "},{"id":3,"href":"/tech-book/docs/algorithms/depth-first-search/","title":"Depth First Search","section":"Algorithms","content":" Depth First Search # Intro # The pre-order traversal of a tree is DFS.\nNode\u0026lt;int\u0026gt;* dfs(Node\u0026lt;int\u0026gt;* root, int target) { if (root == nullptr) return nullptr; if (root-\u0026gt;val == target) return root; // return non-null return value from the recursive calls Node\u0026lt;int\u0026gt;* left = dfs(root-\u0026gt;left, target); if (left != nullptr) return left; // at this point, we know left is null, and right could be null or non-null // we return right child\u0026#39;s recursive call result directly because // - if it\u0026#39;s non-null we should return it // - if it\u0026#39;s null, then both left and right are null, we want to return null return dfs(root-\u0026gt;right, target); } Max depth of a binary tree # Max depth of a binary tree is the longest root-to-leaf path. Given a binary tree, find its max depth. Here, we define the length of the path to be the number of edges on that path, not the number of nodes.\nint dfs(Node\u0026lt;int\u0026gt;* root) { // Null node adds no depth if (root == nullptr) return 0; // num nodes in longest path of current subtree = max num nodes of its two subtrees + 1 current node return std::max(dfs(root-\u0026gt;left), dfs(root-\u0026gt;right)) + 1; } int tree_max_depth(Node\u0026lt;int\u0026gt;* root) { return root? dfs(root) - 1 : 0; } Visible Tree Node | Number of Visible Nodes # In a binary tree, a node is labeled as \u0026ldquo;visible\u0026rdquo; if, on the path from the root to that node, there isn\u0026rsquo;t any node with a value higher than this node\u0026rsquo;s value.\nThe root is always \u0026ldquo;visible\u0026rdquo; since there are no other nodes between the root and itself. Given a binary tree, count the number of \u0026ldquo;visible\u0026rdquo; nodes.\nint dfs(Node\u0026lt;int\u0026gt;* root, int max_sofar) { if (!root) return 0; int total = 0; if (root-\u0026gt;val \u0026gt;= max_sofar) total++; total += dfs(root-\u0026gt;left, std::max(max_sofar, root-\u0026gt;val)); total += dfs(root-\u0026gt;right, std::max(max_sofar, root-\u0026gt;val)); return total; } int visible_tree_node(Node\u0026lt;int\u0026gt;* root) { // start max_sofar with smallest number possible so any value root has is greater than it return dfs(root, std::numeric_limits\u0026lt;int\u0026gt;::min()); } "},{"id":4,"href":"/tech-book/docs/algorithms/easy/","title":"Easy Complexity","section":"Algorithms","content":" Easy Complexity # "},{"id":5,"href":"/tech-book/docs/algorithms/priority-queue-and-heap/","title":"Priority Queue and Heap","section":"Algorithms","content":" Priority Queue and Heap # Priority Queue is an Abstract Data Type, and Heap is the concrete data structure we use to implement a priority queue.\nPriority Queue # A priority queue is a data structure that consists of a collection of items and supports the following operations:\ninsert: insert an item with a key. delete_min/delete_max: remove the item with the smallest/largest key and return it. Note that\nwe only allow getting and deleting the element with the min/max key and NOT any arbitrary key. Implement Priority Queue using an array # To do this, we could try using\nan unsorted array, insert would be O(1) as we just have to put it at the end, but finding and deleting min value would be O(N) since we have to loop through the entire array to find it a sorted array, finding min value would be easy O(1), but it would be O(N) to insert since we have to loop through to find the correct position of the value and move elements after the position to make space and insert into the space There must be a better way! \u0026ldquo;Patience you must have, my young padawan.\u0026rdquo; Enter Heap.\nHeap # Heaps are special tree based data structures. Usually when we say heap, we refer to the binary heap that uses the binary tree structure. However, the tree isn\u0026rsquo;t necessarily always binary, in particular, a k-ary heap (A.K.A. k-heap) is a tree where the nodes have k children. As long as the nodes follow the 2 heap properties, it is a valid heap.\nMax Heap and Min Heap # There are two kinds of heaps - Min Heap and Max Heap. A Min Heap is a tree that has two properties:\nalmost complete, i.e. every level is filled except possibly the last(deepest) level. The filled items in the last level are left-justified. for any node, its key (priority) is greater than its parent\u0026rsquo;s key (Min Heap). A Max Heap has the same property #1 and opposite property #2, i.e. for any node, its key is less than its parent\u0026rsquo;s key.\nOperations # insert # To insert a key into a heap,\nplace the new key at the first free leaf if property #2 is violated, perform a bubble-up def bubble_up(node): while node.parent exist and node.parent.key \u0026gt; node.key: swap node and node.parent node = node.parent As the name of the algorithm suggests, it \u0026ldquo;bubbles up\u0026rdquo; the new node by swapping it with its parent until the order is correct\nSince the height of a heap is O(log(N)), the complexity of bubble-up is O(log(N)).\ndelete_min # What this operation does is:\ndelete a node with min key and return it reorganize the heap so the two properties still hold To do that, we:\nremove and return the root since the node with the minimum key is always at the root replace the root with the last node (the rightmost node at the bottom) of the heap if property #2 is violated, perform a bubble-down def bubble_down(node): while node is not a leaf: smallest_child = child of node with smallest key if smallest_child \u0026lt; node: swap node and smallest_child node = smallest_child else: break Implementing Heap # Being a complete tree makes an array a natural choice to implement a heap since it can be stored compactly and no space is wasted. Pointers are not needed. The parent and children of each node can be calculated with index arithmetic\nFor node i, its children are stored at 2i+1 and 2i+2, and its parent is at floor((i-1)/2). So instead of node.left we\u0026rsquo;d do 2*i+1. (Note that if we are implementing a k-ary heap, then the childrens are at ki+1 to ki+k, and its parent is at floor((i-1)/k).)\n"},{"id":6,"href":"/tech-book/docs/algorithms/two-pointers/","title":"Two Pointers \u0026 Sliding Window","section":"Algorithms","content":" Two Pointers # Valid Palindrome # Determine whether a string is a palindrome, ignoring non-alphanumeric characters and case. Examples:\nInput: Do geese see God? Output: True\nInput: Was it a car or a cat I saw? Output: True\nInput: A brown fox jumping over Output: False\n#include \u0026lt;cctype\u0026gt; // isalnum, tolower #include \u0026lt;iostream\u0026gt; // boolalpha, cin, cout #include \u0026lt;string\u0026gt; // getline bool is_palindrome(std::string s) { int l = 0, r = s.size() - 1; while (l \u0026lt; r) { // Note 1, 2 while (l \u0026lt; r \u0026amp;\u0026amp; !std::isalnum(s[l])) { l++; } while (l \u0026lt; r \u0026amp;\u0026amp; !std::isalnum(s[r])) { r--; } // compare characters ignoring case if (std::tolower(s[l]) != std::tolower(s[r])) return false; l++; r--; } return true; } int main() { std::string s; std::getline(std::cin, s); bool res = is_palindrome(s); std::cout \u0026lt;\u0026lt; std::boolalpha \u0026lt;\u0026lt; res \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; } Remove Duplicates # Given a sorted list of numbers, remove duplicates and return the new length. You must do this in-place and without using extra memory.\nInput: [0, 0, 1, 1, 1, 2, 2].\nOutput: 3.\nYour function should modify the list in place so the first 3 elements becomes 0, 1, 2. Return 3 because the new length is 3.\n#include \u0026lt;algorithm\u0026gt; // copy #include \u0026lt;iostream\u0026gt; // boolalpha, cin, cout #include \u0026lt;iterator\u0026gt; // back_inserter, istream_iterator, ostream_iterator, prev #include \u0026lt;sstream\u0026gt; // istringstream #include \u0026lt;string\u0026gt; // getline, string #include \u0026lt;vector\u0026gt; // vector int remove_duplicates(std::vector\u0026lt;int\u0026gt;\u0026amp; arr) { int slow = 0; for (int fast = 0; fast \u0026lt; arr.size(); fast++) { if (arr.at(fast) != arr.at(slow)) { slow++; arr.at(slow) = arr.at(fast); } } return slow + 1; } template\u0026lt;typename T\u0026gt; std::vector\u0026lt;T\u0026gt; get_words() { std::string line; std::getline(std::cin, line); std::istringstream ss{line}; ss \u0026gt;\u0026gt; std::boolalpha; std::vector\u0026lt;T\u0026gt; v; std::copy(std::istream_iterator\u0026lt;T\u0026gt;{ss}, std::istream_iterator\u0026lt;T\u0026gt;{}, std::back_inserter(v)); return v; } template\u0026lt;typename T\u0026gt; void put_words(const std::vector\u0026lt;T\u0026gt;\u0026amp; v) { if (!v.empty()) { std::copy(v.begin(), std::prev(v.end()), std::ostream_iterator\u0026lt;T\u0026gt;{std::cout, \u0026#34; \u0026#34;}); std::cout \u0026lt;\u0026lt; v.back(); } std::cout \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; } int main() { std::vector\u0026lt;int\u0026gt; arr = get_words\u0026lt;int\u0026gt;(); int res = remove_duplicates(arr); arr.resize(res); put_words(arr); } Sliding Window # Sliding window problems is a variant of the same direction two pointers problems. The function performs on the entire interval between the two pointers instead of only at the two positions. Usually, we keep track of the overall result of the window, and when we \u0026ldquo;slide\u0026rdquo; the window (insert/remove an item), we simply manipulate the result to accomodate the changes to the window. Time complexity wise, this is much more efficient as we do not recalculate the overlapping intervals between two windows over and over again. We try to reduce a nested loop into two passes on the input (one pass with each pointer).\nFixed Size Sliding Window # Given an array (list) nums consisted of only non-negative integers, find the largest sum among all subarrays of length k in nums. For example, if the input is nums = [1, 2, 3, 7, 4, 1], k = 3, then the output would be 14 as the largest length 3 subarray sum is given by [3, 7, 4] which sums to 14.\n#include \u0026lt;algorithm\u0026gt; // copy #include \u0026lt;iostream\u0026gt; // boolalpha, cin, cout, streamsize #include \u0026lt;iterator\u0026gt; // back_inserter, istream_iterator #include \u0026lt;limits\u0026gt; // numeric_limits #include \u0026lt;sstream\u0026gt; // istringstream #include \u0026lt;string\u0026gt; // getline, string #include \u0026lt;vector\u0026gt; // vector int subarray_sum_fixed(std::vector\u0026lt;int\u0026gt; nums, int k) { int window_sum = 0; for (int i = 0; i \u0026lt; k; ++i) { window_sum = window_sum + nums[i]; } int largest = window_sum; for (int right = k; right \u0026lt; nums.size(); ++right) { int left = right - k; window_sum = window_sum - nums[left]; window_sum = window_sum + nums[right]; largest = std::max(largest, window_sum); } return largest; } template\u0026lt;typename T\u0026gt; std::vector\u0026lt;T\u0026gt; get_words() { std::string line; std::getline(std::cin, line); std::istringstream ss{line}; ss \u0026gt;\u0026gt; std::boolalpha; std::vector\u0026lt;T\u0026gt; v; std::copy(std::istream_iterator\u0026lt;T\u0026gt;{ss}, std::istream_iterator\u0026lt;T\u0026gt;{}, std::back_inserter(v)); return v; } void ignore_line() { std::cin.ignore(std::numeric_limits\u0026lt;std::streamsize\u0026gt;::max(), \u0026#39;\\n\u0026#39;); } int main() { std::vector\u0026lt;int\u0026gt; nums = get_words\u0026lt;int\u0026gt;(); int k; std::cin \u0026gt;\u0026gt; k; ignore_line(); int res = subarray_sum_fixed(nums, k); std::cout \u0026lt;\u0026lt; res \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; } Flexible Size Sliding Window - Longest # Recall finding the largest size k subarray sum of an integer array in Largest Subarray Sum. What if we dont need the largest sum among all subarrays of fixed size k, but instead, we want to find the length of the longest subarray with sum smaller than or equal to a target?\nGiven input nums = [1, 6, 3, 1, 2, 4, 5] and target = 10, then the longest subarray that does not exceed 10 is [3, 1, 2, 4], so the output is 4 (length of [3, 1, 2, 4]).\n#include \u0026lt;algorithm\u0026gt; // copy #include \u0026lt;iostream\u0026gt; // boolalpha, cin, cout, streamsize #include \u0026lt;iterator\u0026gt; // back_inserter, istream_iterator #include \u0026lt;limits\u0026gt; // numeric_limits #include \u0026lt;sstream\u0026gt; // istringstream #include \u0026lt;string\u0026gt; // getline, string #include \u0026lt;vector\u0026gt; // vector int subarray_sum_longest(std::vector\u0026lt;int\u0026gt; nums, int target) { int windowSum = 0, length = 0; int left = 0; for (int right = 0; right \u0026lt; nums.size(); ++right) { windowSum = windowSum + nums[right]; while (windowSum \u0026gt; target) { windowSum = windowSum - nums[left]; ++left; } length = std::max(length, right-left+1); } return length; } template\u0026lt;typename T\u0026gt; std::vector\u0026lt;T\u0026gt; get_words() { std::string line; std::getline(std::cin, line); std::istringstream ss{line}; ss \u0026gt;\u0026gt; std::boolalpha; std::vector\u0026lt;T\u0026gt; v; std::copy(std::istream_iterator\u0026lt;T\u0026gt;{ss}, std::istream_iterator\u0026lt;T\u0026gt;{}, std::back_inserter(v)); return v; } void ignore_line() { std::cin.ignore(std::numeric_limits\u0026lt;std::streamsize\u0026gt;::max(), \u0026#39;\\n\u0026#39;); } int main() { std::vector\u0026lt;int\u0026gt; nums = get_words\u0026lt;int\u0026gt;(); int target; std::cin \u0026gt;\u0026gt; target; ignore_line(); int res = subarray_sum_longest(nums, target); std::cout \u0026lt;\u0026lt; res \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; } Flexible Size Sliding Window - Shortest # Let\u0026rsquo;s continue on finding the sum of subarrays. This time given a positive integer array nums, we want to find the length of the shortest subarray such that the subarray sum is at least target. Recall the same example with input nums = [1, 4, 1, 7, 3, 0, 2, 5] and target = 10, then the smallest window with the sum \u0026gt;= 10 is [7, 3] with length 2. So the output is 2.\nWe\u0026rsquo;ll assume for this problem that it\u0026rsquo;s guaranteed target will not exceed the sum of all elements in nums.\n#include \u0026lt;algorithm\u0026gt; // copy #include \u0026lt;iostream\u0026gt; // boolalpha, cin, cout, streamsize #include \u0026lt;iterator\u0026gt; // back_inserter, istream_iterator #include \u0026lt;limits\u0026gt; // numeric_limits #include \u0026lt;sstream\u0026gt; // istringstream #include \u0026lt;string\u0026gt; // getline, string #include \u0026lt;vector\u0026gt; // vector int subarray_sum_shortest(std::vector\u0026lt;int\u0026gt; nums, int target) { int windowSum = 0, length = nums.size(); int left = 0; for (int right = 0; right \u0026lt; nums.size(); ++right) { windowSum = windowSum + nums[right]; while (windowSum \u0026gt;= target) { length = std::min(length, right-left+1); windowSum = windowSum - nums[left]; ++left; } } return length; } template\u0026lt;typename T\u0026gt; std::vector\u0026lt;T\u0026gt; get_words() { std::string line; std::getline(std::cin, line); std::istringstream ss{line}; ss \u0026gt;\u0026gt; std::boolalpha; std::vector\u0026lt;T\u0026gt; v; std::copy(std::istream_iterator\u0026lt;T\u0026gt;{ss}, std::istream_iterator\u0026lt;T\u0026gt;{}, std::back_inserter(v)); return v; } void ignore_line() { std::cin.ignore(std::numeric_limits\u0026lt;std::streamsize\u0026gt;::max(), \u0026#39;\\n\u0026#39;); } int main() { std::vector\u0026lt;int\u0026gt; nums = get_words\u0026lt;int\u0026gt;(); int target; std::cin \u0026gt;\u0026gt; target; ignore_line(); int res = subarray_sum_shortest(nums, target); std::cout \u0026lt;\u0026lt; res \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; } Linked List Cycle # Given a linked list with potentially a loop, determine whether the linked list from the first node contains a cycle in it. For bonus points, do this with constant space.\nbool has_cycle(Node\u0026lt;int\u0026gt;* nodes) { Node\u0026lt;int\u0026gt;* tortoise = next_node(nodes); Node\u0026lt;int\u0026gt;* hare = next_node(next_node(nodes)); while (tortoise != hare \u0026amp;\u0026amp; hare-\u0026gt;next != NULL) { tortoise = next_node(tortoise); hare = next_node(next_node(hare)); } return hare-\u0026gt;next != NULL; } "},{"id":7,"href":"/tech-book/docs/data-center/data-center-ethernet/","title":"Data Center Ethernet","section":"Data Center Tips","content":" Intro # Residential vs Data Center Ethernet # Link Aggregation Control Protocol (LACP) # Spanning Tree Protocol # Multiple Spanning Tree Protocol # Multiple Spanning Tree Protocol # ISIS Protocol # # # # # # # # # # # References:\nRaj Jain Data Center Ethernet by Raj Jain Data Center Tutorial "},{"id":8,"href":"/tech-book/docs/data-center/data-center-technologies/","title":"Data Center Technologies","section":"Data Center Tips","content":" Intro # Data Center Network Topologies: 3-Tier # 3-Tier Data Center Networks # 3-Tier Data Center Networks (Cont) # 3-Tier Hierarchical Network Design # Problem with 3-Tier Topology # Clos Networks # Fat-Tree DCN # Fat-Tree Topology (Cont) # Advantages of 2-Tier Architecture # Rack-Scale Architecture # References:\nRaj Jain Data Center Network Topologies by Raj Jain "},{"id":9,"href":"/tech-book/docs/systemdesign-tips/code-deployment-system/","title":"Design A Code-Deployment System","section":"SystemDesign-Tips","content":" 1. Gathering System Requirements # As with any systems design interview question, the first thing that we want to do is to gather system requirements; we need to figure out what system we\u0026rsquo;re building exactly.\nFrom the answers we were given to our clarifying questions (see Prompt Box), we\u0026rsquo;re building a system that involves repeatedly (in the order of thousands of times per day) building and deploying code to hundreds of thousands of machines spread out across 5-10 regions around the world.\nBuilding code will involve grabbing snapshots of source code using commit SHA identifiers; beyond that, we can assume that the actual implementation details of the building action are taken care of. In other words, we don\u0026rsquo;t need to worry about how we would build JavaScript code or C++ code; we just need to design the system that enables the repeated building of code.\nBuilding code will take up to 15 minutes, it\u0026rsquo;ll result in a binary file of up to 10GB, and we want to have the entire deployment process (building and deploying code to our target machines) take at most 30 minutes.\nEach build will need a clear end-state (SUCCESS or FAILURE), and though we care about availability (2 to 3 nines), we don\u0026rsquo;t need to optimize too much on this dimension.\n2. Coming Up With A Plan # It\u0026rsquo;s important to organize ourselves and to lay out a clear plan regarding how we\u0026rsquo;re going to tackle our design. What are the major, distinguishable components of our how system?\nIt seems like this system can actually very simply be divided into two clear subsystems:\nthe Build System that builds code into binaries the Deployment System that deploys binaries to our machines across the world Note that these subsystems will of course have many components themselves, but this is a very straightforward initial way to approach our problem.\n3. Build System \u0026ndash; General Overview # From a high-level perspective, we can call the process of building code into a binary a job, and we can design our build system as a queue of jobs. Jobs get added to the queue, and each job has a commit identifier (the commit SHA) for what version of the code it should build and the name of the artifact that will be created (the name of the resulting binary). Since we\u0026rsquo;re agnostic to the type of the code being built, we can assume that all languages are handled automatically here.\nWe can have a pool of servers (workers) that are going to handle all of these jobs. Each worker will repeatedly take jobs off the queue (in a FIFO manner—no prioritization for now), build the relevant binaries (again, we\u0026rsquo;re assuming that the actual implementation details of building code are given to us), and write the resulting binaries to blob storage (Google Cloud Storage or S3 for instance). Blob storage makes sense here, because binaries are literally blobs of data.\n4. Build System \u0026ndash; Job Queue # A naive design of the job queue would have us implement it in memory (just as we would implement a queue in coding interviews), but this implementation is very problematic; if there\u0026rsquo;s a failure in our servers that hold this queue, we lose the entire state of our jobs: queued jobs and past jobs.\nIt seems like we would be unnecessarily complicating matters by trying to optimize around this in-memory type of storage, so we\u0026rsquo;re likely better off implementing the queue using a SQL database.\n5. Build System \u0026ndash; SQL Job Queue # We can have a jobs table in our SQL database where every record in the database represents a job, and we can use record-creation timestamps as the queue\u0026rsquo;s ordering mechanism.\nOur table will be:\nid: string, the ID of the job, auto-generated created_at: timestamp commit_sha: string name: string, the pointer to the job\u0026rsquo;s eventual binary in blob storage status: string, QUEUED, RUNNING, SUCCEEDED, FAILED We can implement the actual dequeuing mechanism by looking at the oldest creation_timestamp with a QUEUED status. This means that we\u0026rsquo;ll likely want to index our table on both created_at and status.\n6. Build System \u0026ndash; Concurrency # ACID transactions will make it safe for potentially hundreds of workers to grab jobs off the queue without unintentionally running the same job twice (we\u0026rsquo;ll avoid race conditions). Our actual transaction will look like this:\nBEGIN TRANSACTION; SELECT * FROM jobs_table WHERE status = \u0026#39;QUEUED\u0026#39; ORDER BY created_at ASC LIMIT 1; // if there\u0026#39;s none, we ROLLBACK; UPDATE jobs_table SET status = \u0026#39;RUNNING\u0026#39; WHERE id = id from previous query; COMMIT; All of the workers will be running this transaction every so often to dequeue the next job; let\u0026rsquo;s say every 5 seconds. If we arbitrarily assume that we\u0026rsquo;ll have 100 workers sharing the same queue, we\u0026rsquo;ll have 100/5 = 20 reads per second, which is very easy to handle for a SQL database.\n7. Build System \u0026ndash; Lost Jobs # Since we\u0026rsquo;re designing a large-scale system, we have to expect and handle edge cases. Here, what if there\u0026rsquo;s a network partition with our workers or one of our workers dies mid-build? Since builds last around 15 minutes on average, this will very likely happen. In this case, we want to avoid having a \u0026ldquo;lost job\u0026rdquo; that we were never made aware of, and with our current design, the job will remain RUNNING forever. How do we handle this?\nWe could have an extra column on our jobs table called last_heartbeat. This will be updated in a heartbeat fashion by the worker running a particular job, where that worker will update the relevant row in the table every 3-5 minutes to just let us know that it\u0026rsquo;s still running the job.\nWe can then have a completely separate service that polls the table every so often (say, every 5 minutes, depending on how responsive we want this build system to be), checks all of the RUNNING jobs, and if their last_heartbeat was last modified longer than 2 heartbeats ago (we need some margin of error here), then something\u0026rsquo;s likely wrong, and this service can reset the status of the relevant jobs to QUEUED, which would effectively bring them back to the front of the queue.\nThe transaction that this auxiliary service will perform will look something like this:\nUPDATE jobs_table SET status = \u0026#39;QUEUED\u0026#39; WHERE status = \u0026#39;RUNNING\u0026#39; AND last_heartbeat \u0026lt; NOW() - 10 minutes; 8. Build System \u0026ndash; Scale Estimation # We previously arbitrarily assumed that we would have 100 workers, which made our SQL-database queue able to handle the expected load. We should try to estimate if this number of workers is actually realistic.\nWith some back-of-the-envelope math, we can see that, since a build can take up to 15 minutes, a single worker can run 4 jobs per hour, or ~100 (96) jobs per day. Given thousands of builds per day (say, 5000-10000), this means that we would need 50-100 workers (5000 / 100). So our arbitrary figure was accurate.\nEven if the builds aren\u0026rsquo;t uniformly spread out (in other words, they peak during work hours), our system scales horizontally very easily. We can automatically add or remove workers whenever the load warrants it. We can also scale our system vertically by making our workers more powerful, thereby reducing the build time.\n9. Build System \u0026ndash; Storage # We previously mentioned that we would store binaries in blob storage (GCS). Where does this storage fit into our queueing system exactly?\nWhen a worker completes a build, it can store the binary in GCS before updating the relevant row in the jobs table. This will ensure that a binary has been persisted before its relevant job is marked as SUCCEEDED.\nSince we\u0026rsquo;re going to be deploying our binaries to machines spread across the world, it\u0026rsquo;ll likely make sense to have regional storage rather than just a single global blob store.\nWe can design our system based on regional clusters around the world (in our 5-10 global regions). Each region can have a blob store (a regional GCS bucket). Once a worker successfully stores a binary in our main blob store, the worker is released and can run another job, while the main blob store performs some asynchronous replication to store the binary in all of the regional GCS buckets. Given 5-10 regions and 10GB files, this step should take no more than 5-10 minutes, bringing our total build-and-deploy duration so far to roughly 20-25 minutes (15 minutes for a build and 5-10 minutes for global replication of the binary).\n10. Deployment System \u0026ndash; General Overview # From a high-level perspective, our actual deployment system will need to allow for the very fast distribution of 10GB binaries to hundreds of thousands of machines across all of our global regions. We\u0026rsquo;re likely going to want some service that tells us when a binary has been replicated in all regions, another service that can serve as the source of truth for what binary should currently be run on all machines, and finally a peer-to-peer-network design for our actual machines across the world.\n11. Deployment System \u0026ndash; Replication-Status Service # We can have a global service that continuously checks all regional GCS buckets and aggregates the replication status for successful builds (in other words, checks that a given binary in the main blob store has been replicated across all regions). Once a binary has been replicated across all regions, this service updates a separate SQL database with rows containing the name of a binary and a replication_status. Once a binary has a \u0026ldquo;complete\u0026rdquo; replication_status, it\u0026rsquo;s officially deployable.\n12. Deployment System \u0026ndash; Blob Distribution # Since we\u0026rsquo;re going to deploy 10 GBs to hundreds of thousands of machines, even with our regional clusters, having each machine download a 10GB file one after the other from a regional blob store is going to be extremely slow. A peer-to-peer-network approach will be much faster and will allow us to hit our 30-minute time frame for deployments. All of our regional clusters will behave as peer-to-peer networks.\n13. Deployment System \u0026ndash; Trigger # Let\u0026rsquo;s describe what happens when an engineer presses a button on some internal UI that says \u0026ldquo;Deploy build/binary B1 to every machine globally\u0026rdquo;. This is the action that triggers the binary downloads on all the regional peer-to-peer networks.\nTo simplify this process and to support having multiple builds getting deployed concurrently, we can design this in a goal-state oriented manner.\nThe goal-state will be the desired build version at any point in time and will look something like: \u0026ldquo;current_build: B1\u0026rdquo;, and this can be stored in some dynamic configuration service (a key-value store like Etcd or ZooKeeper). We\u0026rsquo;ll have a global goal-state as well as regional goal-states.\nEach regional cluster will have a K-V store that holds configuration for that cluster about what builds should be running on that cluster, and we\u0026rsquo;ll also have a global K-V store.\nWhen an engineer clicks the \u0026ldquo;Deploy build/binary B1\u0026rdquo; button, our global K-V store\u0026rsquo;s build_version will get updated. Regional K-V stores will be continuously polling the global K-V store (say, every 10 seconds) for updates to the build_version and will update themselves accordingly.\nMachines in the clusters/regions will be polling the relevant regional K-V store, and when the build_version changes, they\u0026rsquo;ll try to fetch that build from the P2P network and run the binary.\n14. System Diagram # Final Systems Architecture\n"},{"id":10,"href":"/tech-book/docs/systemdesign-tips/stock-broker/","title":"Design A Stock-Broker System","section":"SystemDesign-Tips","content":" 1. Gathering System Requirements # As with any systems design interview question, the first thing that we want to do is to gather system requirements; we need to figure out what system we\u0026rsquo;re building exactly.\nWe\u0026rsquo;re building a stock-brokerage platform like Robinhood that functions as the intermediary between end-customers and some central stock exchange. The idea is that the central stock exchange is the platform that actually executes stock trades, whereas the stockbroker is just the platform that customers talk to when they want to place a trade\u0026ndash;the stock brokerage is \u0026ldquo;simpler\u0026rdquo; and more \u0026ldquo;human-readable\u0026rdquo;, so to speak.\nWe only care about supporting market trades\u0026ndash;trades that are executed at the current stock price\u0026ndash;and we can assume that our system stores customer balances (i.e., funds that customers may have previously deposited) in a SQL table.\nWe need to design a PlaceTrade API call, and we know that the central exchange\u0026rsquo;s equivalent API method will take in a callback that\u0026rsquo;s guaranteed to be executed upon completion of a call to that API method.\nWe\u0026rsquo;re designing this system to support millions of trades per day coming from millions of customers in a single region (the U.S., for example). We want the system to be highly available.\n2. Coming Up With A Plan # It\u0026rsquo;s important to organize ourselves and to lay out a clear plan regarding how we\u0026rsquo;re going to tackle our design. What are the major, distinguishable components of our how system?\nWe\u0026rsquo;ll approach the design front to back:\nthe PlaceTrade API call that clients will make the API server(s) handling client API calls the system in charge of executing orders for each customer We\u0026rsquo;ll need to make sure that the following hold:\ntrades can never be stuck forever without either succeeding or failing to be executed a single customer\u0026rsquo;s trades have to be executed in the order in which they were placed balances can never go in the negatives 3. API Call # The core API call that we have to implement is PlaceTrade.\nWe\u0026rsquo;ll define its signature as:\nPlaceTrade( customerId: string, stockTicker: string, type: string (BUY/SELL), quantity: integer, ) =\u0026gt; ( tradeId: string, stockTicker: string, type: string (BUY/SELL), quantity: integer, createdAt: timestamp, status: string (PLACED), reason: string, ) The customer ID can be derived from an authentication token that\u0026rsquo;s only known to the user and that\u0026rsquo;s passed into the API call.\nThe status can be one of:\nPLACED IN PROGRESS FILLED REJECTED That being said, PLACED will actually be the defacto status here, because the other statuses will be asynchronously set once the exchange executes our callback. In other words, the trade status will always be PLACED when the PlaceTrade API call returns, but we can imagine that a GetTrade API call could return statuses other than PLACED.\nPotential reasons for a REJECTED trade might be:\ninsufficient funds\nrandom error\npast market hours\n4. API Server(s) # We\u0026rsquo;ll need multiple API servers to handle all of the incoming requests. Since we don\u0026rsquo;t need any caching when making trades, we don\u0026rsquo;t need any server stickiness, and we can just use some round-robin load balancing to distribute incoming requests between our API servers.\nOnce API servers receive a PlaceTrade call, they\u0026rsquo;ll store the trade in a SQL table. This table needs to be in the same SQL database as the one that the balances table is in, because we\u0026rsquo;ll need to use ACID transactions to alter both tables in an atomic way.\nThe SQL table for trades will look like this:\nid: string, a random, auto-generated string customer_id: string, the id of the customer making the trade stockTicker: string, the ticker symbol of the stock being traded type: string, either BUY or SELL quantity: integer (no fractional shares), the number of shares to trade status: string, the status of the trade; starts as PLACED created_at: timestamp, the time when the trade was created reason: string, the human-readable justification of the trade\u0026rsquo;s status The SQL table for balances will look like this:\nid: string, a random, auto-generated string customer_id: string, the id of the customer related to the balance amount: float, the amount of money that the customer has in USD last_modified: timestamp, the time when the balance was last modified 5. Trade-Execution Queue # With hundreds of orders placed every second, the trades table will be pretty massive. We\u0026rsquo;ll need to figure out a robust way to actually execute our trades and to update our table, all the while making sure of a couple of things:\nWe want to make sure that for a single customer, we only process a single BUY trade at any time, because we need to prevent the customer\u0026rsquo;s balance from ever reaching negative values. Given the nature of market orders, we never know the exact dollar value that a trade will get executed at in the exchange until we get a response from the exchange, so we have to speak to the exchange in order to know whether the trade can go through. We can design this part of our system with a Publish/Subscribe pattern. The idea is to use a message queue like Apache Kafka or Google Cloud Pub/Sub and to have a set of topics that customer ids map to. This gives us at-least-once delivery semantics to make sure that we don\u0026rsquo;t miss new trades. When a customer makes a trade, the API server writes a row to the database and also creates a message that gets routed to a topic for that customer (using hashing), notifying the topic\u0026rsquo;s subscriber that there\u0026rsquo;s a new trade.\nThis gives us a guarantee that for a single customer, we only have a single thread trying to execute their trades at any time.\nSubscribers of topics can be rings of 3 workers (clusters of servers, essentially) that use leader election to have 1 master worker do the work for the cluster (this is for our system\u0026rsquo;s high availability)\u0026ndash;the leader grabs messages as they get pushed to the topic and executes the trades for the customers contained in the messages by calling the exchange. As mentioned above, a single customer\u0026rsquo;s trades are only ever handled by the same cluster of workers, which makes our logic and our SQL queries cleaner.\nAs far as how many topics and clusters of workers we\u0026rsquo;ll need, we can do some rough estimation. If we plan to execute millions of trades per day, that comes down to about 10-100 trades per second given open trading hours during a third of a day and non-uniform trading patterns. If we assume that the core execution logic lasts about a second, then we should have roughly 10-100 topics and clusters of workers to process trades in parallel.\n~100,000 seconds per day (3600 * 24) ~1,000,000 trades per day trades bunched in 1/3rd of the day --\u0026gt; (1,000,000 / 100,000) * 3 = ~30 trades per second 6. Trade-Execution Logic # The subscribers (our workers) are streaming / waiting for messages. Imagine the following message were to arrive in the topic queue:\n{\u0026#34;customerId\u0026#34;: \u0026#34;c1\u0026#34;} The following would be pseudo-code for the worker logic:\n// We get the oldest trade that isn\u0026#39;t in a terminal state. trade = SELECT * FROM trades WHERE customer_id = \u0026#39;c1\u0026#39; AND (status = \u0026#39;PLACED\u0026#39; OR status = \u0026#39;IN PROGRESS\u0026#39;) ORDER BY created_at ASC LIMIT 1; // If the trade is PLACED, we know that it\u0026#39;s effectively // ready to be executed. We set it as IN PROGRESS. if trade.status == \u0026#34;PLACED\u0026#34; { UPDATE trades SET status = \u0026#39;IN PROGRESS\u0026#39; WHERE id = trade.id; } // In the event that the trade somehow already exists in the // exchange, the callback will do the work for us. if exchange.TradeExists(trade.id) { return; } // We get the balance for the customer. balance = SELECT amount FROM balances WHERE customer_id = \u0026#39;c1\u0026#39;; // This is the callback that the exchange will execute once // the trade actually completes. We\u0026#39;ll define it further down // in the walkthrough. callback = ... exchange.Execute( trade.stockTicker, trade.type, trade.quantity, max_price = balance, callback, ) 7. Exchange Callback # Below is some pseudo code for the exchange callback:\nfunction exchange_callback(exchange_trade) { if exchange_trade.status == \u0026#39;FILLED\u0026#39; { BEGIN TRANSACTION; trade = SELECT * FROM trades WHERE id = database_trade.id; if trade.status \u0026lt;\u0026gt; \u0026#39;IN PROGRESS\u0026#39; { ROLLBACK; pubsub.send({customer_id: database_trade.customer_id}); return; } UPDATE balances SET amount -= exchange_trade.amount WHERE customer_id = database_trade.customer_id; UPDATE trades SET status = \u0026#39;FILLED\u0026#39; WHERE id = database_trade.id; COMMIT; } else if exchange_trade.status == \u0026#39;REJECTED\u0026#39; { BEGIN TRANSACTION; UPDATE trades SET status = \u0026#39;REJECTED\u0026#39; WHERE id = database_trade.id; UPDATE trades SET reason = exchange_trade.reason WHERE id = database_trade.id; COMMIT; } pubsub.send({customer_id: database_trade.customer_id}); return http.status(200); } 8. System Diagram # Final Systems Architecture\n"},{"id":11,"href":"/tech-book/docs/systemdesign-tips/design-amazon/","title":"Design Amazon","section":"SystemDesign-Tips","content":" 1. Gathering System Requirements # As with any systems design interview question, the first thing that we want to do is to gather system requirements; we need to figure out what system we\u0026rsquo;re building exactly.\nWe\u0026rsquo;re designing the e-commerce side of the Amazon website, and more specifically, the system that supports users searching for items on the Amazon home page, adding items to cart, submitting orders, and those orders being assigned to relevant Amazon warehouses for shipment.\nWe need to handle items going out of stock, and we\u0026rsquo;ve been given some guidelines for a simple \u0026ldquo;stock-reservation\u0026rdquo; system when users begin the checkout process.\nWe have access to two smart services: one that handles user search queries and one that handles warehouse order assignment. It\u0026rsquo;s our job to figure out how these services fit into our larger design.\nWe\u0026rsquo;ll specifically be designing the system that supports amazon.com (i.e., Amazon\u0026rsquo;s U.S. operations), and we\u0026rsquo;ll assume that this system can be replicated for other regional Amazon stores. For the rest of this walkthrough, whenever we refer to \u0026ldquo;Amazon,\u0026rdquo; we\u0026rsquo;ll be referring specifically to Amazon\u0026rsquo;s U.S. store.\nWhile the system should have low latency when searching for items and high availability in general, serving roughly 10 orders per second in the U.S., we\u0026rsquo;ve been told to focus mostly on core functionality.\n2. Coming Up With A Plan # We\u0026rsquo;ll tackle this system by first looking at a high-level overview of how it\u0026rsquo;ll be set up, then diving into its storage components, and finally looking at how the core functionality comes to life. We can divide the core functionality into two main sections:\nThe user side. The warehouse side. We can further divide the user side as follows:\nBrowsing items given a search term. Modifying the cart. Beginning the checkout process. Submitting and canceliing orders. 3. High-Level System Overview # Within a region, user and warehouse requests will get round-robin-load-balanced to respective sets of API servers, and data will be written to and read from a SQL database for that region.\nWe\u0026rsquo;ll go with a SQL database because all of the data that we\u0026rsquo;ll be dealing with (items, carts, orders, etc.) is, by nature, structured and lends itself well to a relational model.\n4. SQL Tables # We\u0026rsquo;ll have six SQL tables to support our entire system\u0026rsquo;s storage needs.\nItems\nThis table will store all of the items on Amazon, with each row representing an item.\nitemId: uuid name: string description: string price: integer currency: enum other\u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; Carts\nThis table will store all of the carts on Amazon, with each row representing a cart. We\u0026rsquo;ve been told that each user can only have a single cart at once.\ncartId: uuid customerId: uuid items: []{itemId, quantity} \u0026hellip; \u0026hellip; \u0026hellip; Orders\nThis table will store all of the orders on Amazon, with each row representing an order.\n| orderId: uuid |\tcustomerId: uuid\t| orderStatus: enum |\titems: []{itemId, quantity} |\tprice: integer |\tpaymentInfo: PaymentInfo\t| shippingAddress: string\t| timestamp: datetime\t| other\u0026hellip; | | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; | | \u0026hellip; |\t\u0026hellip; |\t\u0026hellip; |\t\u0026hellip; |\t\u0026hellip; |\t\u0026hellip; |\t\u0026hellip; |\t\u0026hellip; |\t\u0026hellip;|\nAggregated Stock\nThis table will store all of the item stocks on Amazon that are relevant to users, with each row representing an item. See the Core User Functionality section for more details.\nitemId: uuid stock: integer \u0026hellip; \u0026hellip; Warehouse Orders\nThis table will store all of the orders that Amazon warehouses get, with each row representing a warehouse order. Warehouse orders are either entire normal Amazon orders or subsets of normal Amazon orders.\nwarehouseOrderId: uuid parentOrderId: uuid warehouseId: uuid orderStatus: enum items: []{itemId, quantity} shippingAddress: string \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; Warehouse Stock\nThis table will store all of the item stocks in Amazon warehouses, with each row representing an {item, warehouse} pairing. The physicalStock field represents an item\u0026rsquo;s actual physical stock in the warehouse in question, serving as a source of truth, while the availableStock field represents an item\u0026rsquo;s effective available stock in the relevant warehouse; this stock gets decreased when orders are assigned to warehouses. See the Core Warehouse Functionality section for more details.\nitemId: uuid warehouseId: uuid physicalStock: integer availableStock: integer \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; 5. Core User Functionality # GetItemCatalog(search)\nThis is the endpoint that users call when they\u0026rsquo;re searching for items. The request is routed by API servers to the smart search-results service, which interacts directly with the items table, caches popular item searches, and returns the results.\nThe API servers also fetch the relevant item stocks from the aggregated_stock table.\nUpdateCartItemQuantity(itemId, quantity)\nThis is the endpoint that users call when they\u0026rsquo;re adding or removing items from their cart. The request writes directly to the carts table, and users can only call this endpoint when an item has enough stock in the aggregated_stock table.\nBeginCheckout() \u0026amp; CancelCheckout()\nThese are the endpoints that users call when they\u0026rsquo;re beginning the checkout process and cancelling it. The BeginCheckout request triggers another read of the aggregated_stock table for the relevant items. If some of the items in the cart don\u0026rsquo;t have enough stock anymore, the UI alerts the users accordingly. For items that do have enough stock, the API servers write to the aggregated_stock table and decrease the relevant stocks accordingly, effectively \u0026ldquo;reserving\u0026rdquo; the items during the duration of the checkout. The CancelCheckout request, which also gets automatically called after 10 minutes of being in the checkout process, writes to the aggregated_stock table and increases the relevant stocks accordingly, thereby \u0026ldquo;unreserving\u0026rdquo; the items. Note that all of the writes to the aggregated_stock are ACID transactions, which allows us to comfortably rely on this SQL table as far as stock correctness is concerned.\nSubmitOrder(), CancelOrder(), \u0026amp; GetMyOrders()\nThese are the endpoints that users call when they\u0026rsquo;re submitting and cancelling orders. Both the SubmitOrder and CancelOrder requests write to the orders table, and CancelOrder also writes to the aggregated_stock table, increasing the relevant stocks accordingly (SubmitOrder doesn\u0026rsquo;t need to because the checkout process already has). GetMyOrders simply reads from the orders table. Note that an order can only be cancelled if it hasn\u0026rsquo;t yet been shipped, which is knowable from the orderStatus field.\n6. Core Warehouse Functionality # On the warehouse side of things, we\u0026rsquo;ll have the smart order-assignment service read from the orders table, figure out the best way to split orders up and assign them to warehouses based on shipping addresses, item stocks, and other data points, and write the final warehouse orders to the warehouse_orders table.\nIn order to know which warehouses have what items and how many, the order-assignment service will rely on the availableStock of relevant items in the warehouse_stock table. When the service assigns an order to a warehouse, it decreases the availableStock of the relevant items for the warehouse in question in the warehouse_stock table. These availableStock values are re-increased by the relevant warehouse if its order ends up being cancelled.\nWhen warehouses get new item stock, lose item stock for whatever reason, or physically ship their assigned orders, they\u0026rsquo;ll update the relevant physicalStock values in the warehouse_stock table. If they get new item stock or lose item stock, they\u0026rsquo;ll also write to the aggregated_stock table (they don\u0026rsquo;t need to do this when shipping assigned orders, since the aggregated_stock table already gets updated by the checkout process on the user side of things).\n7. System Diagram # Final Systems Architecture\n"},{"id":12,"href":"/tech-book/docs/systemdesign-tips/design-slack/","title":"Design Slack","section":"SystemDesign-Tips","content":" 1. Gathering System Requirements # As with any systems design interview question, the first thing that we want to do is to gather system requirements; we need to figure out what system we\u0026rsquo;re building exactly.\nWe\u0026rsquo;re designing the core communication system behind Slack, which allows users to send instant messages in Slack channels.\nSpecifically, we\u0026rsquo;ll want to support:\nLoading the most recent messages in a Slack channel when a user clicks on the channel. Immediately seeing which channels have unread messages for a particular user when that user loads Slack. Immediately seeing which channels have unread mentions of a particular user, for that particular user, when that user loads Slack, and more specifically, the number of these unread mentions in each relevant channel. Sending and receiving Slack messages instantly, in real time. Cross-device synchronization: if a user has both the Slack desktop app and the Slack mobile app open, with an unread channel in both, and if they read this channel on one device, the second device should immediately be updated and no longer display the channel as unread. The system should have low latencies and high availability, catering to a single region of roughly 20 million users. The largest Slack organizations will have as many as 50,000 users, with channels of the same size within them.\nThat being said, for the purpose of this design, we should primarily focus on latency and core functionality; availability and regionality can be disregarded, within reason.\n2. Coming Up With A Plan # We\u0026rsquo;ll tackle this system by dividing it into two main sections:\nHandling what happens when a Slack app loads. Handling real-time messaging as well as cross-device synchronization. We can further divide the first section as follows:\nSeeing all of the channels that a user is a part of. Seeing messages in a particular channel. Seeing which channels have unread messages. Seeing which channels have unread mentions and how many they have. 3. Persistent Storage Solution \u0026amp; App Load # While a large component of our design involves real-time communication, another large part of it involves retrieving data (channels, messages, etc.) at any given time when the Slack app loads. To support this, we\u0026rsquo;ll need a persistent storage solution.\nSpecifically, we\u0026rsquo;ll opt for a SQL database since we can expect this data to be structured and to be queried frequently.\nWe can start with a simple table that\u0026rsquo;ll store every Slack channel.\nChannels\nid (channelId): uuid orgId: uuid name: string description: string \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; Then, we can have another simple table representing channel-member pairs: each row in this table will correspond to a particular user who is in a particular channel. We\u0026rsquo;ll use this table, along with the one above, to fetch a user\u0026rsquo;s relevant when the app loads.\nChannel Members\nid: uuid orgId: uuid channelId: uuid userId: uuid \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; We\u0026rsquo;ll naturally need a table to store all historical messages sent on Slack. This will be our largest table, and it\u0026rsquo;ll be queried every time a user fetches messages in a particular channel. The API endpoint that\u0026rsquo;ll interact with this table will return a paginated response, since we\u0026rsquo;ll typically only want the 50 or 100 most recent messages per channel.\nAlso, this table will only be queried when a user clicks on a channel; we don\u0026rsquo;t want to fetch messages for all of a user\u0026rsquo;s channels on app load, since users will likely never look at most of their channels.\nHistorical Messages\nid: uuid orgId: uuid channelId: uuid senderId: uuid sentAt: timestamp body: string mentions: List \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; In order not to fetch recent messages for every channel on app load, all the while supporting the feature of showing which channels have unread messages, we\u0026rsquo;ll need to store two extra tables: one for the latest activity in each channel (this table will be updated whenever a user sends a message in a channel), and one for the last time a particular user has read a channel (this table will be updated whenever a user opens a channel).\nLatest Channel Timestamps\nid: uuid orgId: uuid channelId: uuid lastActive: timestamp \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; Channel Read Receipts\nid: uuid orgId: uuid channelId: uuid userId: uuid lastSeen: timestamp \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; For the number of unread user mentions that we want to display next to channel names, we\u0026rsquo;ll have another table similar to the read-receipts one, except this one will have a count of unread user mentions instead of a timestamp. This count will be updated (incremented) whenever a user tags another user in a channel message, and it\u0026rsquo;ll also be updated (reset to 0) whenever a user opens a channel with unread mentions of themself.\nUnread Channel-User-Mention Counts\nid: uuid orgId: uuid channelId: uuid userId: uuid count: int \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; 4. Load Balancing # For all of the API calls that clients will issue on app load, including writes to our database (when sending a message or marking a channel as read), we\u0026rsquo;re going to want to load balance.\nWe can have a simple round-robin load balancer, forwarding requests to a set of server clusters that will then handle passing requests to our database.\n5. \u0026ldquo;Smart\u0026rdquo; Sharding # Since our tables will be very large, especially the messages table, we\u0026rsquo;ll need to have some sharding in place.\nThe natural approach is to shard based on organization size: we can have the biggest organizations (with the biggest channels) in their individual shards, and we can have smaller organizations grouped together in other shards.\nAn important point to note here is that, over time, organization sizes and Slack activity within organizations will change. Some organizations might double in size overnight, others might experience seemingly random surges of activity, etc.. This means that, despite our relatively sound sharding strategy, we might still run into hot spots, which is very bad considering the fact that we care about latency so much.\nTo handle this, we can add a \u0026ldquo;smart\u0026rdquo; sharding solution: a subsystem of our system that\u0026rsquo;ll asynchronously measure organization activity and \u0026ldquo;rebalance\u0026rdquo; shards accordingly. This service can be a strongly consistent key-value store like Etcd or ZooKeeper, mapping orgIds to shards. Our API servers will communicate with this service to know which shard to route requests to.\n6. Pub/Sub System for Real-Time Behavior # There are two types of real-time behavior that we want to support:\nSending and receiving messages in real time. Cross-device synchronization (instantly marking a channel as read if you have Slack open on two devices and read the channel on one of them). For both of these functionalities, we can rely on a Pub/Sub messaging system, which itself will rely on our previously described \u0026ldquo;smart\u0026rdquo; sharding strategy.\nEvery Slack organization or group of organizations will be assigned to a Kafka topic, and whenever a user sends a message in a channel or marks a channel as read, our previously mentioned API servers, which handle speaking to our database, will also send a Pub/Sub message to the appropriate Kafka topic.\nThe Pub/Sub messages will look like:\n{ \u0026#34;type\u0026#34;: \u0026#34;chat\u0026#34;, \u0026#34;orgId\u0026#34;: \u0026#34;AAA\u0026#34;, \u0026#34;channelId\u0026#34;: \u0026#34;BBB\u0026#34;, \u0026#34;userId\u0026#34;: \u0026#34;CCC\u0026#34;, \u0026#34;messageId\u0026#34;: \u0026#34;DDD\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2020-08-31T01:17:02\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;this is a message\u0026#34;, \u0026#34;mentions\u0026#34;: [\u0026#34;CCC\u0026#34;, \u0026#34;EEE\u0026#34;] }, { \u0026#34;type\u0026#34;: \u0026#34;read-receipt\u0026#34;, \u0026#34;orgId\u0026#34;: \u0026#34;AAA\u0026#34;, \u0026#34;channelId\u0026#34;: \u0026#34;BBB\u0026#34;, \u0026#34;userId\u0026#34;: \u0026#34;CCC\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2020-08-31T01:17:02\u0026#34; } We\u0026rsquo;ll then have a different set of API servers who subscribe to the various Kakfa topics (probably one API server cluster per topic), and our clients (Slack users) will establish long-lived TCP connections with these API server clusters to receive Pub/Sub messages in real time.\nWe\u0026rsquo;ll want a load balancer in between the clients and these API servers, which will also use the \u0026ldquo;smart\u0026rdquo; sharding strategy to match clients with the appropriate API servers, which will be listening to the appropriate Kafka topics.\nWhen clients receive Pub/Sub messages, they\u0026rsquo;ll handle them accordingly (mark a channel as unread, for example), and if the clients refresh their browser or their mobile app, they\u0026rsquo;ll go through the entire \u0026ldquo;on app load\u0026rdquo; system that we described earlier.\nSince each Pub/Sub message comes with a timestamp, and since reading a channel and sending Slack messages involve writing to our persistent storage, the Pub/Sub messages will effectively be idempotent operations.\n7. System Diagram # Final Systems Architecture\n"},{"id":13,"href":"/tech-book/docs/networking-tips/ecmp/","title":"ECMP Load Balancing","section":"Networking Tips","content":" Intro # ECMP Hashing # ECMP is classified into per-flow load balancing and per-packet load balancing.\nPer-flow load balancing can ensure the packet sequence and ensure that the same data flow is forwarded according to the routing entry with the same next hop and different data flows are forwarded according to routing entries with different next hops.\nWhen multiple routes are installed in the routing table, a hash is used to determine which path a packet follows.\nHashes on the following fields:\nIP protocol Ingress interface Source IPv4 or IPv6 address Destination IPv4 or IPv6 address Further, hashes on these additional fields:\nSource MAC address Destination MAC address Ethertype VLAN ID For TCP/UDP frames, also hashes on:\nSource port Destination port Per-packet load balancing improves ECMP bandwidth utilization and evenly load balances traffic among equal-cost routes, but it cannot prevent out-of-order packet delivery. To resolve this issue, the device or terminal that receives traffic must support reassembly of out-of-order packets.\nThe following per-packet load balancing modes are supported: Random mode: A route is randomly selected among multiple equal-cost routes to forward packets. When the IP address and MAC address of known unicast packets remain unchanged, configure random per-packet load balancing. Round-robin mode: Each equal-cost route is used to forward packets in turn. When known unicast packets have a similar length, configure round-robin per-packet load balancing.\nEth-Trunk/Bundle Load Balancing # Ethernet link aggregation, also known as Eth-Trunk, bundles multiple physical links into a logical link to increase link bandwidth. The bundled links back up each other, increasing reliability.\nEth-Trunk load balancing is classified into per-packet load balancing and per-flow load balancing.\nPer-packet load balancing improves Eth-Trunk bandwidth utilization, but it cannot prevent out-of-order packet delivery. To resolve this issue, the device or terminal that receives traffic must support reassembly of out-of-order packets. The following per-packet load balancing modes are supported: Random mode: The outbound interface of packets is selected randomly based on the time when packets reach the Eth-Trunk. When the IP address and MAC address of known unicast packets remain unchanged, configure random per-packet load balancing. Round-robin mode: Each member interface of an Eth-Trunk forwards traffic in turn. When known unicast packets have a similar length, configure round-robin per-packet load balancing. Per-flow load balancing ensures the packet sequence and ensures that the same data flow is forwarded through the same physical link and different data flows are forwarded through different physical links. Table 1-2 describes the per-flow load balancing modes for different types of packets. How Do I Solve the Hash Polarization Problem? # Hash polarization, also known as hash imbalance, indicates that traffic is unevenly load balanced after being hashed twice or more. This situation is common when hash operations are performed across devices multiple times. For example, a device performs ECMP hashing and forwards traffic to two or more connected devices, which then perform ECMP or Eth-Trunk hashing again. Hash polarization may also occur if the outbound interfaces of ECMP routes are multiple Eth-Trunk interfaces on the same device. The implementation of the hash function on switches heavily depends on chips. Therefore, hash polarization may occur if switches using the same type of chips are located at adjacent network layers. If both Eth-Trunk hashing and ECMP hashing exist on the same device, hash polarization may also occur. Therefore, if ECMP or Eth-Trunk hashing is deployed on a multi-layer network, consider the risk of hash polarization.\nSuggestions # If load imbalance or hash polarization occurs during traffic forwarding, you can adjust the hash algorithm on the device to resolve the problem.\nHash algorithm: is configured by specifying the hash-mode hash-mode-id parameter. Seed value: is configured by specifying the seed seed-data parameter. If devices from multiple vendors exist on the network, you are advised to configure the same seed value for these devices. Offset: is configured by specifying the universal-id universal-id parameter. Typically, one hash algorithm corresponds to one offset. When devices from multiple vendors exist on the network, you are advised to configure the same offset for these devices. Resilient Hashing # When a next hop fails or is removed from an ECMP pool, the hashing or hash bucket assignment can change. For deployments where there is a need for flows to always use the same next hop, like TCP anycast deployments, this can create session failures.\nResilient hashing is an alternate mechanism for managing ECMP groups. The ECMP hash performed with resilient hashing is exactly the same as the default hashing mode. Only the method in which next hops are assigned to hash buckets differs — they’re assigned to buckets by hashing their header fields and using the resulting hash to index into the table of 2^n hash buckets. Since all packets in a given flow have the same header hash value, they all use the same flow bucket.\nReferences:\nhttps://docs.nvidia.com/networking-ethernet-software/cumulus-linux-43/Layer-3/Routing/Equal-Cost-Multipath-Load-Sharing-Hardware-ECMP/ https://support.huawei.com/enterprise/en/doc/EDOC1100086965 "},{"id":14,"href":"/tech-book/docs/systemdesign-tips/google-drive/","title":"Google Drive - Design","section":"SystemDesign-Tips","content":" 1. Gathering System Requirements # As with any systems design interview question, the first thing that we want to do is to gather system requirements; we need to figure out what system we\u0026rsquo;re building exactly.\nWe\u0026rsquo;re designing the core user flow of the Google Drive web application. This consists of storing two main entities: folders and files. More specifically, the system should allow users to create folders, upload and download files, and rename and move entities once they\u0026rsquo;re stored. We don\u0026rsquo;t have to worry about ACLs, sharing entities, or any other auxiliary Google Drive features.\nWe\u0026rsquo;re going to be building this system at a very large scale, assuming 1 billion users, each with 15GB of data stored in Google Drive on average. This adds up to approximately 15,000 PB of data in total, without counting any metadata that we might store for each entity, like its name or its type.\nWe need this service to be Highly Available and also very redundant. No data that\u0026rsquo;s successfully stored in Google Drive can ever be lost, even through catastrophic failures in an entire region of the world.\n2. Coming Up With A Plan # It\u0026rsquo;s important to organize ourselves and to lay out a clear plan regarding how we\u0026rsquo;re going to tackle our design. What are the major, distinguishable components of our how system?\nFirst of all, we\u0026rsquo;ll need to support the following operations:\nFor Files\nUploadFile DownloadFile DeleteFile RenameFile MoveFile For Folders\nCreateFolder GetFolder DeleteFolder RenameFolder MoveFolder Secondly, we\u0026rsquo;ll have to come up with a proper storage solution for two types of data:\nFile Contents: The contents of the files uploaded to Google Drive. These are opaque bytes with no particular structure or format. Entity Info: The metadata for each entity. This might include fields like entityID, ownerID, lastModified, entityName, entityType. This list is non-exhaustive, and we\u0026rsquo;ll most likely add to it later on. Let\u0026rsquo;s start by going over the storage solutions that we want to use, and then we\u0026rsquo;ll go through what happens when each of the operations outlined above is performed.\n3. Storing Entity Info # To store entity information, we can use key-value stores. Since we need high availability and data replication, we need to use something like Etcd, Zookeeper, or Google Cloud Spanner (as a K-V store) that gives us both of those guarantees as well as consistency (as opposed to DynamoDB, for instance, which would give us only eventual consistency).\nSince we\u0026rsquo;re going to be dealing with many gigabytes of entity information (given that we\u0026rsquo;re serving a billion users), we\u0026rsquo;ll need to shard this data across multiple clusters of these K-V stores. Sharding on entityID means that we\u0026rsquo;ll lose the ability to perform batch operations, which these key-value stores give us out of the box and which we\u0026rsquo;ll need when we move entities around (for instance, moving a file from one folder to another would involve editing the metadata of 3 entities; if they were located in 3 different shards that wouldn\u0026rsquo;t be great). Instead, we can shard based on the ownerID of the entity, which means that we can edit the metadata of multiple entities atomically with a transaction, so long as the entities belong to the same user.\nGiven the traffic that this website needs to serve, we can have a layer of proxies for entity information, load balanced on a hash of the ownerID. The proxies could have some caching, as well as perform ACL checks when we eventually decide to support them. The proxies would live at the regional level, whereas the source-of-truth key-value stores would be accessed globally.\n4. Storing File Data # When dealing with potentially very large uploads and data storage, it\u0026rsquo;s often advantageous to split up data into blobs that can be pieced back together to form the original data. When uploading a file, the request will be load balanced across multiple servers that we\u0026rsquo;ll call \u0026ldquo;blob splitters\u0026rdquo;, and these blob splitters will have the job of splitting files into blobs and storing these blobs in some global blob-storage solution like GCS or S3 (since we\u0026rsquo;re designing Google Drive, it might not be a great idea to pick S3 over GCS :P).\nOne thing to keep in mind is that we need a lot of redundancy for the data that we\u0026rsquo;re uploading in order to prevent data loss. So we\u0026rsquo;ll probably want to adopt a strategy like: try pushing to 3 different GCS buckets and consider a write successful only if it went through in at least 2 buckets. This way we always have redundancy without necessarily sacrificing availability. In the background, we can have an extra service in charge of further replicating the data to other buckets in an async manner. For our main 3 buckets, we\u0026rsquo;ll want to pick buckets in 3 different availability zones to avoid having all of our redundant storage get wiped out by potential catastrophic failures in the event of a natural disaster or huge power outage.\nIn order to avoid having multiple identical blobs stored in our blob stores, we\u0026rsquo;ll name the blobs after a hash of their content. This technique is called Content-Addressable Storage, and by using it, we essentially make all blobs immutable in storage. When a file changes, we simply upload the entire new resulting blobs under their new names computed by hashing their new contents.\nThis immutability is very powerful, in part because it means that we can very easily introduce a caching layer between the blob splitters and the buckets, without worrying about keeping caches in sync with the main source of truth when edits are made\u0026ndash;an edit just means that we\u0026rsquo;re dealing with a completely different blob.\n5. Entity Info Structure # Since folders and files will both have common bits of metadata, we can have them share the same structure. The difference will be that folders will have an is_folder flag set to true and a list of children_ids, which will point to the entity information for the folders and files within the folder in question. Files will have an is_folder flag set to false and a blobs field, which will have the IDs of all of the blobs that make up the data within the relevant file. Both entities can also have a parent_id field, which will point to the entity information of the entity\u0026rsquo;s parent folder. This will help us quickly find parents when moving files and folders.\n`File Info` `{` `blobs: [\u0026#39;blob_content_hash_0\u0026#39;, \u0026#39;blob_content_hash_1\u0026#39;],` `id: \u0026#39;some_unique_entity_id\u0026#39;` `is_folder: false,` `name: \u0026#39;some_file_name\u0026#39;,` `owner_id: \u0026#39;id_of_owner\u0026#39;,` `parent_id: \u0026#39;id_of_parent\u0026#39;,` `}` `Folder Info` `{` `children_ids: [\u0026#39;id_of_child_0\u0026#39;, \u0026#39;id_of_child_1\u0026#39;],` `id: \u0026#39;some_unique_entity_id\u0026#39;` `is_folder: true,` `name: \u0026#39;some_folder_name\u0026#39;,` `owner_id: \u0026#39;id_of_owner\u0026#39;,` `parent_id: \u0026#39;id_of_parent\u0026#39;,` `} `\n6. Garbage Collection # Any change to an existing file will create a whole new blob and de-reference the old one. Furthermore, any deleted file will also de-reference the file\u0026rsquo;s blobs. This means that we\u0026rsquo;ll eventually end up with a lot of orphaned blobs that are basically unused and taking up storage for no reason. We\u0026rsquo;ll need a way to get rid of these blobs to free some space.\nWe can have a Garbage Collection service that watches the entity-info K-V stores and keeps counts of the number of times every blob is referenced by files; these counts can be stored in a SQL table.\nReference counts will get updated whenever files are uploaded and deleted. When the reference count for a particular blob reaches 0, the Garbage Collector can mark the blob in question as orphaned in the relevant blob stores, and the blob will be safely deleted after some time if it hasn\u0026rsquo;t been accessed.\n7. End To End API Flow # Now that we\u0026rsquo;ve designed the entire system, we can walk through what happens when a user performs any of the operations we listed above.\nCreateFolder is simple; since folders don\u0026rsquo;t have a blob-storage component, creating a folder just involves storing some metadata in our key-value stores.\nUploadFile works in two steps. The first is to store the blobs that make up the file in the blob storage. Once the blobs are persisted, we can create the file-info object, store the blob-content hashes inside its blobs field, and write this metadata to our key-value stores.\nDownloadFile fetches the file\u0026rsquo;s metadata from our key-value stores given the file\u0026rsquo;s ID. The metadata contains the hashes of all of the blobs that make up the content of the file, which we can use to fetch all of the blobs from blob storage. We can then assemble them into the file and save it onto local disk.\nAll of the Get, Rename, Move, and Delete operations atomically change the metadata of one or several entities within our key-value stores using the transaction guarantees that they give us.\n8. System Diagram # Final Systems Architecture\n"},{"id":15,"href":"/tech-book/docs/manageability/why-grpc-on-http2/","title":"gRPC on HTTP/2","section":"Manageability","content":" Intro # tbd\nReferences:\nHTTP/2: Smarter at scale gRPC on HTTP/2: Engineering a robust, high performance protocol "},{"id":16,"href":"/tech-book/docs/networking-tips/ip-tos-dscp/","title":"IP Precedence And TOS | DSCP","section":"Networking Tips","content":" Intro # 8 Bits of Type of Service in IP Header.\nIP Precedence # RFC791/RFC1349 Interpretation\n** Bits ** 7-5 IP Precedence\n111\tNetwork Control\n110\tInternetwork Control\n101\tCritic/ECP\n100\tFlash Override\n011\tFlash\n010\tImmediate\n001\tPriority\n000\tRoutine\nBits\n4 (1 = Low Delay; 0 = Normal Delay)\n3 (1 = High Throughput; 0 = Normal Throughput)\n2 (1 = High Reliability; 0 = Normal Reliability)\n1 (1 = Minimise monetary cost (RFC 1349))\n0 (Must be 0)\nTOS | DSCP (Differentiated Services Code Point) # RFC 2474 (Differentiated Services) Interpretation\nBits\n7-2\tDSCP\n1-0\tECN (Explicit Congestion Notification)\nDefault Forwarding (DF) PHB # Typically best-effort traffic The recommended DSCP for DF is 0\nExpedited Forwarding (EF) PHB # Dedicated to low-loss, low-latency traffic The recommended DSCP for EF is 101110B (46 or 2E(hex))\nAssured Forwarding (AF) PHB # Gives assurance of delivery under prescribed conditions\nDrop probability Class 1 Class 2 Class 3 Class 4 Low AF11 (DSCP 10) 001010 AF21 (DSCP 18) 010010 AF31 (DSCP 26) 011010 AF41 (DSCP 34) 100010 Medium AF12 (DSCP 12) 001100 AF22 (DSCP 20) 010100 AF32 (DSCP 28) 011100 AF42 (DSCP 36) 100100 High AF13 (DSCP 14) 001110 AF23 (DSCP 22) 010110 AF33 (DSCP 30) 011110 AF43 (DSCP 38) 100110 Class Selector PHBs # This maintains backward compatibility with the IP precedence field.\nService class DSCP Name DSCP Value IP precedence Examples of application Standard CS0 (DF) 0 0 (000) Low-priority data CS1 8 1 (001) File transfer (FTP, SMB) Network operations, administration and management (OAM) CS2 16 2 (010) SNMP, SSH, Ping, Telnet, syslog Broadcast video CS3 24 3 (011) RTSP broadcast TV, treaming of live audio and video events, video surveillance,video-on-demand Real-time interactive CS4 32 4 (100) Gaming, low priority video conferencing Signaling CS5 40 5 (101) Peer-to-peer (SIP, H.323, H.248), NTP Network control CS6 48 6 (110) Routing protocols (OSPF, BGP, ISIS, RIP) Reserved for future use CS7 56 7 (111) DF= Default Forwarding\nPHB == Per-Hop-Behavior\nCS: Class Selector (RFC 2474) AFxy: Assured Forwarding (x=class, y=drop precedence) (RFC2597) EF: Expedited Forwarding (RFC 3246) References:\nhttps://en.wikipedia.org/wiki/Differentiated_services https://bogpeople.com/networking/dscp.shtml "},{"id":17,"href":"/tech-book/docs/algorithms/medium/","title":"Medium Complexity","section":"Algorithms","content":" Medium Complexity # Number Swapper: Write a function to swap a number in place (that is, without temporary variables). Hints - with just addition/substruction arithmatic, XOR.\nTic Tac Win: Design an algorithm to figure out if someone has won a game of tic-tac-toe.\nHashing # Two Sum: Find a pair in array whose sum equals to the target input: [10,4,1,3,2] Target: 7 Output: [4,3]\nHint - insert in hash and then start searching from first element, find the difference with the target and find the hash.\nLogest consecutive sequence: find length of longest consecutive sequence from given array input: [10, 4, 20, 1,3,2] Output: [1,2,3,4]\nSliding Window # Two Sum: find a pair whose sum is equal to target\ninput: [10,4,1,3,2] Target: 7 Output: [4,3]\nHint - sort, start from bengining and end at the same time\nTraping Water: Given an arrary of height of bars (width = 1) calculate the amount of water trapped.\ninput: 5, 2, 3, 4, 1, 3 Output = 5\nHint - use sliding window.\nRecursion # Combination Sum: Return the list of numbers from given array whose sum = target\nGenerate all pairs of valid parenthesis\nInput: 2\nOutput: {\n​\t()(),\n​\t(())\n​\t}\nBinary Tree # Height of the binary tree\nHints - use recursion\nDiameter of the binary tree\nThe diameter of the binary tree is the length of the longest path between any two nodes in the tree.\nHints - use recursion\nConvert to the Sum Tree: Convert it such that every node\u0026rsquo;s value is equal to sum of its left and right sub tree.\nHints - use recursion\nMaximum path sum in binary tree\nLowest common ancestor in a binary tree\n"},{"id":18,"href":"/tech-book/docs/networking-tips/mlag/","title":"Multi Chassis Link Aggregation Basics","section":"Networking Tips","content":" Intro # Link Aggregation Basics # Link aggregation is an ancient technology that allows you to bond multiple parallel links into a single virtual link (link aggregation group – LAG). With parallel links being replaced by a single virtual link, STP detects no loops and all the physical links can be fully utilized.\nI was also told that link aggregation makes your bridged network more stable. Every link state change triggers a Topology Change Notification in spanning tree, potentially sending a large part of your network through the STP listening-learning-forwarding state diagram. A link failure of a LAG member does not change the state of the aggregation group (unless it was the last working link in the group), and since STP rides on top of aggregated interfaces, it does not react to the failure.\nMulti-Chassis Link Aggregation(MLAG) # Imagine you could pretend two physical boxes use a single control plane and coordinated switching fabrics. The links terminated on two physical boxes would seem to terminate within the same control plane and you could aggregate them using LACP. Welcome to the wonderful world of Multi-Chassis Link Aggregation (MLAG).\nMLAG nicely solves two problems:\nWhen used in the network core, multiple links appear as a single link. No bandwidth is wasted due to STP loop prevention while you still retain almost full redundancy – just make sure you always read the smallprint to understand what happens when one of the two switches in the MLAG pair fails. When used between a server and a pair of top-of-rack switches, it allows the server to use the full aggregate bandwidth while still retaining resiliency against a link or switch failure. Link Aggregation Control Protocol (LACP) # LACP (Link Aggregation Control Protocol): a subcomponent of IEEE 802.3ad standard, provides a method to control the bundling of several physical ports together to form a single logical channel. LACP allows a network device to negotiate an automatic bundling of links by sending LACP packets to the peer.\nReferences:\nhttps://blog.ipspace.net/2010/10/multi-chassis-link-aggregation-basics.html https://community.fs.com/article/mlag-vs-stacking-vs-lacp.html "},{"id":19,"href":"/tech-book/docs/networking-tips/qos/","title":"QoS","section":"Networking Tips","content":" Intro # Traffic Shaping # The traffic shapers that we will look at here are commonly called token bucket shapers and are based on a token bucket algorithm which we will see can serve multiple purposes in achieving network QoS. We should note that frequently in networking literature such algorithms were also referred to as leaky bucket algorithms. In all but one case, that we will not be discussing here, these algorithms produce the same results. Use the token bucket and leaky bucket Wikipedia entries as a guide if you are reading older networking literature.\nThe basic token bucket traffic shaper is characterized by two parameters: a rate, r, and a bucket size, b. The rate can be in either bits or bytes per second and the bucket size can be in either bits or bytes.\nThe key pieces of the shaper are:\nToken generator with rate r. This requires some type of relatively accurate timing mechanism. The granularity of this mechanism heavily influences the accuracy to which r can be approximated.\nToken bucket with size b. This can be implemented with up/down counters or adders/subtractors in hardware or software.\nA buffer (queue) to hold packets\nLogic to control the release of packets based on packet size and current bucket fill.\nPolicing # A Single Rate Three Color Marker # A policer called a single rate three color marker (srTCM) is defined in RFC2697 an informational RFC. The single rate means we will use a single token generator with a specified rate parameter. By three color they mean that packets will be marked with by three levels of compliance (Green, Yellow, Red) from compliant to most out of compliance.\nThis particular policer is defined by three traffic parameters, an update algorithm, and marking criteria. The three traffic parameters are:\nCommitted Information Rate (CIR) measured in bytes of IP packets per second, i.e., it includes the IP header, but not link specific headers. This is the token rate.\nCommitted Burst Size (CBS) measured in bytes. This is the token bucket size associated with the rate.\nExcess Burst Size (EBS) measured in bytes. This is a bucket filled from the overflow of the first bucket hence the expression \u0026ldquo;excesses burst size\u0026rdquo;. This bucket allows us to save up tokens from periods of inactivity for later use. The CBS and EBS must be configured so that at least one of them is larger than 0.\nThere are two modes of operation for the srTCM one, called Color-Aware works with packets that have been previously marked by another policer in the network, the other mode called Color-Blind works with packets that have not been previously marked. The Color-Blind mode of operation is shown in below.\nTwo Rate Three Color Marker # Another policer called a two rate three color marker (trTCM) is defined in RFC2698, an informational RFC. The two rate means we will use a two token generators each with a specified rate parameter. Once again the three colors Green, Yellow, Red indicate the level of compliance which gets translated into packet drop precedence when the packets get marked. The parameters for this policer are: the Peak Information Rate (PIR), the Peak Burst Size (PBS), the Committed Information Rate (CIR), and the Committed Burst Size (CBS). All use units similar to that of the srTCM. The initialization, update, and marking behavior for the Color-Blind mode is shown below.\nReferences:\nhttps://www.grotto-networking.com/BBQoS.html "},{"id":20,"href":"/tech-book/docs/networking-tips/spine-leaf-arch/","title":"Spine-leaf Architecture Basics","section":"Networking Tips","content":" Intro # What Is Spine-leaf Architecture? # The spine-leaf architecture consists of only two layers of switches: spine and leaf switches. The spine layer consists of switches that perform routing and work as the core of the network. The leaf layer involves access switches that connect to servers, storage devices, and other end-users. This structure helps data center networks reduce hop count and reduce network latency.\nIn the spine-leaf architecture, each leaf switch is connected to each spine switch. With this design, any server can communicate with any other server, and there is no more than one interconnected switch path between any two leaf switches.\nWhy Use Spine-leaf Architecture? # The spine-leaf architecture has become a popular data center architecture, bringing many advantages to the data center, such as scalability, network performance, etc. The benefits of spine-leaf architecture in modern networks are summarized here in five points.\nIncreased redundancy: The spine-leaf architecture connects the servers with the core network, and has higher flexibility in hyper-scale data centers. In this case, the leaf switch can be deployed as a bridge between the server and the core network. Each leaf switch connects to all spine switches, which creates a large non-blocking fabric, increasing the level of redundancy and reducing traffic bottlenecks.\nIncreased bandwidth: The spine-leaf architecture can effectively avoid traffic congestion by applying protocols or techniques such as transparent interconnection of multiple links (TRILL) and shortest path bridging (SPB). The spine-leaf architecture can be Layer 2 or Layer 3, so uplinks can be added to the spine switch to expand inter-layer bandwidth and reduce oversubscription to secure network stability.\nImprove scalability: The spine-leaf architecture has multiple links that can carry traffic. The addition of switches will improve scalability and help enterprises to expand their business later.\nReduced expenses: A spine-leaf architecture increases the number of connections each switch can handle, so data centers require fewer devices to deploy. Many data center networks employ spine-leaf architecture to minimize costs.\nMinimal latency and congestion: By limiting the maximum number of hops to two between any source and destination nodes, we establish a more direct traffic path, enhancing overall performance and mitigating bottlenecks. The only exception is when the destination is on the same leaf switch.\nSpine-leaf vs. Traditional Three-Tier Architecture # The main difference between spine-leaf architecture and 3-tier architecture lies in the number of network layers, and the traffic they transform is north-south or east-west traffic.\nAs shown in the following figure, the traditional three-tier network architecture consists of three layers: core, aggregation and access. The access switches are connected to servers and storage devices, the aggregation layer aggregates the access layer traffic, provides redundant connections at the access layer, and the core layer provides network transmission. But this three-layer topology is usually designed for north-south traffic and uses the STP protocol, supporting up to 100 switches. In the case of continuous expansion of network data, this will inevitably result in port blockage and limited scalability.\nThe spine-leaf architecture is to add east-west traffic parallelism to the north-south network architecture of the backbone, fundamentally solving the bottleneck problem of the traditional three-tier network architecture. It increases the exchange layer under the access layer, and the data transmission between two nodes is completed directly at this layer, thereby diverting backbone network transmission. Compared with traditional three-tier architecture, the spine-leaf architecture provides a connection through the spine with a single hop between leaves, minimizing any latency and bottle necks. In spine-leaf architectures, the switch configuration is fixed so that no network changes are required for a dynamic server environment.\nHow to Design Spine-leaf Architecture? # Before designing a spine-leaf architecture, you need to figure out some important and relevant considerations, especially the oversubscription rate and the size of the spine switch. Surely, we have also given a detailed example for your reference.\nDesign Considerations of Spine-leaf Architecture # Oversubscription rate: It is the contention rate when all devices are sending traffic at the same time. It can be measured in the north/south direction (traffic entering/leaving the data center) and in the east/west direction (traffic between devices within the data center). The most appropriate oversubscription ratio for modern network architectures is 3:1 or less, which is measured and delineated as a ratio between upstream bandwidth (to backbone switches) and downstream capacity (to servers/storage).\nFor example, a leaf switch has 48 x 10G ports for a total of 480Gb/s of port capacity. If you connect 4 x 40G uplink ports from each leaf switch to a 40G spine switch, it will have 160Gb/s of uplink capacity. The ratio is 480:160, or 3:1. However, data center uplinks are typically 40G or 100G and can be migrated over time from a starting point of 40G (Nx 40G) to 100G (Nx 100G). It is important to note that the uplink should always run faster than the downlink to not have port link blockage.\nLeaf and spine sizing: The maximum number of leaf switches in the topology is determined by the port density of the spine switches. And the number of spine switches will be governed by the combination of the required throughput between the leaf switches, the number of redundant/ECMP (equivalent multipath) paths, and their port density. So the number of spine-leaf switches and port density need to be taken into account to prevent network problems.\nLayer 2 or Layer 3 design: A two-tier spine-leaf fabric can be built at either Layer 2 (configuring VLANs) or Layer 3 (subnetting). Layer 2 designs need to provide maximum flexibility, allowing VLANs to span anywhere and MAC addresses to migrate anywhere. Layer 3 designs need to provide the fastest convergence times and maximum scale with fan-out ECMP supporting up to 32 or more active spine switches.\nReferences:\nhttps://community.fs.com/article/leaf-spine-with-fs-com-switches.html "},{"id":21,"href":"/tech-book/docs/networking-tips/tcp-congestion/","title":"TCP Congestion Control","section":"Networking Tips","content":" Intro # TCP is a protocol that is used to transmit information from one computer on the internet to another, and is the protocol I’ll be focused on in this post. What distinguishes TCP from other networking protocols is that it guarantees 100% transmission. This means that if you send 100kb of data from one computer to another using TCP, all 100kb will make it to the other side. This property of TCP is very powerful and is the reason that many network applications we use, such as the web and email are built on top of it. The way TCP is able to accomplish this goal of trasmitting all the information that is sent over the wire is that for every segment of data that is sent from party A to party B, party B sends an “acknowledgement” segment back to party A indicating that it got that message.\nWhen does congestion happen? # Congestion is problem in computer networks because at the end of the day, information transfer rates are limited by physical channels like ethernet cables or cellular links, and on the internet, many individual devices are connected to these links.\nDetour: What is a link? # Before I dive into what some solutions to this problem are, I want to be a little bit more specific about the properties of links. There are three important details to know about a link:\ndelay (milliseconds) - the time it takes for one packet to get from the beginning to the end of a link bandwidth (megabits/second) - the number of packets that can get through the link in a second queue - the size of the queue for storing packets waiting to be sent out if a link is full, and the strategy for managing that queue when it hits its capacity Using the analogy of the link as a pipe, you can think of the delay as the length of the pipe, and the bandwidth as the circumference of the pipe. An important statistic about a link is the bandwidth-delay product (BDP). If senders are sending more bytes than the BDP, the link’s queue will fill and eventually start dropping packets.\nApproaches # There are two main indicators: packet loss and increased round trip times for packets. When congestion happens, queues on links begin to fill up, and packets get dropped. If a sender notices packet loss, it’s a pretty good indicator that congestion is occuring. Another consequence of queues filling up though is that if packets are spending more time in a queue before making it onto the link, the round trip time, which measures the time from when the sender sends a segment out to the time that it receives an acknowledgement, will increase. While today there are congestion control schemes that take into account both of these indicators, in the original implementations of congestion control, only packet loss was used.\nTherefore, in addition to being able to avoid congestion, congestion control approaches need to be able to “explore” the available bandwidth.\nControl-Based Algorithms # The Congestion Window # A key concept to understand about any congestion control algorithm is the concept of a congestion window. The congestion window refers to the number of segments that a sender can send without having seen an acknowledgment yet. If the congestion window on a sender is set to 2, that means that after the sender sends 2 segments, it must wait to get an acknowledgment from the receiver in order to send any more. The congestion window is often referred to as the “flight size”, because it also corresponds to the number of segments “in flight” at any given point in time.\nThe higher the congestion window, more packets you’ll be able to get across to the receiver in the same time period. To understand this intuitively, if the delay on the network is 88ms, and the congestion window is 10 segments, you’ll be able to send 10 segments for every round trip (88*2 = 176 ms), and if it’s 20 segments, you’ll be able to send 20 segments in the same time period.\nOf course, the risk with raising the congestion window too high is that it will lead to congestion. The goal of a congestion control algorithm, then, is to figure out the right size congestion window to use.\nFrom a theoretical perspective, the right size congestion window to use is the bandwidth-delay product of the link, which as we discussed earlier is the full capacity of the link. The idea here is that if the congestion window is equal to the BDP of the link, it will be fully utilized, and not cause congestion.\nTCP Tahoe # TCP Tahoe is a congestion control scheme that was invented back in the 80s, when congestion was first becoming a problem on the internet. The algorithm itself is fairly simple, and grows the congestion window in two phases.\nPhase 1 # Slow Start: The algorithm begins in a state called “slow start”. In Slow Start, the congestion window grows by 1 every time an acknowledgement is received. This effectively doubles the congestion window on every round trip. If the congestion window is 4, four packets will be in flight at once, and when each of those packets is acknowledged, the congestion window will increase by 1, resulting in a window of size 8. This process continues until the congestion window hits a value called the “Slow Start Threshold” ssthresh. This is a configurable number.\nPhase 2 # Congestion Avoidance: Once the congestion window has hit the ssthresh, it moves from “slow start” into congestion avoidance mode. In congestion avoidance, the congestion window increases by 1 on every round trip. So if the congestion window is 4, the window will increase to 5 after all four of those packets in flight have been acknowledged. This increases the window much more slowly.\nIf Tahoe detects that a packet is lost, it will resend the packet, the slow start threshold is updated to be half the current congestion window, the congestion window is set back to 1, and the algorithm goes back to slow start.\nDetecting Packet Loss \u0026amp; Fast Retransmit # There are two ways that a TCP sender could detect that a packet is lost.\nThe sender “times out”. The sender puts a timeout on every packet that is sent out into the wild, and when that timeout is hit without that packet having been acknowledged, it resends the packet and sets the congestion window to 1.\nThe receiver sends back “duplicate acks”. In TCP, receivers only acknowledge packets that are sent in order. If a packet is sent out of order, it will send out an acknowledgement for the last packet it saw in order. So, if a receiver has received segments 1,2, and 3, and then receives segment #5, it will ack segment #3 again, because #5 came in out of order. In Tahoe, if a sender receives 3 duplicate acks, it considers a packet lost. This is considered “Fast Retransmit”, because it doesn’t wait for the timeout to happen.\nThere are a number of issues with this approach though, which is why it is no longer used today. In particular, it takes a really long time, especially on higher bandwidth networks, for the algorithm to actually take full advantage of the available bandwidth. This is because the window size grows pretty slowly after hitting the slow start threshold. Another issue is that packet loss doesn’t necessarily mean that congestion is occuring–some links, like WiFi, are just inherently lossy. Reacting drastically by cutting the window size to 1 isn’t necessarily always appropriate. A final issue is that this algorithm uses packet loss as the indicator for whether there’s congestion. If the packet loss is happening due to congestion, you are already too late–the window is too high, and you need to let the queues drain.\nTCP CUBIC # This algorithm was implemented in 2005, and is currently the default congestion control algorithm used on Linux systems. Like Tahoe, it relies on packet loss as the indicator of congestion. However, unlike Tahoe, it works far better on high bandwidth networks, since rather than increasing the window by 1 on every round trip, it uses, as the name would suggest, a cubic function to determine what the window size should be, and therefore grows much more quickly.\nAvoidance-Based Algorithms # BBR(Bottleneck Bandwidth and RTT) - (Bufferbloat) # This is a very recent algorithm developed by Google, and unlike Tahoe or CUBIC, uses delay as the indicator of congestion, rather than packet loss. The rough thinking behind this is that delays are a leading indicator of congestion–they occur before packets actually start getting lost. Slowing down the rate of sending before the packets get lost ends up leading to higher throughput.\nActive Queue Management # Random Early Detection # Each router is programmed to monitor its own queue length and, when it detects that congestion is imminent, to notify the source to adjust its congestion window. RED, invented by Sally Floyd and Van Jacobson in the early 1990s.\nRED is most commonly implemented such that it implicitly notifies the source of congestion by dropping one of its packets. The source is, therefore, effectively notified by the subsequent timeout or duplicate ACK. In case you haven’t already guessed, RED is designed to be used in conjunction with TCP, which currently detects congestion by means of timeouts (or some other means of detecting packet loss such as duplicate ACKs). As the “early” part of the RED acronym suggests, the gateway drops the packet earlier than it would have to, so as to notify the source that it should decrease its congestion window sooner than it would normally have. In other words, the router drops a few packets before it has exhausted its buffer space completely, so as to cause the source to slow down, with the hope that this will mean it does not have to drop lots of packets later on.\nhow RED decides when to drop a packet and what packet it decides to drop. To understand the basic idea, consider a simple FIFO queue. Rather than wait for the queue to become completely full and then be forced to drop each arriving packet (the tail drop policy), we could decide to drop each arriving packet with some drop probability whenever the queue length exceeds some drop level. This idea is called early random drop. The RED algorithm defines the details of how to monitor the queue length and when to drop a packet.\nExplicit Congestion Notification || IP \u0026amp; TCP Flags # While TCP’s congestion control mechanism was initially based on packet loss as the primary congestion signal, it has long been recognized that TCP could do a better job if routers were to send a more explicit congestion signal. That is, instead of dropping a packet and assuming TCP will eventually notice (e.g., due to the arrival of a duplicate ACK), any AQM algorithm can potentially do a better job if it instead marks the packet and continues to send it along its way to the destination. This idea was codified in changes to the IP and TCP headers known as Explicit Congestion Notification (ECN), as specified in RFC 3168.\nSpecifically, this feedback is implemented by treating two bits in the IP TOS field as ECN bits. One bit is set by the source to indicate that it is ECN-capable, that is, able to react to a congestion notification. This is called the ECT bit (ECN-Capable Transport). The other bit is set by routers along the end-to-end path when congestion is encountered, as computed by whatever AQM algorithm it is running. This is called the CE bit (Congestion Encountered).\nIn addition to these two bits in the IP header (which are transport-agnostic), ECN also includes the addition of two optional flags to the TCP header. The first, ECE (ECN-Echo/Experienced), communicates from the receiver to the sender that it has received a packet with the CE bit set. The second, CWR (Congestion Window Reduced) communicates from the sender to the receiver that it has reduced the congestion window.\nBeyond TCP # Datacenters (DCTCP) # There have been several efforts to optimize TCP for cloud datacenters, where Data Center TCP was one of the first. There are several aspects of the datacenter environment that warrant an approach that differs from more traditional TCP. These include:\nRound trip time for intra-DC traffic are small; Buffers in datacenter switches are also typically small; All the switches are under common administrative control, and thus can be required to meet certain standards; A great deal of traffic has low latency requirements; That traffic competes with high bandwidth flows. It should be noted that DCTCP is not just a version of TCP, but rather, a system design that changes both the switch behavior and the end host response to congestion information received from switches.\nThe central insight in DCTCP is that using loss as the main signal of congestion in the datacenter environment is insufficient. By the time a queue has built up enough to overflow, low latency traffic is already failing to meet its deadlines, negatively impacting performance. Thus DCTCP uses a version of ECN to provide an early signal of congestion. But whereas the original design of ECN treated an ECN marking much like a dropped packet, and cut the congestion window in half, DCTCP takes a more finely-tuned approach. DCTCP tries to estimate the fraction of bytes that are encountering congestion rather than making the simple binary decision that congestion is present. It then scales the congestion window based on this estimate. The standard TCP algorithm still kicks in should a packet actually be lost. The approach is designed to keep queues short by reacting early to congestion while not over-reacting to the point that they run empty and sacrifice throughput.\nThe key challenge in this approach is to estimate the fraction of bytes encountering congestion. Each switch is simple. If a packet arrives and the switch sees the queue length (K) is above some threshold; e.g., K \u0026gt; (RTT * C) / 7\nwhere C is the link rate in packets per second, then the switch sets the CE bit in the IP header. The complexity of RED is not required.\nThe receiver then maintains a boolean variable for every flow, which we’ll denote DCTCP.CE, and sets it initially to false. When sending an ACK, the receiver sets the ECE (Echo Congestion Experienced) flag in the TCP header if and only if DCTCP.CE is true. It also implements the following state machine in response to every received packet:\nIf the CE bit is set and DCTCP.CE=False, set DCTCP.CE to True and send an immediate ACK.\nIf the CE bit is not set and DCTCP.CE=True, set DCTCP.CE to False and send an immediate ACK.\nOtherwise, ignore the CE bit.\nHTTP Performance (QUIC) # HTTP has been around since the invention of the World Wide Web in the 1990s and from its inception it has run over TCP. HTTP/1.0, the original version, had quite a number of performance problems due to the way it used TCP, such as the fact that every request for an object required a new TCP connection to be set up and then closed after the reply was returned. HTTP/1.1 was proposed at an early stage to make better use of TCP. TCP continued to be the protocol used by HTTP for another twenty-plus years.\nIn fact, TCP continued to be problematic as a protocol to support the Web, especially because a reliable, ordered byte stream isn’t exactly the right model for Web traffic. In particular, since most web pages contain many objects, it makes sense to be able to request many objects in parallel, but TCP only provides a single byte stream. If one packet is lost, TCP waits for its retransmission and successful delivery before continuing, while HTTP would have been happy to receive other objects that were not affected by that single lost packet. Opening multiple TCP connections would appear to be a solution to this, but that has its own set of drawbacks including a lack of shared information about congestion across connections.\nOther factors such as the rise of high-latency wireless networks, the availability of multiple networks for a single device (e.g., Wi-Fi and cellular), and the increasing use of encrypted, authenticated connections on the Web also contributed to the realization that the transport layer for HTTP would benefit from a new approach. The protocol that emerged to fill this need was QUIC.\nQUIC originated at Google in 2012 and was subsequently developed as a proposed standard at the IETF. It has already seen a solid amount of deployment—it is in most Web browsers, many popular websites, and is even starting to be used for non-HTTP applications. Deployability was a key consideration for the designers of the protocol. There are a lot of moving parts to QUIC—its specification spans three RFCs of several hundred pages—but we focus here on its approach to congestion control, which embraces many of the ideas we have seen to date in this book.\nLike TCP, QUIC builds congestion control into the transport, but it does so in a way that recognizes that there is no single perfect congestion control algorithm. Instead, there is an assumption that different senders may use different algorithms. The baseline algorithm in the QUIC specification is similar to TCP NewReno, but a sender can unilaterally choose a different algorithm to use, such as CUBIC. QUIC provides all the machinery to detect lost packets in support of various congestion control algorithms.\nMultipath Transport # While the early hosts connected to the Internet had only a single network interface, it is common these days to have interfaces to at least two different networks on a device. The most common example is a mobile phone with both cellular and WiFi interfaces. Another example is datacenters, which often allocate multiple network interfaces to servers to improve fault tolerance. Many applications use only one of the available networks at a time, but the potential exists to improve performance by using multiple interfaces simultaneously. This idea of multipath communication has been around for decades and led to a body of work at the IETF to standardize extensions to TCP to support end-to-end connections that leverage multiple paths between pairs of hosts. This is known as Multipath TCP (MPTCP).\nA pair of hosts sending traffic over two or more paths simultaneously has implications for congestion control. For example, if both paths share a common bottleneck link, then a naive implementation of one TCP connection per path would acquire twice as much share of the bottleneck bandwidth as a standard TCP connection. The designers of MPTCP set out to address this potential unfairness while also realizing the benefits of multiple paths. The proposed congestion control approach could equally be applied to other transports such as QUIC.\nReferences:\nhttps://tcpcc.systemsapproach.org/aqm.html https://squidarth.com/rc/programming/networking/2018/07/18/intro-congestion "},{"id":22,"href":"/tech-book/posts/2022-01-18-first-doc/","title":"First Blog","section":"Blog","content":" Preface # This is my black board for my future technical book. There is no structure of this blog posts. Whenever I find a good technical literature, I am planning to add it here.\nFeedback is very important for any development cycle. Please drop a message at prasenjit.manna@gmail.com.\nThanks, Prasenjit Manna\n"},{"id":23,"href":"/tech-book/docs/algorithms/","title":"Algorithms","section":"Example Site","content":" Algorithms # In this sections, all the interesting algorithms will be classified into simple, medium and hard.\n"},{"id":24,"href":"/tech-book/docs/data-center/","title":"Data Center Tips","section":"Example Site","content":" Data Center Topics # In this sections, all the data center topics will be classified into simple, medium and hard.\n"},{"id":25,"href":"/tech-book/docs/manageability/","title":"Manageability","section":"Example Site","content":" Manageability Topics # In this sections, all the manageability topics will be classified into simple, medium and hard.\n"},{"id":26,"href":"/tech-book/docs/networking-tips/","title":"Networking Tips","section":"Example Site","content":" Networking Tips # In this sections, all the interesting networking tips will be classified into simple, medium and hard.\n"},{"id":27,"href":"/tech-book/docs/programming-tips/","title":"Programming Tips","section":"Example Site","content":" Programming Tips # Bit Manipulation # XORing a bit with 1 always flips the bit, whereas XO Ring with O will never change it. Miscellaneous # Passing a 2D array to a C++ function\nThere are three ways to pass a 2D array to a function:\nThe parameter is a 2D array\nint array[10][10]; void passFunc(int a[][10]) { // ... } passFunc(array); The parameter is an array containing pointers\nint *array[10]; for(int i = 0; i \u0026lt; 10; i++) array[i] = new int[10]; void passFunc(int *a[10]) //Array containing pointers { // ... } passFunc(array); The parameter is a pointer to a pointer\nint **array; array = new int *[10]; for(int i = 0; i \u0026lt;10; i++) array[i] = new int[10]; void passFunc(int **a) { // ... } passFunc(array); "},{"id":28,"href":"/tech-book/docs/systemdesign-tips/","title":"SystemDesign-Tips","section":"Example Site","content":" System-Tips # In this sections, all the essential concents will be described.\nStorage # Disk - HDD(Hard-disk drive) and SSD(solid state drive). SSD is faster than HDD, hence costlier also. Persistent Storage. Memory - RAM (Random access momory). Volatile storage Latency and Throughput # Latency - Time it takes for a certain operation to complete, unit msec or sec.\nReading 1 MB from RAM: 250 us (0.25ms)\nReading 1 MB from SSD: 1,000 ps (1 ms)\nReading 1 MB from HDD: 20,000 is (20 ms)\nTransfer 1 MB over Network: 10,000 pus (10 ms)\nInter-Continental Round Trip: 150,000 ps (150 ms)\nThroughput - The number of operations that a system can handle properly per time unit. For instance the throughput of a sec measured in requests per second (RPS or QPS).\nAvailability # Availability - The odds of a particular server or service being up and running at any point in time, usually measured in percentages. A server that has 99% availability will be operational 99% of the time (this would be described as having two nines of availability).\nHigh Availability - Used to describe systems that have particularly high levels of availability, typically 5 nines or more; sometimes abbreviated \u0026ldquo;HA\u0026rdquo;,\nNines - Typically refers to percentages of uptime. For example, 5 nines of availability means an uptime of 99.999% of the time. Below are the downtimes expected per year depending on those 9s:\n99% (two 9s): 87.7 hours\n99.9% (three 9s): 8.8 hours\n99.99%: 52.6 minutes\n99.999%: 5.3 minutes\nCaching # Cache - A piece of hardware or software that stores data, typically meant to retrieve that data faster than otherwise. Caches are often used to store responses to network requests as well as results of computationally tong operations. Note that data in a cache can become stale if the main source of truth for that data (i.e., the main database behind the cache) gets updated and the cache doesn\u0026rsquo;t.\nCache Hit When requested data is found in a cache.\nCache Miss When requested data could have been found in a cache but isn\u0026rsquo;t. This is typically used to refer to a negative consequence of a system failure or of a poor design choice. For example: If a server goes down, our load balancer will have to forward requests to a new server, which will result in cache misses.\nCache Eviction Policy The policy by which values get evicted or removed from a cache. Popular cache eviction policies include LRU (least-recently used), FIFO (first in first out), and LFU (least-frequently used).\nContent Delivery Network A CDN is a third-party service that acts like a cache for your servers. Sometimes, web applications can be slow for users in a particular region if your servers are located only in another region. A CDN has servers all around the world, meaning that the latency to a CDN\u0026rsquo;s servers will almost always be far better than the latency to your servers. A CDN\u0026rsquo;s servers are often referred to as PoPs (Points of Presence). Two of the most popular CDNs are Cloudflare and Google Claud CDN.\nProxies # Forward Proxy A server that sits between a client and servers and acts on behalf of the client, typically used to mask the client\u0026rsquo;s identity (IP address), Note that forward proxies are often referred to as just proxies.\nReverse Proxy A server that sits between clients and servers and acts on behalf of the servers, typically used for logging, load balancing, or caching. | nginx @ Pronounced \u0026ldquo;engine X°\u0026ndash;not \u0026ldquo;N jinx”, Nginx is a very popular webserver that\u0026rsquo;s often used as a reverse proxy and load balancer. Learn more: https://www.nginx.com/\nLoad Balancer A type of reverse proxy that distributes traffic across servers, Load balancers can be found in many parts of a system, from the DNS layer all the way to the database layer. Learn more: https://www.nginx.com/\nServer-Selection Strategy How a load balancer chooses servers when distributing traffic amongst multiple servers. Commonly used strategies include roundrobin, random selection, performance-based selection (choosing the server with the best performance metrics, like the fastest response time or the least amount of traffic), and IP-based routing.\nDatabases # Relational Database A type of structured database in which data is stored following a tabular format; often supports powerful querying using SQL. Learn more: https://www.postgresql.org/\nNon-Relational Database In contrast with relational database (SQL databases), a type of database that is free of Imposed, tabular-like structure. Non-relational databases are often referred to as NoSQL databases,\nACID Transaction\nAtomicity: The operations that constitute the transaction will either all succeed or ail fail. There is no in-between state. Consistency: The transaction cannot bring the database to an invalid state. After the transaction is committed or rolled back, the rules for each record will still apply, and all future transactions will see the effect of the transaction. Also named Strong Consistency. isolation: The execution of multiple transactions concurrently will have the same effect as if they had been executed sequentially. Durability: Any committed transaction is written to non-volatile storage. It will not be undone by a crash, power loss, or network partition. Strong Consistency Strong Consistency usually refers to the consistency of ACID transactions, as opposed to Eventual Consistency.\nEventual Consistency A consistency mode which is unlike Strong Consistency. In this model, reads might return a view of the system that is stale. An eventually consistent datastore will give guarantees that the state of the database will eventuall reflect writes within a time period.\nNo-SQL Databases # Key-Value Store A Key-Value Store is a flexible NoSQL database that\u0026rsquo;s often used for caching and dynamic configuration. Popular aptions include DynamoDB, Etcd, Redis, and ZooKeeper,\netcd Etcd is a strongly consistent and highly available key-value store that\u0026rsquo;s often used to Implement leader election in a system, Learn more: https://etcd.io/\nRedis An in-memory key-value store. Does offer some persistent storage options but is typically used as a really fast, best-effort caching solution, Redis is also often used to implement rate limiting. Learn more: https://redis.io/\nZookeeper Zookeeper Is a strongly consistent, highly available key-value store. It\u0026rsquo;s often used to store important configuration of to perform leader election. Learn more: https://zookeeper.apache.org/\nDynamoDB An key-value store by AWS, this provides eventual consistency.\nBlob Storage Widely used kind of storage, in small and large scale systems. They don’t really count as databases per se, partially because they only allow the user to store and retrieve data based on the name of the blob. This is sort of like a key-value store but usually blob stores have different guarantees. They might be slower than KV stores but values can be megabytes large (or sometimes gigabytes large). Usually people use this to store things like large binaries, database snapshots, or images and other static assets that a website might have. Blob storage is rather complicated to have on premise, and only giant companies like Google and Amazon have infrastructure that supports it. So usually in the context of System Design interviews you can assume that you will be able to use GCS or S3. These are blob storage services hosted by Google and Amazon respectively, that cost money depending on how much storage you use and how often you store and retrieve blobs from that storage.\nGoogle Cloud Storage(GCS) - is a blob storage service provided by Google. Learn more: https://cloud.google.com/storage\nS3 - ls a blob storage service provided by Amazon through Amazon Web Services (AWS). Learn more: https://aws.amazon.com/s3/\nTime Series Database A TSDB is a special kind of database optimized for storing and analyzing time-indexed data: data points that specifically occur at a given moment in time. Examples of TSDBs are InfluxDB, Prometheus, and Graphite.\ninfluxDB - popular open-source time series database, Learn more; https://www.influxdata.com/\nPrometheus - A popular open source time series database, typically used for monitoring purposes. https://prometheus.io\nGraph Database A type of database that stores data following the graph data model. Data entries in a graph database can have explicitly defined relationships, much like nodes in a graph can have edges. Graph databases take advantage of their underlying graph structure to perform complex queries on deeply connected data very fast. Graph databases are thus often preferred to relational databases when dealing with systems where data points naturally form a graph and have multiple tevels of relationships—for example, social networks.\nNeo4j - a popular grpah DB, consists of nodes, relationships, propreties and labels. https://neo4j.com Spatial Database A type of database optimized for storing and querying spatial data like locations on a map. Spatial databases rely on spatial indexes fike quadtrees to quickly perform spatial queries like finding all locations in the vicinity of a region.\nReplication \u0026amp; Shrading # Replication - The act of duplicating the data from one database server to others. This is sometimes used to increase the redundancy of your system and tolerate regional failures for instance. Other times you can use replication to move data closer to your clients, thus decreasing latency of accessing specific data.\nSharding - Sometimes called data partitioning, sharding is the act of splitting a database into two or more pieces called shards and is typically done to increase the throughput of your database. Popular sharding strategies include:\nSharding based on a client\u0026rsquo;s region. Sharding based on the type of data being stored (e.g: user data gets stored in one shard, payments data gets stored In another shard) Sharding based on the hash of a column (only for structured data) Peer-To-Peer Networks # Peer-To-Peer Network A collection of machines referred to as peers that divide a workload between themselves to presumably complete the workload faster than would otherwise be possible. Peer-to-peer networks are often used in file-distribution systems.\nGossip Protocol When a set of machines talk to each other in a uncoordinated manner in a cluster to spread information through a system without requiring a central source of data. - ad °\nRate Limiting # Rate Limiting The act of limiting the number of requests sent to or from a system. Rate limiting is most often used to limit the number of incoming requests in order to prevent DoS attacks and can be enforced at the IP-address level, at the user-account level, or at the region level, for example. Rate limiting can also be implemented in tiers; for instance, a type of network request could be limited to 1 per second, 5 per 10 seconds, and 10 per minute.\nDoS Attack Short for “denial-of-service attack”, a DoS attack is an attack in which a malicious user tries to bring down or damage a system in order to render it unavailable to users. Much of the time, it consists of flooding it with traffic. Some DoS attacks are easily preventable with rate limiting, while others can be far trickier to defend against.\nDDoS Attack\nShort for “distributed denial-of-service attack\u0026rdquo;, a DDoS attack is a DoS attack in which the traffic flooding the target system comes from many different sources (like thousands of machines), making It much harder to defend against.\nRedis An in-memory key-value store. Does offer some persistent storage options but Is typically used as a really fast, best-effort caching solution, Redis is also often used to implement rate limiting. Learn more: https://redis.io/\nPublish/Subscribe Pattern # Publish/Subscribe Pattern Often shortened as Pub/Sub, the Publish/Subscribe pattern Is a popular messaging model that consists of publishers and subscribers. Publishers publish messages to special topics (sometimes called channels) without caring about or even knowing who will read those messages, and subscribers subscribe to topics and read messages coming through those topics. Pub/Sub systems often come with very powerful guarantees like at-least-once delivery, persistent storage, ordering of messages, and replayability of messages.\nApache Kafka A distributed messaging system created by Linkedin. Very useful when using the streaming paradigm as opposed to polling. Learn more: https://kafka.apache.org/\nCloud pub/sub A highly-scalable Pub/Sub messaging service created by Google, Guarantees at-least-once delivery of messages and supports “rewinding” in order to reprocess messages. Learn more: https://cloud.google.com/pubsub/\nMapReduce # MapReduce A popular framework for processing very large datasets in a distributed setting efficiently, quickly, and in a fault-tolerant manner. A MapReduce job is comprised of 3 main steps:\nthe Map step, which runs a map function on the various chunks of the dataset and transforms these chunks into intermediate key-value pairs. the Shuffle step, which reorganizes the intermediate key-value pairs such that pairs of the same key are routed to the same machine in the final step. the Reduce step, which runs a reduce function on the newly shuffled key-value pairs and transforms them into more meaningful data. The canonical example of a MapReduce use case is counting the number of occurrences of words in a large text file. When dealing with a MapReduce library, engineers and/or systems administrators only need to worry about the map and reduce functions, as well as their inputs and outputs. All other concerns, including the parallelization of tasks and the fault-tolerance of the MapReduce job, are abstracted away and taken care of by the MapReduce implementation. Distributed File System A Distributed Ale System is an abstraction over a (usually large) cluster of machines that allows them to act like one large file system. The two most popular implementations of a DFS are the Google File System (GFS) and the Hadoop Distributed File System (HDFS). Typically, DFSs take care of the classic availability and replication guarantees that can be tricky to obtain in a distributed-system setting, The overarching idea is that files are split into chunks of a certain size (4MB or 64MB, for instance), and those chunks are sharded across a large cluster of machines. A central control plane is in charge of deciding where each chunk resides, routing reads to the right nodes, and handling communication between machines, Olfferent DFS implementations have slightly different APIs and semantics, but they achieve the same common goal: extremely largescale persistent storage,\nHadoop A popular, open-source framework that supports MapReduce jobs and many other kinds of data-processing pipelines. Its central component is HDFS (Hadoop Distributed File System), on top of which other technologies have been developed. Learn more: https://hadoop.apache.org/\nSecurity And HTTPS # Symmetric Encryption A type of encryption that relies on only a single key to both encrypt and decrypt data. The key must be known to all parties involved in the communication and must therefore typically be shared between the parties at one point or another. Symmetric-key algorithms tend to be faster than their asymmetric counterparts. The most widely used symmetric-key algorithms are part of the Advanced Encryption Standard (AES).\nAsymmetric Encryption Also known as public-key encryption, asymmetric encryption relies on two keys—a public key and a private key—to encrypt and decrypt data. The keys are generated using cryptographic algorithms and are mathematically connected such that data encrypted with the public key can only be decrypted with the private key. While the private key must be kept secure to maintain the fidelity of this encryption paradigm, the public key can be openly shared. Asymmetric-key algorithms tend to be slower than their symmetric counterparts.\nAES Stands for Advanced Encryption Standard. AES is a widely used encryption standard that has three symmetric-key algorithms (AES-128, AES-192, and AES-256). Of note, AES is considered to be the \u0026ldquo;gold standard\u0026rdquo; in encryption and is even used by the U.S. National Security Agency to encrypt top secret information. .\nHTTPS The HyperText Transfer Protocol Secure is an extension of HTTP that\u0026rsquo;s used for secure communication online. It requires servers to have trusted certificates (usually SSL certificates) and uses the Transport Layer Security (TLS), a security protocol built on top of TCP, to encrypt data communicated between a client and a server. { TLs The Transport Layer Security is a security protocol over which HTTP runs in order to achieve secure communication online. \u0026ldquo;HTTP over TLS\u0026rdquo; Is also known as HTTPS.\nSSL Certificate A digital certificate granted to a server by a certificate authority. Contains the server\u0026rsquo;s public key, to be used as part of the TLS handshake process in an HTTPS connection. An SSL certificate effectively confirms that a public key belongs to the server claiming it belongs to them. SSL certificates are a crucial defense against man-in-the-middie attacks,\nCertificate Authority A trusted entity that signs digital certificates—namely, SSL certificates that are relied on in HTTPS connections.\nTLS Handshake The process through which a client and a server communicating over HTTPS exchange encryption-related information and establish a secure communication. The typical steps in a TLS handshake are roughly as follows:\nThe client sends a client hello string of random bytes—to the server. The server responds with a server hello another string of random bytes—as well as its SSL certificate, which contains its publle key. The client verifies that the certificate was issued by a certificate authority and sends a premaster secret—yet another string of random bytes, this time encrypted with the server\u0026rsquo;s public key—to the server. The client and the server use the client hello, the server helio, and the premaster secret to then generate the same symmetric encryption session keys, to be used to encrypt and decrypt all data communicated during the remainder of the connection. "}]