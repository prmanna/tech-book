[{"id":0,"href":"/tech-book/docs/","title":"Example Site","section":"Introduction","content":" Introduction # Ferre hinnitibus erat accipitrem dixi Troiae tollens # Lorem markdownum, a quoque nutu est quodcumque mandasset veluti. Passim inportuna totidemque nympha fert; repetens pendent, poenarum guttura sed vacet non, mortali undas. Omnis pharetramque gramen portentificisque membris servatum novabis fallit de nubibus atque silvas mihi. Dixit repetitaque Quid; verrit longa; sententia mandat quascumque nescio solebat litore; noctes. Hostem haerentem circuit plenaque tamen.\nPedum ne indigenae finire invergens carpebat Velit posses summoque De fumos illa foret Est simul fameque tauri qua ad # Locum nullus nisi vomentes. Ab Persea sermone vela, miratur aratro; eandem Argolicas gener.\nMe sol # Nec dis certa fuit socer, Nonacria dies manet tacitaque sibi? Sucis est iactata Castrumque iudex, et iactato quoque terraeque es tandem et maternos vittis. Lumina litus bene poenamque animos callem ne tuas in leones illam dea cadunt genus, et pleno nunc in quod. Anumque crescentesque sanguinis progenies nuribus rustica tinguet. Pater omnes liquido creditis noctem.\nif (mirrored(icmp_dvd_pim, 3, smbMirroredHard) != lion(clickImportQueue, viralItunesBalancing, bankruptcy_file_pptp)) { file += ip_cybercrime_suffix; } if (runtimeSmartRom == netMarketingWord) { virusBalancingWin *= scriptPromptBespoke + raster(post_drive, windowsSli); cd = address_hertz_trojan; soap_ccd.pcbServerGigahertz(asp_hardware_isa, offlinePeopleware, nui); } else { megabyte.api = modem_flowchart - web + syntaxHalftoneAddress; } if (3 \u0026lt; mebibyteNetworkAnimated) { pharming_regular_error *= jsp_ribbon + algorithm * recycleMediaKindle( dvrSyntax, cdma); adf_sla *= hoverCropDrive; templateNtfs = -1 - vertical; } else { expressionCompressionVariable.bootMulti = white_eup_javascript( table_suffix); guidPpiPram.tracerouteLinux += rtfTerabyteQuicktime(1, managementRosetta(webcamActivex), 740874); } var virusTweetSsl = nullGigo; Trepident sitimque # Sentiet et ferali errorem fessam, coercet superbus, Ascaniumque in pennis mediis; dolor? Vidit imi Aeacon perfida propositos adde, tua Somni Fluctibus errante lustrat non.\nTamen inde, vos videt e flammis Scythica parantem rupisque pectora umbras. Haec ficta canistris repercusso simul ego aris Dixit! Esse Fama trepidare hunc crescendo vigor ululasse vertice exspatiantur celer tepidique petita aversata oculis iussa est me ferro.\n"},{"id":1,"href":"/tech-book/docs/algorithms/breadth-first-search/","title":"Breadth First Search","section":"Algorithms","content":" Breadth First Search # Intro # Hopefully, by this time, you\u0026rsquo;ve drunk enough DFS kool-aid to understand its immense power and seen enough visualization to create a call stack in your mind. Now let me introduce the companion spell Breadth First Search (BFS). The names are self-explanatory. While depth first search reaches for depth (child nodes) first before breadth (nodes in the same level/depth), breadth first search visits all nodes in a level before starting to visit the next level. While DFS uses recursion/stack to keep track of progress, BFS uses a queue (First In First Out). When we dequeue a node, we enqueue its children.\nBFS template # from collections import deque def bfs_by_queue(root): queue = deque([root]) # at least one element in the queue to kick start bfs while len(queue) \u0026gt; 0: # as long as there is an element in the queue node = queue.popleft() # dequeue for child in node.children: # enqueue children if OK(child): # early return if problem condition met return FOUND(child) queue.append(child) return NOT_FOUND "},{"id":2,"href":"/tech-book/docs/programming-tips/c++/","title":"C++ Tips","section":"Programming Tips","content":" C++ Tips # Templates # A template is a simple yet very powerful tool in C++. The simple idea is to pass the data type as a parameter so that we don’t need to write the same code for different data types. For example, a software company may need to sort() for different data types. Rather than writing and maintaining multiple codes, we can write one sort() and pass the datatype as a parameter.\nC++ adds two new keywords to support templates: ‘template’ and ‘typename’. The second keyword can always be replaced by the keyword ‘class’.\nFunction Templates # We write a generic function that can be used for different data types.\n#include \u0026lt;iostream\u0026gt; using namespace std; template \u0026lt;typename T\u0026gt; T myFunc (T x, T y) { return (x\u0026gt;y)? x:y; } int main () { cout \u0026lt;\u0026lt; myFunc\u0026lt;int\u0026gt;(3,7) \u0026lt;\u0026lt; endl; std::cout \u0026lt;\u0026lt; myFunc\u0026lt;char\u0026gt;(\u0026#39;g\u0026#39;, \u0026#39;e\u0026#39;) \u0026lt;\u0026lt; std::endl; return 0; } Class Templates # Class templates like function templates, class templates are useful when a class defines something that is independent of the data type. Can be useful for classes like LinkedList, BinaryTree, Stack, Queue, Array, etc.\n// C++ Program to implement // template Array class #include \u0026lt;iostream\u0026gt; using namespace std; template \u0026lt;typename T\u0026gt; class Array { private: T* ptr; int size; public: Array(T arr[], int s); void print(); }; template \u0026lt;typename T\u0026gt; Array\u0026lt;T\u0026gt;::Array(T arr[], int s) { ptr = new T[s]; size = s; for (int i = 0; i \u0026lt; size; i++) ptr[i] = arr[i]; } template \u0026lt;typename T\u0026gt; void Array\u0026lt;T\u0026gt;::print() { for (int i = 0; i \u0026lt; size; i++) cout \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; *(ptr + i); cout \u0026lt;\u0026lt; endl; } int main() { int arr[5] = { 1, 2, 3, 4, 5 }; Array\u0026lt;int\u0026gt; a(arr, 5); a.print(); return 0; } Reference: Templates in C++ with Examples\nArray # An array in C++ holds a fixed number of elements of a single type in contiguous memory locations.\nstd::string clothes[3]; // declare a string array of size 3 named clothes int numbers[5]; // declare an int array of size 5 named numbers By default, the values in C++ arrays are undetermined at the time of declaration (meaning that the values can be random). An array can be initialized to specific values by enclosing the values in curly braces. The number of values in {} should not exceed the size of the array. If the number of values inside the braces is less than the size of the array, the rest of the array will be set to the default value for their type, such as 0 for int and false for bool. If an array is initialized to empty curly braces, all elements in the array is set to their default value.\nint a[3] = { 3, 4, 5 }; // int array a = [3, 4, 5] int b[5] = { 1 }; // int array b = [1, 0, 0, 0, 0] int c[2] = { }; // int array c = [0, 0] An element in an array is accessed by its index in O(1) time with square brackets.\nstd::string clothes[5]; // declare an array of size 5 clothes[0] = \u0026#34;shirt\u0026#34;; // initialize first element clothes[1] = \u0026#34;dress\u0026#34;; // initialize second element std::cout \u0026lt;\u0026lt; clothes[0]; // print out \u0026#34;shirt\u0026#34; C++ arrays do not have a length attribute that lets you access its size easily. The common way to get the size of an array is by:\nint arr[5] = { 7, 3, 4, 6, 8 } std::cout \u0026lt;\u0026lt; sizeof(arr)/sizeof(arr[0]) \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; // print out 5 Since C++17, we can also retrieve the size of an array with std::size().\nint arr[] = { 6, 7, 8 } std::cout \u0026lt;\u0026lt; std::size(arr) \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; // print out 3 Vector # When you need to create a dynamically resizable array, C++ provides a useful sequence container vector. Like arrays, it provides efficient constant time element access.\nvector\u0026lt;data_type\u0026gt; vectorName; Commonly used Vector Functions\npush_back() – It is used to insert the elements at the end of the vector. pop_back() – It is used to pop or remove elements from the end of the vector. clear() – It is used to remove all the elements of the vector. empty() – It is used to check if the vector is empty. at(i) – It is used to access the element at the specified index ‘i’. front() – It is used to access the first element of the vector. back() – It is used to access the last element of the vector. erase() – It is used to remove an element at a specified position. size() - returns the size of the array resize() - resize the array with the new size vector_name[i] gets or sets item stored at index i, O(1) emplace_back(item) adds item to the end of the vector, amortized O(1) size() returns the size of the vector, O(1) The emplace_back() operation, which appends a new element to the end of the vector, runs in amortized constant time. A vector will resize itself when needed. A typical resizing implementation is that when the array is full, the array doubles in size, and the old content is copied over to the newer, larger array. The doubling takes O(n) time, but it happens rarely that the overall insertion still takes O(1) time.\nIn general, to iterate through an array, a for loop is easier to reason than while since there\u0026rsquo;s no condition to manage that could skip elements. In C++, there are two types of for loops: the simple for loop and the for-each loop. Use for-each loop if you don\u0026rsquo;t need to access the index since it avoids the possible error of messsing up the index.\nstd::vector\u0026lt;int\u0026gt; numbers{20, 6, 13, 5}; // simple for loop goes through indices so we fetch elements using indices for (int i = 0; i \u0026lt; numbers.size(); i++) { int number = numbers[i]; std::cout \u0026lt;\u0026lt; number \u0026lt;\u0026lt; std::endl; } // for-each loop directly fetches elements for (int number : numbers) { std::cout \u0026lt;\u0026lt; number \u0026lt;\u0026lt; std::endl; } Linked List # C++ doesn\u0026rsquo;t have a built-in singly linked list. Normally at an interview, you\u0026rsquo;ll be given a definition like this:\nstruct LinkedListNode { int val; LinkedListNode* next; } append to end is O(1) finding an element is O(N) Besides problems that specifically ask for linked lists, you don\u0026rsquo;t normally define and use linked list. If you need a list with O(1) append you can use a vector, and if you want O(1) for both prepend and append you can use deque.\nStack # C++ provides a container adaptor std::stack that works on the basis of LIFO (last-in-first-out).\npush: push(item) adds item to end of stack, O(1) pop: pop() removes item at the end of stack, O(1) size: size(), O(1) top: top() returns but doesn\u0026rsquo;t remove item at the end of stack, O(1) Stack is arguably the most magical data structure in computer science. In fact, with two stacks and a finite-state machine you can build a Turing Machine that can solve any problem a computer can solve.\nRecursion and function calls are implemented with stack behind the scene. We\u0026rsquo;ll discuss this in the recursion review module.\nQueue # We normally use std::deque when we need a queue.\nenqueue: push_back(item) inserts item at the end of the queue, O(1) dequeue: pop_front() removes and return item from the head of the queue, O(1) size: size(), O(1) peek: front() returns but don\u0026rsquo;t remove item at the head of the queue, O(1) In coding interviews, we see queues most often in breadth-first search. We\u0026rsquo;ll also cover monotonic deque where elements are sorted inside the deque that is useful in solving some advanced coding problems.\nHash Table # std::unordered_map in C++ implements hash table.\nat(key) gets item mapped to key if present, O(1) map[key] gets item mapped to key if present (returns 0 if not present), and inserts the key if not, O(N) map[key] = item sets item mapped to key, O(1) count(key) finds out if key is present or not, O(1) erase(key) removes item mapped to key if present, O(1) It\u0026rsquo;s worth mentioning that these are average case time complexity. A hash table\u0026rsquo;s worst time complexity is actually O(N) due to hash collision and other things. For the vast majority of the cases and certainly most coding interviews, the assumption of constant time lookup/insert/delete is valid.\nUse a hash table if you want to create a mapping from A to B. Many starter interview problems can be solved with hash tables.\nHash Set # std::unordered_set in C++ is useful in answering existence queries in constant time.\ncount(item) checks if item is in a set, O(1) insert(item) adds item to a set if it\u0026rsquo;s not already present, O(1) erase(item) removes item from a set if it\u0026rsquo;s present, O(1) size() gets the size of the set, O(1) Hash set is useful when you only need to know existence of a key. Example use cases include DFS and BFS on graphs.\nTree # Normally at an interview, you\u0026rsquo;d be given the following implementation for a binary tree:\nstruct Node { int val; Node* left; Node* right; Node(T val, Node* left = nullptr, Node* right = nullptr) : val{val}, left{left}, right{right} {} }; For n-nary trees:\n#include \u0026lt;vector\u0026gt; struct Node { int val; std::vector\u0026lt;Node*\u0026gt; children; Node(int val, std::vector\u0026lt;Node*\u0026gt; children = {}) : val{val}, children{children} {} }; Infinity # Infinity is useful when you want to initialize a variable that is greater or smaller than any value that your algorithm may want to compare with. std::numeric_limits\u0026lt;T\u0026gt; provides the maximum and minimum values of fundamental arithmatic types in C++.\n#include \u0026lt;limits\u0026gt; int max = std::numeric_limits\u0026lt;int\u0026gt;::max(); // 2147483648 int min = std::numeric_limits\u0026lt;int\u0026gt;::min(); // -2147483648 "},{"id":3,"href":"/tech-book/docs/algorithms/depth-first-search/","title":"Depth First Search","section":"Algorithms","content":" Depth First Search # Intro # The pre-order traversal of a tree is DFS.\nNode\u0026lt;int\u0026gt;* dfs(Node\u0026lt;int\u0026gt;* root, int target) { if (root == nullptr) return nullptr; if (root-\u0026gt;val == target) return root; // return non-null return value from the recursive calls Node\u0026lt;int\u0026gt;* left = dfs(root-\u0026gt;left, target); if (left != nullptr) return left; // at this point, we know left is null, and right could be null or non-null // we return right child\u0026#39;s recursive call result directly because // - if it\u0026#39;s non-null we should return it // - if it\u0026#39;s null, then both left and right are null, we want to return null return dfs(root-\u0026gt;right, target); } Max depth of a binary tree # Max depth of a binary tree is the longest root-to-leaf path. Given a binary tree, find its max depth. Here, we define the length of the path to be the number of edges on that path, not the number of nodes.\nint dfs(Node\u0026lt;int\u0026gt;* root) { // Null node adds no depth if (root == nullptr) return 0; // num nodes in longest path of current subtree = max num nodes of its two subtrees + 1 current node return std::max(dfs(root-\u0026gt;left), dfs(root-\u0026gt;right)) + 1; } int tree_max_depth(Node\u0026lt;int\u0026gt;* root) { return root? dfs(root) - 1 : 0; } Visible Tree Node | Number of Visible Nodes # In a binary tree, a node is labeled as \u0026ldquo;visible\u0026rdquo; if, on the path from the root to that node, there isn\u0026rsquo;t any node with a value higher than this node\u0026rsquo;s value.\nThe root is always \u0026ldquo;visible\u0026rdquo; since there are no other nodes between the root and itself. Given a binary tree, count the number of \u0026ldquo;visible\u0026rdquo; nodes.\nint dfs(Node\u0026lt;int\u0026gt;* root, int max_sofar) { if (!root) return 0; int total = 0; if (root-\u0026gt;val \u0026gt;= max_sofar) total++; total += dfs(root-\u0026gt;left, std::max(max_sofar, root-\u0026gt;val)); total += dfs(root-\u0026gt;right, std::max(max_sofar, root-\u0026gt;val)); return total; } int visible_tree_node(Node\u0026lt;int\u0026gt;* root) { // start max_sofar with smallest number possible so any value root has is greater than it return dfs(root, std::numeric_limits\u0026lt;int\u0026gt;::min()); } "},{"id":4,"href":"/tech-book/docs/algorithms/easy/","title":"Easy Complexity","section":"Algorithms","content":" Easy Complexity # "},{"id":5,"href":"/tech-book/docs/algorithms/priority-queue-and-heap/","title":"Priority Queue and Heap","section":"Algorithms","content":" Priority Queue and Heap # Priority Queue is an Abstract Data Type, and Heap is the concrete data structure we use to implement a priority queue.\nPriority Queue # A priority queue is a data structure that consists of a collection of items and supports the following operations:\ninsert: insert an item with a key. delete_min/delete_max: remove the item with the smallest/largest key and return it. Note that\nwe only allow getting and deleting the element with the min/max key and NOT any arbitrary key. Implement Priority Queue using an array # To do this, we could try using\nan unsorted array, insert would be O(1) as we just have to put it at the end, but finding and deleting min value would be O(N) since we have to loop through the entire array to find it a sorted array, finding min value would be easy O(1), but it would be O(N) to insert since we have to loop through to find the correct position of the value and move elements after the position to make space and insert into the space There must be a better way! \u0026ldquo;Patience you must have, my young padawan.\u0026rdquo; Enter Heap.\nHeap # Heaps are special tree based data structures. Usually when we say heap, we refer to the binary heap that uses the binary tree structure. However, the tree isn\u0026rsquo;t necessarily always binary, in particular, a k-ary heap (A.K.A. k-heap) is a tree where the nodes have k children. As long as the nodes follow the 2 heap properties, it is a valid heap.\nMax Heap and Min Heap # There are two kinds of heaps - Min Heap and Max Heap. A Min Heap is a tree that has two properties:\nalmost complete, i.e. every level is filled except possibly the last(deepest) level. The filled items in the last level are left-justified. for any node, its key (priority) is greater than its parent\u0026rsquo;s key (Min Heap). A Max Heap has the same property #1 and opposite property #2, i.e. for any node, its key is less than its parent\u0026rsquo;s key.\nOperations # insert # To insert a key into a heap,\nplace the new key at the first free leaf if property #2 is violated, perform a bubble-up def bubble_up(node): while node.parent exist and node.parent.key \u0026gt; node.key: swap node and node.parent node = node.parent As the name of the algorithm suggests, it \u0026ldquo;bubbles up\u0026rdquo; the new node by swapping it with its parent until the order is correct\nSince the height of a heap is O(log(N)), the complexity of bubble-up is O(log(N)).\ndelete_min # What this operation does is:\ndelete a node with min key and return it reorganize the heap so the two properties still hold To do that, we:\nremove and return the root since the node with the minimum key is always at the root replace the root with the last node (the rightmost node at the bottom) of the heap if property #2 is violated, perform a bubble-down def bubble_down(node): while node is not a leaf: smallest_child = child of node with smallest key if smallest_child \u0026lt; node: swap node and smallest_child node = smallest_child else: break Implementing Heap # Being a complete tree makes an array a natural choice to implement a heap since it can be stored compactly and no space is wasted. Pointers are not needed. The parent and children of each node can be calculated with index arithmetic\nFor node i, its children are stored at 2i+1 and 2i+2, and its parent is at floor((i-1)/2). So instead of node.left we\u0026rsquo;d do 2*i+1. (Note that if we are implementing a k-ary heap, then the childrens are at ki+1 to ki+k, and its parent is at floor((i-1)/k).)\n"},{"id":6,"href":"/tech-book/docs/algorithms/two-pointers/","title":"Two Pointers \u0026 Sliding Window","section":"Algorithms","content":" Two Pointers # Valid Palindrome # Determine whether a string is a palindrome, ignoring non-alphanumeric characters and case. Examples:\nInput: Do geese see God? Output: True\nInput: Was it a car or a cat I saw? Output: True\nInput: A brown fox jumping over Output: False\n#include \u0026lt;cctype\u0026gt; // isalnum, tolower #include \u0026lt;iostream\u0026gt; // boolalpha, cin, cout #include \u0026lt;string\u0026gt; // getline bool is_palindrome(std::string s) { int l = 0, r = s.size() - 1; while (l \u0026lt; r) { // Note 1, 2 while (l \u0026lt; r \u0026amp;\u0026amp; !std::isalnum(s[l])) { l++; } while (l \u0026lt; r \u0026amp;\u0026amp; !std::isalnum(s[r])) { r--; } // compare characters ignoring case if (std::tolower(s[l]) != std::tolower(s[r])) return false; l++; r--; } return true; } int main() { std::string s; std::getline(std::cin, s); bool res = is_palindrome(s); std::cout \u0026lt;\u0026lt; std::boolalpha \u0026lt;\u0026lt; res \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; } Remove Duplicates # Given a sorted list of numbers, remove duplicates and return the new length. You must do this in-place and without using extra memory.\nInput: [0, 0, 1, 1, 1, 2, 2].\nOutput: 3.\nYour function should modify the list in place so the first 3 elements becomes 0, 1, 2. Return 3 because the new length is 3.\n#include \u0026lt;algorithm\u0026gt; // copy #include \u0026lt;iostream\u0026gt; // boolalpha, cin, cout #include \u0026lt;iterator\u0026gt; // back_inserter, istream_iterator, ostream_iterator, prev #include \u0026lt;sstream\u0026gt; // istringstream #include \u0026lt;string\u0026gt; // getline, string #include \u0026lt;vector\u0026gt; // vector int remove_duplicates(std::vector\u0026lt;int\u0026gt;\u0026amp; arr) { int slow = 0; for (int fast = 0; fast \u0026lt; arr.size(); fast++) { if (arr.at(fast) != arr.at(slow)) { slow++; arr.at(slow) = arr.at(fast); } } return slow + 1; } template\u0026lt;typename T\u0026gt; std::vector\u0026lt;T\u0026gt; get_words() { std::string line; std::getline(std::cin, line); std::istringstream ss{line}; ss \u0026gt;\u0026gt; std::boolalpha; std::vector\u0026lt;T\u0026gt; v; std::copy(std::istream_iterator\u0026lt;T\u0026gt;{ss}, std::istream_iterator\u0026lt;T\u0026gt;{}, std::back_inserter(v)); return v; } template\u0026lt;typename T\u0026gt; void put_words(const std::vector\u0026lt;T\u0026gt;\u0026amp; v) { if (!v.empty()) { std::copy(v.begin(), std::prev(v.end()), std::ostream_iterator\u0026lt;T\u0026gt;{std::cout, \u0026#34; \u0026#34;}); std::cout \u0026lt;\u0026lt; v.back(); } std::cout \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; } int main() { std::vector\u0026lt;int\u0026gt; arr = get_words\u0026lt;int\u0026gt;(); int res = remove_duplicates(arr); arr.resize(res); put_words(arr); } Sliding Window # Sliding window problems is a variant of the same direction two pointers problems. The function performs on the entire interval between the two pointers instead of only at the two positions. Usually, we keep track of the overall result of the window, and when we \u0026ldquo;slide\u0026rdquo; the window (insert/remove an item), we simply manipulate the result to accomodate the changes to the window. Time complexity wise, this is much more efficient as we do not recalculate the overlapping intervals between two windows over and over again. We try to reduce a nested loop into two passes on the input (one pass with each pointer).\nFixed Size Sliding Window # Given an array (list) nums consisted of only non-negative integers, find the largest sum among all subarrays of length k in nums. For example, if the input is nums = [1, 2, 3, 7, 4, 1], k = 3, then the output would be 14 as the largest length 3 subarray sum is given by [3, 7, 4] which sums to 14.\n#include \u0026lt;algorithm\u0026gt; // copy #include \u0026lt;iostream\u0026gt; // boolalpha, cin, cout, streamsize #include \u0026lt;iterator\u0026gt; // back_inserter, istream_iterator #include \u0026lt;limits\u0026gt; // numeric_limits #include \u0026lt;sstream\u0026gt; // istringstream #include \u0026lt;string\u0026gt; // getline, string #include \u0026lt;vector\u0026gt; // vector int subarray_sum_fixed(std::vector\u0026lt;int\u0026gt; nums, int k) { int window_sum = 0; for (int i = 0; i \u0026lt; k; ++i) { window_sum = window_sum + nums[i]; } int largest = window_sum; for (int right = k; right \u0026lt; nums.size(); ++right) { int left = right - k; window_sum = window_sum - nums[left]; window_sum = window_sum + nums[right]; largest = std::max(largest, window_sum); } return largest; } template\u0026lt;typename T\u0026gt; std::vector\u0026lt;T\u0026gt; get_words() { std::string line; std::getline(std::cin, line); std::istringstream ss{line}; ss \u0026gt;\u0026gt; std::boolalpha; std::vector\u0026lt;T\u0026gt; v; std::copy(std::istream_iterator\u0026lt;T\u0026gt;{ss}, std::istream_iterator\u0026lt;T\u0026gt;{}, std::back_inserter(v)); return v; } void ignore_line() { std::cin.ignore(std::numeric_limits\u0026lt;std::streamsize\u0026gt;::max(), \u0026#39;\\n\u0026#39;); } int main() { std::vector\u0026lt;int\u0026gt; nums = get_words\u0026lt;int\u0026gt;(); int k; std::cin \u0026gt;\u0026gt; k; ignore_line(); int res = subarray_sum_fixed(nums, k); std::cout \u0026lt;\u0026lt; res \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; } Flexible Size Sliding Window - Longest # Recall finding the largest size k subarray sum of an integer array in Largest Subarray Sum. What if we dont need the largest sum among all subarrays of fixed size k, but instead, we want to find the length of the longest subarray with sum smaller than or equal to a target?\nGiven input nums = [1, 6, 3, 1, 2, 4, 5] and target = 10, then the longest subarray that does not exceed 10 is [3, 1, 2, 4], so the output is 4 (length of [3, 1, 2, 4]).\n#include \u0026lt;algorithm\u0026gt; // copy #include \u0026lt;iostream\u0026gt; // boolalpha, cin, cout, streamsize #include \u0026lt;iterator\u0026gt; // back_inserter, istream_iterator #include \u0026lt;limits\u0026gt; // numeric_limits #include \u0026lt;sstream\u0026gt; // istringstream #include \u0026lt;string\u0026gt; // getline, string #include \u0026lt;vector\u0026gt; // vector int subarray_sum_longest(std::vector\u0026lt;int\u0026gt; nums, int target) { int windowSum = 0, length = 0; int left = 0; for (int right = 0; right \u0026lt; nums.size(); ++right) { windowSum = windowSum + nums[right]; while (windowSum \u0026gt; target) { windowSum = windowSum - nums[left]; ++left; } length = std::max(length, right-left+1); } return length; } template\u0026lt;typename T\u0026gt; std::vector\u0026lt;T\u0026gt; get_words() { std::string line; std::getline(std::cin, line); std::istringstream ss{line}; ss \u0026gt;\u0026gt; std::boolalpha; std::vector\u0026lt;T\u0026gt; v; std::copy(std::istream_iterator\u0026lt;T\u0026gt;{ss}, std::istream_iterator\u0026lt;T\u0026gt;{}, std::back_inserter(v)); return v; } void ignore_line() { std::cin.ignore(std::numeric_limits\u0026lt;std::streamsize\u0026gt;::max(), \u0026#39;\\n\u0026#39;); } int main() { std::vector\u0026lt;int\u0026gt; nums = get_words\u0026lt;int\u0026gt;(); int target; std::cin \u0026gt;\u0026gt; target; ignore_line(); int res = subarray_sum_longest(nums, target); std::cout \u0026lt;\u0026lt; res \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; } Flexible Size Sliding Window - Shortest # Let\u0026rsquo;s continue on finding the sum of subarrays. This time given a positive integer array nums, we want to find the length of the shortest subarray such that the subarray sum is at least target. Recall the same example with input nums = [1, 4, 1, 7, 3, 0, 2, 5] and target = 10, then the smallest window with the sum \u0026gt;= 10 is [7, 3] with length 2. So the output is 2.\nWe\u0026rsquo;ll assume for this problem that it\u0026rsquo;s guaranteed target will not exceed the sum of all elements in nums.\n#include \u0026lt;algorithm\u0026gt; // copy #include \u0026lt;iostream\u0026gt; // boolalpha, cin, cout, streamsize #include \u0026lt;iterator\u0026gt; // back_inserter, istream_iterator #include \u0026lt;limits\u0026gt; // numeric_limits #include \u0026lt;sstream\u0026gt; // istringstream #include \u0026lt;string\u0026gt; // getline, string #include \u0026lt;vector\u0026gt; // vector int subarray_sum_shortest(std::vector\u0026lt;int\u0026gt; nums, int target) { int windowSum = 0, length = nums.size(); int left = 0; for (int right = 0; right \u0026lt; nums.size(); ++right) { windowSum = windowSum + nums[right]; while (windowSum \u0026gt;= target) { length = std::min(length, right-left+1); windowSum = windowSum - nums[left]; ++left; } } return length; } template\u0026lt;typename T\u0026gt; std::vector\u0026lt;T\u0026gt; get_words() { std::string line; std::getline(std::cin, line); std::istringstream ss{line}; ss \u0026gt;\u0026gt; std::boolalpha; std::vector\u0026lt;T\u0026gt; v; std::copy(std::istream_iterator\u0026lt;T\u0026gt;{ss}, std::istream_iterator\u0026lt;T\u0026gt;{}, std::back_inserter(v)); return v; } void ignore_line() { std::cin.ignore(std::numeric_limits\u0026lt;std::streamsize\u0026gt;::max(), \u0026#39;\\n\u0026#39;); } int main() { std::vector\u0026lt;int\u0026gt; nums = get_words\u0026lt;int\u0026gt;(); int target; std::cin \u0026gt;\u0026gt; target; ignore_line(); int res = subarray_sum_shortest(nums, target); std::cout \u0026lt;\u0026lt; res \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; } Linked List Cycle # Given a linked list with potentially a loop, determine whether the linked list from the first node contains a cycle in it. For bonus points, do this with constant space.\nbool has_cycle(Node\u0026lt;int\u0026gt;* nodes) { Node\u0026lt;int\u0026gt;* tortoise = next_node(nodes); Node\u0026lt;int\u0026gt;* hare = next_node(next_node(nodes)); while (tortoise != hare \u0026amp;\u0026amp; hare-\u0026gt;next != NULL) { tortoise = next_node(tortoise); hare = next_node(next_node(hare)); } return hare-\u0026gt;next != NULL; } "},{"id":7,"href":"/tech-book/docs/ai-ml-dc/1-ai-ml-networking/","title":"AI/ML Networking","section":"Data Center Networking for AI Clusters","content":" Intro # AI/ML Networking Part I: RDMA Basics # TBD\nAI/ML Networking: Part-II: Introduction of Deep Neural Networks # Machine Learning (ML) is a subset of Artificial Intelligence (AI). ML is based on algorithms that allow learning, predicting, and making decisions based on data rather than pre-programmed tasks. ML leverages Deep Neural Networks (DNNs), which have multiple layers, each consisting of neurons that process information from sub-layers as part of the training process. Large Language Models (LLMs), such as OpenAI’s GPT (Generative Pre-trained Transformers), utilize ML and Deep Neural Networks.\nFor network engineers, it is crucial to understand the fundamental operations and communication models used in ML training processes. To emphasize the importance of this, I quote the Chinese philosopher and strategist Sun Tzu, who lived around 600 BCE, from his work The Art of War.\nIf you know the enemy and know yourself, you need not fear the result of a hundred battles.\nWe don’t have to be data scientists to design a network for AI/ML, but we must understand the operational fundamentals and communication patterns of ML. Additionally, we must have a deep understanding of network solutions and technologies to build a lossless and cost-effective network for enabling efficient training processes.\nIn the upcoming two posts, I will explain the basics of: a) Data Models: Layers and neurons, forward and backward passes, and algorithms. b) Parallelization Strategies: How training times can be reduced by dividing the model into smaller entities, batches, and even micro-batches, which are processed by several GPUs simultaneously.\nThe number of parameters, the selected data model, and the parallelization strategy affect the network traffic that crosses the data center switch fabric.\nAfter these two posts, we will be ready to jump into the network part. At this stage, you may need to read (or re-read) my previous post about Remote Direct Memory Access (RDMA), a solution that enables GPUs to write data from local memory to remote GPUs\u0026rsquo; memory. AI/ML Networking: Part-III: Basics of Neural Networks Training Process # Neural Network Architecture Overview # Deep Neural Networks (DNN) leverage various architectures for training, with one of the simplest and most fundamental being the Feedforward Neural Network (FNN). Figure 2-1 illustrates our simple, three-layer FNN.\nInput Layer: # The first layer doesn’t have neurons, instead the input data parameters X1, X2, and X3 are in this layer, from where they are fed to first hidden layer. Hidden Layer: # The neurons in the hidden layer calculate a weighted sum of the input data, which is then passed through an activation function. In our example, we are using the Rectified Linear Unit (ReLU) activation function. These calculations produce activation values for neurons. The activation value is modified input data value received from the input layer and published to upper layer.\nOutput Layer: # Neurons in this layer calculate the weighted sum in the same manner as neurons in the hidden layer, but the result of the activation function is the final output.\nThe process described above is known as the Forwarding pass operation. Once the forward pass process is completed, the result is passed through a loss function, where the received value is compared to the expected value. The difference between these two values triggers the backpropagation process. The Loss calculation is the initial phase of Backpropagation process. During backpropagation, the network fine-tunes the weight values , neuron by neuron, from the output layer through the hidden layers. The neurons in the input layer do not participate in the backpropagation process because they do not have weight values to be adjusted.\nAfter the backpropagation process, a new iteration of the forward pass begins from the first hidden layer. This loop continues until the received and expected values are close enough to expected value, indicating that the training is complete. Figure 2-1: Deep Neural Network Basic Structure and Operations.\nForwarding Pass # Next, let\u0026rsquo;s examine the operation of a Neural Network in more detail. Figure 2-2 illustrates a simple, three-layer Feedforward Neural Network (FNN) data model. The input layer has two neurons, H1 and H2, each receiving one input data value: a value of one (1) is fed to neuron H1 by input neuron X1, and a value of zero (0) is fed to neuron H2 by input neuron X2. The neurons in the input layer do not calculate a weighted sum or an activation value but instead pass the data to the next layer, which is the first hidden layer.\nThe hidden layer in our example consists of two neurons. These neurons use the ReLU activation function to calculate the activation value. During the initialization phase, the weight values for these neurons are assigned using the He Initialization method, which is often used with the ReLU function. The He Initialization method calculates the variance as 2/n where n is the number of neurons in the previous layer. In this example, with two input neurons, this gives a variance of 1 (=2/2). The weights are then drawn from a normal distribution ~N(0,√variance), which in this case is ~N(0,1). Basically, this means that the randomly generated weight values are centered around zero with a standard deviation of one.\nIn Figure 2-2, the weight value for neuron H3 in the hidden layer is 0.5 for both input sources X1 (input data 1) and X2 (input data 0). Similarly, for the hidden layer neuron H4, the weight value is 1 for both input sources X1 (input data 1) and X2 (input data 0). Neurons in the hidden and output layers also have a bias variable. If the input to a neuron is zero, the output would also be zero if there were no bias. The bias ensures that a neuron can still produce a meaningful output even when the input is zero (i.e., the neuron is inactive). Neurons H3 and O5 have a bias value of 0.5, while neuron H4 has a bias value of 0 (I am using zero for simplify the calculation). Let’s start the forward pass process from neuron H3 in the hidden layer. First, we calculate the weighted sum using the formula below, where Z3 represents the weighted sum of input. Here, Xn is the actual input data value received from the input layer’s neuron, and Wn is the weight associated with that particular input neuron.\nThe weighted sum calculation (Z3) for neuron H3:\nZ3 = (X1 ⋅ W31) + (X2 ⋅ W32) + b3 Given: Z3 = (1 ⋅ 0.5) + (0 ⋅ 0.5) + 0 Z3 = 0.5 + 0 + 0 Z3 = 0.5 To get the activation value a3 (shown as H3=0.5 in figure), we apply the ReLU function. The ReLU function outputs zero (0) if the calculated weighted sum Z is less than or equal to zero; otherwise, it outputs the value of the weighted sum Z.\nThe activation value a3 for H3 is:\nReLU (Z3) = ReLU (0.5) = 0.5\nThe weighted sum calculation for neuron H4:\nZ4 = (X1 ⋅ W41) + (X2 ⋅ W42) + b4 Given: Z4 = (1 ⋅ 1) + (0 ⋅1) + 0.5 Z4 = 1 + 0 + 0.5 Z4 = 1.5 The activation value using ReLU for Z4 is:\nReLU (Z4) = ReLU (1.5) = 1.5\nFigure 2-2: Forwarding Pass on Hidden Layer.\nAfter neurons H3 and H4 publish their activation values to neuron O5 in the output layer, O5 calculates the weighted sum Z5 for inputs with weights W53=1and W54=1. Using Z5, it calculates the output using the ReLU function. The difference between the received output value (Yr) and the expected value (Ye) triggers a backpropagation process. In our example, Yr−Ye=0.5.\nBackpropagation process # The loss function measures the difference between the predicted output and the actual expected output. The loss function value indicates how well the neural network is performing. A high loss value means the network\u0026rsquo;s predictions are far from the actual values, while a low loss value means the predictions are close.\nAfter calculating the loss, backpropagation is initiated to minimize this loss. Backpropagation involves calculating the gradient of the loss function with respect to each weight and bias in the network. This step is crucial for adjusting the weights and biases to reduce the loss in subsequent forwarding pass iterations.\nLoss function is calculated using the formula below:\nLoss (L) = (H3 x W53 + H4 x W54 + b5 – Ye)2 Given: L = (0.5 x 1 + 1.5 x 1 + 0.5 - 2)2 L = (0.5 + 1.5 + 0.5 - 2)2 L = 0.52 L= 0.25 Figure 2-3: Forwarding Pass on Output Layer.\nThe result of the loss function is then fed into the gradient calculation process, where we compute the gradient of the loss function with respect to each weight and bias in the network. The gradient calculation result is then used to fine-tune the old weight values. The Eta hyper-parameter η (the learning rate) controls the step size during weight updates in the backpropagation process, balancing the speed of convergence with the stability of training. In our example, we are using a learning rate of 1/100 = 0.01. The term hyper-parameters refers to parameters that affect the final result.\nFirst, we compute the partial derivative of the loss function (gradient calculation) with respect to the old weight values. The following example shows the gradient calculation for weight W53. The same computation applies to W54 and b3.\nGradient Calculation:\n∂L = 2W53 x (Yr – Ye) ∂W53 Given = 2 x 0.5 x (2.5 - 2) = 1 x 0.5 = 0.5 New weight value calculation.\nW53 (new) = W53(old) – η x ∂L/∂W53 Given: W53 (new) = 1–0.01 x 0.5 W53 (new) = 0.995 Figure 2-4: Backpropagation - Gradient Calculation and New Weight Value Computation.\nFigure 2-5 shows the formulas for calculating the new bias b3. The process is the same than what was used with updating the weight values.\nFigure 2-5: Backpropagation - Gradient Calculation and New Bias Computation.\nAfter updating the weights and biases, the backpropagation process moves to the hidden layer. Gradient computation in the hidden layer is more complex because the loss function only includes weights from the output layer as you can see from the Loss function formula below:\nLoss (L) = (H3 x W53 + H4 x W54 + b5 – Ye)2\nThe formula for computing the weights and biases for neurons in the hidden layers uses the chain rule. The mathematical formula shown below, but the actual computation is beyond the scope of this chapter.\n∂L = ∂L x ∂H3 ∂W31 ∂H3 ∂W31 After the backpropagation process is completed, the next iteration of the forward pass starts. This loop continues until the received result is close enough to the expected result.\nIf the size of the input data exceeds the GPU’s memory capacity or if the computing power of one GPU is insufficient for the data model, we need to decide on a parallelization strategy. This strategy defines how the training workload is distributed across several GPUs. Parallelization impacts network load if we need more GPUs than are available on one server. Dividing the workload among GPUs within a single GPU-server or between multiple GPU-servers triggers synchronization of calculated gradients between GPUs. When the gradient is calculated, the GPUs synchronize the results and compute the average gradient, which is then used to update the weight values.\nThe upcoming chapter introduces pipeline parallelization and synchronization processes in detail. We will also discuss why lossless connection is required for AI/ML.\nAI/ML Networking: Part-IV: Convolutional Neural Network (CNN) Introduction # Feed-forward Neural Networks are suitable for simple tasks like basic time series prediction without long-term relationships. However, FNNs is not a one-size-fits-all solution. For instance, digital image training process uses pixel values of image as input data. Consider training a model to recognize a high resolution (600 dpi), 3.937 x 3.937 inches digital RGB (red, green, blue) image. The number of input parameters can be calculated as follows:\nWidth: 3.937 in x 600 ≈ 2362 pixels\nHeight: 3.937 in x 600 ≈ 2362 pixels\nPixels in image: 2362 x 2362 = 5,579,044 pixels\nRGB (3 channels): 5,579,044 pxls x 3 channels = 16 737 132\nTotal input parameters: 16 737 132\nMemory consumption: ≈ 16 MB\nFNNs are not ideal for digital image training. If we use FNN for training in our example, we fed 16,737,132 input parameters to the first hidden layer, each having unique weight. For image training, there might be thousands of images, handling millions of parameters demands significant computation cycles and is a memory-intensive process. Besides, FNNs treat each pixel as an independent unit. Therefore, FNN algorithm does not understand dependencies between pixels and cannot recognize the same image if it shifts within the frame. Besides, FNN does not detect edges and other crucial details. A better model for training digital images is Convolutional Neural Networks (CNNs). Unlike in FFN neural networks where each neuron has a unique set of weights, CNNs use the same set of weights (Kernel/Filter) across different regions of the image, which reduces the number of parameters. Besides, CNN algorithm understands the pixel dependencies and can recognize patterns and objects regardless of their position in the image. The input data processing in CNNs is hierarchical. The first layer, convolutional layers, focuses on low-level features such as textures and edges. The second layer, pooling layer, captures higher-level features like shapes and objects. These two layers significantly reduce the input data parameters before they are fed into the neurons in the first hidden layer, the fully connected layer, where each neuron has unique weights (like FNNs).\nConvolution Layer # The convolution process uses a shared kernel (also known as filters), which functions similarly to a neuron in a feed-forward neural network. The kernel\u0026rsquo;s coverage area, 3x3 in our example, defines how many pixels of an input image is covered at a given stride. The kernel assigns a unique weight (w) to each covered pixel (x) in a one-to-one mapping fashion and calculates the weighted sum (z) from the input data. For instance, in figure 3-1 the value of the pixel W1 is multiplied with the weight value W1 (X1W1), and pixel X2 is multiplies with weight value W2 (X2W2) and so on. Then the results are summed, which returns a weighted sum Z. The result Z is then passed through the ReLU function, which defines the value of the new pixel (P1). This new pixel is then placed into a new image matrix.\nFigure 3-1: CNN Overview – Convolution, Initial State (Stride 0).\nAfter calculating the new pixel value for the initial coverage area with stride zero, the kernel shifts to the right by the number of steps defined in the kernel\u0026rsquo;s stride value. In our example, the stride is set to one, and the kernel moves one step to the right, covering pixels shown in Figure 3-2. Then the weighted sum is (z) calculated and run through the ReLU function, and the second pixel is added to the new matrix. Since there are no more pixels to the right, the kernel moves down by one step and returns to the first column (Figure 3-3).\nFigure 3-2: CNN Overview – Convolution, Second State (Stride 1).\nFigures 3-3 and 3-4 shows the last two steps of the convolution. Notice that we have used the same weight values in each iteration. In the initial state weight w1 was associated with the first pixel (X1W1), and in the second phase with the second pixel (X2W1) and so on. The new image matrix produced by the convolutional process have decreased the 75% from the original digital image. Figure 3-3: CNN Overview – Convolution, Third State (Stride 2).\nFigure 3-4: CNN Overview – Convolution, Fourth State (Stride 3).\nIf we don\u0026rsquo;t want to decrease the size of the new matrix, we must use padding. Padding adds pixels to the edges of the image. For example, a padding value of one (1) adds one pixel to the image edges, providing a new matrix that is the same size as the original image.\nFigure 3-5: CNN Overview – Padding.\nFigure 3-6 illustrates the progression of the convolution layer from the initial state (which I refer to as stride 0) to the final phase, stride 3. The kernel covers a 3x3 pixel area in each phase and moves with a stride of 1. I use the notation SnXn to denote the stride and the specific pixel. For example, in the initial state, the first pixel covered by the kernel is labeled as S0X1, and the last pixel is labeled as S0X11.\nWhen the kernel shifts to the left, covering the next region, the first pixel is marked as S1X2 and the last as S1X12. The same notation is applied to the weighted sum calculation. The weighted value for the first pixel is represented as (S0X1) · W1 = Z1 and for the last one as (S0X11) · W9 = Z9. The weighted input for all pixel values is then summed, and a bias is added to obtain the weighted sum for the given stride.\nIn the initial state, this calculation results in = Z0, which is passed through the ReLU function. The output of this function provides the value for the first pixel in the new image matrix.\nFigure 3-6: CNN Overview – Convolution Summary.\nConvolution Layer Example # In Figure 3-7, we have a simple 12x12 = 144 pixels grayscale image representing the letter \u0026ldquo;H.\u0026rdquo; In the image, the white pixels have a binary value of 255, the gray pixels have a binary value of 87, and the darkest pixels have a binary value of 2. Our kernel size is 4x4, covering 16 pixels in each stride. Because the image is grayscale, we only have one channel. The kernel uses the ReLU activation function to determine the value for the new pixel. Initially, at stride 0, the kernel is placed over the first region in the image. The kernel has a unique weight value for each pixel it covers, and it calculates the weighted sum for all 16 pixels. The value of the first pixel (X1 = 87) is multiplied by its associated kernel weight (W1 = 0.12), which gives us a new value of 10.4. This computation runs over all 16 pixels covered by the kernel (results shown in Figure 3-7). The new values are then summed, and a bias is added, giving a weighted sum of Z0 = 91.4. Because the value of Z0 is positive, the ReLU function returns an activation value of 91.4 (if Z \u0026gt; 0, Z = Z; otherwise, Z = 0). The activation value of 91.4 becomes the value of our new pixel in the new image matrix.\nFigure 3-7: Convolution Layer Operation Example – Stride 0 (Initial State).\nNext, the kernel shifts one step to the right (Stride 1) and multiplies the pixel values by the associated weights. The changing parameters are the values of the pixels, while the kernel weight values remain the same. After the multiplication process is done and the weighted sum is calculated, the result is run through the ReLU function. At this stride, the result of the weighted sum (Z1) is negative, so the ReLU function returns zero (0). This value is then added to the matrix. At this phase, we have two new pixels in the matrix.\nFigure 3-8: Convolution Layer Operation Example – Stride 1.\nThe next four figures 3-9, 3-10, and 3-11 illustrates how the kernel is shifted over the input digital image and producing a new 9x9 image matrix.\nFigure 3-9: Convolution Layer Operation Example – Stride 2.\nFigure 3-10: Convolution Layer Operation Example – Stride 8.\nFigure 3-11: Convolution Layer Operation Example – Stride 10.\nFigure 3-12 illustrates the completed convolutional layer computation. At this stage, the number of pixels in the original input image has decreased from 144 to 81, representing a reduction of 56.25%.\nFigure 3-12: Convolution Layer Operation Example – The Last Stride. Pooling Layer # After the original image is processed by the convolution layer, the resulting output is used as input data for the next layer, the pooling layer. The pooling layer performs a simpler operation than the convolution layer. Like the convolution layer, the pooling layer uses a kernel to generate new values. However, instead of applying a convolution operation, the pooling layer selects the highest value within the kernel (if MaxPooling is applied) or computes the average of the values covered by the kernel (Average Pooling).\nIn this example, we use MaxPooling with a kernel size of 2x2 and a stride of 2. The first pixel is selected from values 91, 0, 112, and 12, corresponding to pixels in positions 1, 2, 10, and 11, respectively. Since the pixel at position 10 has the highest value (112), it is selected for the new matrix.\nFigure 3-13: Pooling Layer – Stride 0 (Initial Phase).\nAfter selecting the highest value from the initial phase, the kernel moves to the next region, covering the values 252, 153, 212, and 52. The highest value, 252, is then placed into the new matrix. Figure 3-14: Pooling Layer – Stride 1.\nFigure 3-15 illustrates how MaxPooling progresses to the third region, covering the values 141, 76, 82, and 35. The highest value, 141, is then placed into the matrix.\nFigure 3-15: Pooling Layer – Stride 2.\nFigure 3-16 describes how the original 12x12 (144 pixels) image is first processed by the convolution layer, reducing it to a 9x9 (81 pixels) matrix, and then by the pooling layer, further reducing it to a 5x5 (25 pixels) matrix.\nFigure 3-16: CNN Convolution and Pooling Layer Results.\nAs the final step, the matrix generated by the pooling layer is flattened and used as input for the neurons in the fully connected layer. This means we have 25 input neurons, each with a unique weight assigned to every input parameter. The neurons in the input layer calculate the weighted sum, which neurons then use to determine the activation value. This activation value serves as the input data for the output layer. The neurons in the output layer produce the result based on their calculations, with the output H proposed by three output neurons.\nFigure 3-17: The Model of Convolution Neural Network (CNN).\nReferences:\nhttps://nwktimes.blogspot.com/2024/06/aiml-networking-part-i-rdma-basics.html https://nwktimes.blogspot.com/2024/07/aiml-networking-part-ii-introduction-of.html https://nwktimes.blogspot.com/2024/07/aiml-networking-part-iii-basics-of.html https://nwktimes.blogspot.com/2024/08/aiml-networking-part-iv-convolutional.html "},{"id":8,"href":"/tech-book/docs/5g/5g-intro/","title":"An Overview of 5G Networking","section":"5G","content":"from https://datatracker.ietf.org/doc/draft-ietf-teas-5g-ns-ip-mpls/\nAppendix B. An Overview of 5G Networking # This section provides a brief introduction to 5G mobile networking with a perspective on the Transport Network. This section does not intend to replace or define 3GPP architecture, instead its objective is to provide an overview for readers that do not have a mobile background. For more comprehensive information, refer to [TS-23.501].\nB.1. Key Building Blocks # [TS-23.501] defines the Network Functions (UPF, Access and Mobility Function (AMF), etc.) that compose the 5G System (5GS) Architecture together with related interfaces (e.g., N1 and N2). This architecture has built-in control and user plane separation, and the control plane leverages a Service- Based Architecture (SBA). Figure 33 outlines an example 5GS architecture with a subset of possible NFs and network interfaces.\n+-----+ +-----+ +-----+ +-----+ +-----+ +-----+ |NSSF | | NEF | | NRF | | PCF | | UDM | | AF | +--+--+ +--+--+ +--+--+ +--+--+ +--+--+ +--+--+ Nnssf| Nnef| Nnrf| Npcf| Nudm| |Naf ---+--------+--+-----+----------+---+----+--------+---- Nausf| Namf| Nsmf| +--+--+ +--+--+ +--+------+ |AUSR | | AMF | | SMF | +-----+ +--+--+ +--+------+ / | | \\\\ Control Plane N1 / |N2 |N4 \\\\N4 ------------------------------------------------------------ User Plane / | | \\\\ +---+ +--+--+ N3 +--+--+ N9 +-----+ N6 .---. |UE +--+(R)AN+-----+ UPF +----+ UPF +----( DN ) +---+ +-----+ +-----+ +-----+ \u0026#39;---\u0026#39; Figure 33: 5GS Architecture and Service-based Interfaces\nSimilar to previous versions of 3GPP mobile networks [RFC6459], a 5G mobile network is split into the following four major domains (Figure 34):\nUE, MS, MN, and Mobile:\nThe terms User Equipment (UE), Mobile Station (MS), Mobile Node (MN), and mobile refer to the devices that are hosts with the ability to obtain Internet connectivity via a 3GPP network. An MS is comprised of a Terminal Equipment (TE) and a Mobile Terminal (MT).\nRadio Access Network (RAN):\nProvides wireless connectivity to UEs. A RAN is made up of the Antenna that transmits and receives signals to UEs and the Base Station that digitizes the signal and converts the Radio Frequency (RF) data stream to IP packets.\nCore Network (CN):\nControls the CP of the RAN and provides connectivity to the Data Network (e.g., the Internet or a private VPN). The Core Network hosts dozens of services such as authentication, phone registry, charging, access to Public Switched Telephony Network (PSTN) and handover.\nTransport Network (TN):\nProvides connectivity between 5G NFs. The TN may provide connectivity from the RAN to the CN as well as within the RAN or within the CN. The traffic generated by NFs is - mostly - based on IP or Ethernet.\n+----------------------------------------------+ | +------------+ +------------+ | | +----+ | | | | | .-------. | | UE +------+ RAN | | CN +----( DN ) | +----+ | | | | | \u0026#39;-------\u0026#39; | +------+-----+ +------+-----+ | | | | | | +-----+-----------------+----+ | | | Transport Network | | | +----------------------------+ | | | | 5G System | +----------------------------------------------+ Figure 34: Building Blocks of 5G Architecture (A High-Level Representation)\nB.2. Core Network (CN) # The 5G Core Network (5GC) is made up of a set of NFs which fall into two main categories (Figure 35):\n5GC User Plane:\nThe UPF is the interconnect point between the mobile infrastructure and the Data Network (DN). It interfaces with the RAN via the N3 interface by encapsulating/ decapsulating the user plane traffic in GTP tunnels (aka GTP-U or Mobile user plane).\n5GC Control Plane:\nThe 5G control plane is made up of a comprehensive set of NFs. The description of these entities is out of the scope of this document. The following NFs and interfaces are worth mentioning, since their connectivity may rely on the Transport Network:\nthe AMF connects with the RAN control plane over the N2 interface\nthe SMF controls the 5GC UPF via the N4 interface\n+---------+ +-------------------------+ | RAN | | 5G Core (5GC) | | | | | | | | \\[AUSF NRF UDM ...\\] | | | | (SBA) | | | | | | | N2 | +-----+ N11 +-----+ | | CP -----------+ AMF +-----+ SMF | | | | | +-----+ +--+--+ | | | | | | Control Plane ----------------------------------------------------------- | | | | N4 | User Plane | | N3 | +--+--+ | N6 .-------. | UP -----------------------+ UPF +-------\u0026gt;( DN ) | | | +-----+ | \\`-------\u0026#39; +---------+ +-------------------------+ Figure 35: 5G Core Network (CN)\nB.3. Radio Access Network (RAN) # The RAN connects cellular wireless devices to a mobile Core Network. The RAN is made up of three components, which form the Radio Base Station:\nThe Baseband Unit (BBU) provides the interface between the Core Network and the Radio Network. It connects to the Radio Unit and is responsible for the baseband signal processing to packet.\nThe Radio Unit (RU) is located close to the Antenna and controlled by the BBU. It converts the Baseband signal received from the BBU to a Radio frequency signal.\nThe Antenna converts the electric signal received from the RU to radio waves\nThe 5G RAN Base Station is called a gNodeB (gNB). It connects to the Core Network via the N3 (User Plane) and N2 (Control Plane) interfaces.\nThe 5G RAN architecture supports RAN disaggregation in various ways. Notably, the BBU can be split into a DU (Distributed Unit) for digital signal processing and a CU (Centralized Unit) for RAN Layer 3 processing. Furthermore, the CU can be itself split into Control Plane (CU-CP) and User Plane (CU-UP).\nFigure 36 depicts a disaggregated RAN with NFs and interfaces.\n+---------------------------------+ +-----------+ | | N3 | | +----+ NR | +----+ 5G Core | | UE +------+ gNodeB | | | +----+ | +----+ (5GC) | | | N2 | | +---------------------------------+ +-----------+ | | .+ +. \\\\ / \\\\ / +---------------------------------+ +-----------+ | +-------------------+ | | | | | | | | | +----+ NR | +----+ F2 |+----+ F1-U +-----+| | N3 | +-----+ | | UE +--------+ RU +-----+ DU +------+CU-UP+----------+ UPF | | +----+ | +----+ |+-+--+ +--+--+| | | +-----+ | | | | |E1 | | | | | | | F1-C | | | | | | | | +--+--+| | N2 | +-----+ | | | +---------+CU-CP+----------+ AMF | | | | +-----+| | | +-----+ | | | BBU split | | | 5G Core | | +-------------------+ | | | | Disaggregated gNodeB | | | +---------------------------------+ +-----------+ Figure 36: RAN Disaggregation\nB.4. Transport Network (TN) # The 5G transport architecture defines three main segments for the Transport Network, which are commonly referred to as Fronthaul (FH), Midhaul (MH), and Backhaul (BH) [TR-GSTR-TN5G]:\nFronthaul happens before the BBU processing. In 5G, this interface is based on eCPRI with Ethernet or IP encapsulation.\nMidhaul is optional: this segment is introduced in the BBU split presented in Appendix B.3, where Midhaul network refers to the DU- CU interconnection (i.e., F1 interface). At this level, all traffic is encapsulated in IP (signaling and user plane).\nBackhaul happens after BBU processing. Therefore, it maps to the interconnection between the RAN and the CN. All traffic is encapsulated in IP.\nFigure 37 illustrates the different segments of the Transport Network with the relevant NFs.\n+---------------------------------------------------------+ | Transport Network | | | | Fronthaul Midhaul Backhaul | | +-----------+ +------------+ +-----------+ | | | | | | | | | +--|-----------|-|------------|-|-----------|-------------+ +-+--+ +-+-++ +-+-++ +-+---+ .---. | RU | | DU | | CU | | UPF :----( DN ) +----+ +----+ +----+ +-----+ \\`---\u0026#39; Figure 37: 5G Transport Segments\nA given part of the transport network can carry several 5G transport segments concurrently, as outlined in Figure 38. This is because different types of 5G NFs might be placed in the same location (e.g., the UPF from one slice might be placed in the same location as the CU-UP from another slice).¶\n+---------+ |+----+ | Colocated ||RU-1| | RU/DU |+-+--+ | | | FH-1 | |+-+--+ | ||DU-1| | +----+ +-----+ .---. |+-+--+ | |CU-1| |UPF-1+--------( DN ) +--|------+ +-+-++ +-+---+ \\`---\u0026#39; +--|-----------|-|------------|----------------------------+ | | MH-1 | | BH-1 | Transport Network | | +-----------+ +------------+ | | +-----------+ +------------+ +-----------+ | | | FH-2 | | MH-2 | | BH-2 | | +--|-----------|-|------------|-|-----------|--------------+ +-+--+ +-+-++ +-+-++ +-+---+ .---. |RU-2| |DU-2| |CU-2| |UPF-2+----( DN ) +----+ +----+ +----+ +-----+ \\`---\u0026#39; Figure 38: Concurrent 5G Transport Segments\n"},{"id":9,"href":"/tech-book/docs/data-center/data-center-ethernet/","title":"Data Center Ethernet","section":"Data Center Tips","content":" Intro # Residential vs Data Center Ethernet # Link Aggregation Control Protocol (LACP) # Spanning Tree Protocol # Multiple Spanning Tree Protocol # Multiple Spanning Tree Protocol # ISIS Protocol # Shortest Path Bridge (SPB) # VLAN # LLDP # Data Center Bridging # Pause Frame # Priority Based flow control (PFC) # Enhanced Transmission Selection # # Congestion Notification # DCBX # References:\nRaj Jain Data Center Ethernet by Raj Jain Data Center Tutorial "},{"id":10,"href":"/tech-book/docs/data-center/data-center-technologies/","title":"Data Center Technologies","section":"Data Center Tips","content":" Intro # Data Center Network Topologies: 3-Tier # 3-Tier Data Center Networks # 3-Tier Data Center Networks (Cont) # 3-Tier Hierarchical Network Design # Problem with 3-Tier Topology # Clos Networks # Fat-Tree DCN # Fat-Tree Topology (Cont) # Advantages of 2-Tier Architecture # Rack-Scale Architecture # References:\nRaj Jain Data Center Network Topologies by Raj Jain "},{"id":11,"href":"/tech-book/docs/systemdesign-tips/code-deployment-system/","title":"Design A Code-Deployment System","section":"SystemDesign-Tips","content":" 1. Gathering System Requirements # As with any systems design interview question, the first thing that we want to do is to gather system requirements; we need to figure out what system we\u0026rsquo;re building exactly.\nFrom the answers we were given to our clarifying questions (see Prompt Box), we\u0026rsquo;re building a system that involves repeatedly (in the order of thousands of times per day) building and deploying code to hundreds of thousands of machines spread out across 5-10 regions around the world.\nBuilding code will involve grabbing snapshots of source code using commit SHA identifiers; beyond that, we can assume that the actual implementation details of the building action are taken care of. In other words, we don\u0026rsquo;t need to worry about how we would build JavaScript code or C++ code; we just need to design the system that enables the repeated building of code.\nBuilding code will take up to 15 minutes, it\u0026rsquo;ll result in a binary file of up to 10GB, and we want to have the entire deployment process (building and deploying code to our target machines) take at most 30 minutes.\nEach build will need a clear end-state (SUCCESS or FAILURE), and though we care about availability (2 to 3 nines), we don\u0026rsquo;t need to optimize too much on this dimension.\n2. Coming Up With A Plan # It\u0026rsquo;s important to organize ourselves and to lay out a clear plan regarding how we\u0026rsquo;re going to tackle our design. What are the major, distinguishable components of our how system?\nIt seems like this system can actually very simply be divided into two clear subsystems:\nthe Build System that builds code into binaries the Deployment System that deploys binaries to our machines across the world Note that these subsystems will of course have many components themselves, but this is a very straightforward initial way to approach our problem.\n3. Build System \u0026ndash; General Overview # From a high-level perspective, we can call the process of building code into a binary a job, and we can design our build system as a queue of jobs. Jobs get added to the queue, and each job has a commit identifier (the commit SHA) for what version of the code it should build and the name of the artifact that will be created (the name of the resulting binary). Since we\u0026rsquo;re agnostic to the type of the code being built, we can assume that all languages are handled automatically here.\nWe can have a pool of servers (workers) that are going to handle all of these jobs. Each worker will repeatedly take jobs off the queue (in a FIFO manner—no prioritization for now), build the relevant binaries (again, we\u0026rsquo;re assuming that the actual implementation details of building code are given to us), and write the resulting binaries to blob storage (Google Cloud Storage or S3 for instance). Blob storage makes sense here, because binaries are literally blobs of data.\n4. Build System \u0026ndash; Job Queue # A naive design of the job queue would have us implement it in memory (just as we would implement a queue in coding interviews), but this implementation is very problematic; if there\u0026rsquo;s a failure in our servers that hold this queue, we lose the entire state of our jobs: queued jobs and past jobs.\nIt seems like we would be unnecessarily complicating matters by trying to optimize around this in-memory type of storage, so we\u0026rsquo;re likely better off implementing the queue using a SQL database.\n5. Build System \u0026ndash; SQL Job Queue # We can have a jobs table in our SQL database where every record in the database represents a job, and we can use record-creation timestamps as the queue\u0026rsquo;s ordering mechanism.\nOur table will be:\nid: string, the ID of the job, auto-generated created_at: timestamp commit_sha: string name: string, the pointer to the job\u0026rsquo;s eventual binary in blob storage status: string, QUEUED, RUNNING, SUCCEEDED, FAILED We can implement the actual dequeuing mechanism by looking at the oldest creation_timestamp with a QUEUED status. This means that we\u0026rsquo;ll likely want to index our table on both created_at and status.\n6. Build System \u0026ndash; Concurrency # ACID transactions will make it safe for potentially hundreds of workers to grab jobs off the queue without unintentionally running the same job twice (we\u0026rsquo;ll avoid race conditions). Our actual transaction will look like this:\nBEGIN TRANSACTION; SELECT * FROM jobs_table WHERE status = \u0026#39;QUEUED\u0026#39; ORDER BY created_at ASC LIMIT 1; // if there\u0026#39;s none, we ROLLBACK; UPDATE jobs_table SET status = \u0026#39;RUNNING\u0026#39; WHERE id = id from previous query; COMMIT; All of the workers will be running this transaction every so often to dequeue the next job; let\u0026rsquo;s say every 5 seconds. If we arbitrarily assume that we\u0026rsquo;ll have 100 workers sharing the same queue, we\u0026rsquo;ll have 100/5 = 20 reads per second, which is very easy to handle for a SQL database.\n7. Build System \u0026ndash; Lost Jobs # Since we\u0026rsquo;re designing a large-scale system, we have to expect and handle edge cases. Here, what if there\u0026rsquo;s a network partition with our workers or one of our workers dies mid-build? Since builds last around 15 minutes on average, this will very likely happen. In this case, we want to avoid having a \u0026ldquo;lost job\u0026rdquo; that we were never made aware of, and with our current design, the job will remain RUNNING forever. How do we handle this?\nWe could have an extra column on our jobs table called last_heartbeat. This will be updated in a heartbeat fashion by the worker running a particular job, where that worker will update the relevant row in the table every 3-5 minutes to just let us know that it\u0026rsquo;s still running the job.\nWe can then have a completely separate service that polls the table every so often (say, every 5 minutes, depending on how responsive we want this build system to be), checks all of the RUNNING jobs, and if their last_heartbeat was last modified longer than 2 heartbeats ago (we need some margin of error here), then something\u0026rsquo;s likely wrong, and this service can reset the status of the relevant jobs to QUEUED, which would effectively bring them back to the front of the queue.\nThe transaction that this auxiliary service will perform will look something like this:\nUPDATE jobs_table SET status = \u0026#39;QUEUED\u0026#39; WHERE status = \u0026#39;RUNNING\u0026#39; AND last_heartbeat \u0026lt; NOW() - 10 minutes; 8. Build System \u0026ndash; Scale Estimation # We previously arbitrarily assumed that we would have 100 workers, which made our SQL-database queue able to handle the expected load. We should try to estimate if this number of workers is actually realistic.\nWith some back-of-the-envelope math, we can see that, since a build can take up to 15 minutes, a single worker can run 4 jobs per hour, or ~100 (96) jobs per day. Given thousands of builds per day (say, 5000-10000), this means that we would need 50-100 workers (5000 / 100). So our arbitrary figure was accurate.\nEven if the builds aren\u0026rsquo;t uniformly spread out (in other words, they peak during work hours), our system scales horizontally very easily. We can automatically add or remove workers whenever the load warrants it. We can also scale our system vertically by making our workers more powerful, thereby reducing the build time.\n9. Build System \u0026ndash; Storage # We previously mentioned that we would store binaries in blob storage (GCS). Where does this storage fit into our queueing system exactly?\nWhen a worker completes a build, it can store the binary in GCS before updating the relevant row in the jobs table. This will ensure that a binary has been persisted before its relevant job is marked as SUCCEEDED.\nSince we\u0026rsquo;re going to be deploying our binaries to machines spread across the world, it\u0026rsquo;ll likely make sense to have regional storage rather than just a single global blob store.\nWe can design our system based on regional clusters around the world (in our 5-10 global regions). Each region can have a blob store (a regional GCS bucket). Once a worker successfully stores a binary in our main blob store, the worker is released and can run another job, while the main blob store performs some asynchronous replication to store the binary in all of the regional GCS buckets. Given 5-10 regions and 10GB files, this step should take no more than 5-10 minutes, bringing our total build-and-deploy duration so far to roughly 20-25 minutes (15 minutes for a build and 5-10 minutes for global replication of the binary).\n10. Deployment System \u0026ndash; General Overview # From a high-level perspective, our actual deployment system will need to allow for the very fast distribution of 10GB binaries to hundreds of thousands of machines across all of our global regions. We\u0026rsquo;re likely going to want some service that tells us when a binary has been replicated in all regions, another service that can serve as the source of truth for what binary should currently be run on all machines, and finally a peer-to-peer-network design for our actual machines across the world.\n11. Deployment System \u0026ndash; Replication-Status Service # We can have a global service that continuously checks all regional GCS buckets and aggregates the replication status for successful builds (in other words, checks that a given binary in the main blob store has been replicated across all regions). Once a binary has been replicated across all regions, this service updates a separate SQL database with rows containing the name of a binary and a replication_status. Once a binary has a \u0026ldquo;complete\u0026rdquo; replication_status, it\u0026rsquo;s officially deployable.\n12. Deployment System \u0026ndash; Blob Distribution # Since we\u0026rsquo;re going to deploy 10 GBs to hundreds of thousands of machines, even with our regional clusters, having each machine download a 10GB file one after the other from a regional blob store is going to be extremely slow. A peer-to-peer-network approach will be much faster and will allow us to hit our 30-minute time frame for deployments. All of our regional clusters will behave as peer-to-peer networks.\n13. Deployment System \u0026ndash; Trigger # Let\u0026rsquo;s describe what happens when an engineer presses a button on some internal UI that says \u0026ldquo;Deploy build/binary B1 to every machine globally\u0026rdquo;. This is the action that triggers the binary downloads on all the regional peer-to-peer networks.\nTo simplify this process and to support having multiple builds getting deployed concurrently, we can design this in a goal-state oriented manner.\nThe goal-state will be the desired build version at any point in time and will look something like: \u0026ldquo;current_build: B1\u0026rdquo;, and this can be stored in some dynamic configuration service (a key-value store like Etcd or ZooKeeper). We\u0026rsquo;ll have a global goal-state as well as regional goal-states.\nEach regional cluster will have a K-V store that holds configuration for that cluster about what builds should be running on that cluster, and we\u0026rsquo;ll also have a global K-V store.\nWhen an engineer clicks the \u0026ldquo;Deploy build/binary B1\u0026rdquo; button, our global K-V store\u0026rsquo;s build_version will get updated. Regional K-V stores will be continuously polling the global K-V store (say, every 10 seconds) for updates to the build_version and will update themselves accordingly.\nMachines in the clusters/regions will be polling the relevant regional K-V store, and when the build_version changes, they\u0026rsquo;ll try to fetch that build from the P2P network and run the binary.\n14. System Diagram # Final Systems Architecture\n"},{"id":12,"href":"/tech-book/docs/systemdesign-tips/stock-broker/","title":"Design A Stock-Broker System","section":"SystemDesign-Tips","content":" 1. Gathering System Requirements # As with any systems design interview question, the first thing that we want to do is to gather system requirements; we need to figure out what system we\u0026rsquo;re building exactly.\nWe\u0026rsquo;re building a stock-brokerage platform like Robinhood that functions as the intermediary between end-customers and some central stock exchange. The idea is that the central stock exchange is the platform that actually executes stock trades, whereas the stockbroker is just the platform that customers talk to when they want to place a trade\u0026ndash;the stock brokerage is \u0026ldquo;simpler\u0026rdquo; and more \u0026ldquo;human-readable\u0026rdquo;, so to speak.\nWe only care about supporting market trades\u0026ndash;trades that are executed at the current stock price\u0026ndash;and we can assume that our system stores customer balances (i.e., funds that customers may have previously deposited) in a SQL table.\nWe need to design a PlaceTrade API call, and we know that the central exchange\u0026rsquo;s equivalent API method will take in a callback that\u0026rsquo;s guaranteed to be executed upon completion of a call to that API method.\nWe\u0026rsquo;re designing this system to support millions of trades per day coming from millions of customers in a single region (the U.S., for example). We want the system to be highly available.\n2. Coming Up With A Plan # It\u0026rsquo;s important to organize ourselves and to lay out a clear plan regarding how we\u0026rsquo;re going to tackle our design. What are the major, distinguishable components of our how system?\nWe\u0026rsquo;ll approach the design front to back:\nthe PlaceTrade API call that clients will make the API server(s) handling client API calls the system in charge of executing orders for each customer We\u0026rsquo;ll need to make sure that the following hold:\ntrades can never be stuck forever without either succeeding or failing to be executed a single customer\u0026rsquo;s trades have to be executed in the order in which they were placed balances can never go in the negatives 3. API Call # The core API call that we have to implement is PlaceTrade.\nWe\u0026rsquo;ll define its signature as:\nPlaceTrade( customerId: string, stockTicker: string, type: string (BUY/SELL), quantity: integer, ) =\u0026gt; ( tradeId: string, stockTicker: string, type: string (BUY/SELL), quantity: integer, createdAt: timestamp, status: string (PLACED), reason: string, ) The customer ID can be derived from an authentication token that\u0026rsquo;s only known to the user and that\u0026rsquo;s passed into the API call.\nThe status can be one of:\nPLACED IN PROGRESS FILLED REJECTED That being said, PLACED will actually be the defacto status here, because the other statuses will be asynchronously set once the exchange executes our callback. In other words, the trade status will always be PLACED when the PlaceTrade API call returns, but we can imagine that a GetTrade API call could return statuses other than PLACED.\nPotential reasons for a REJECTED trade might be:\ninsufficient funds\nrandom error\npast market hours\n4. API Server(s) # We\u0026rsquo;ll need multiple API servers to handle all of the incoming requests. Since we don\u0026rsquo;t need any caching when making trades, we don\u0026rsquo;t need any server stickiness, and we can just use some round-robin load balancing to distribute incoming requests between our API servers.\nOnce API servers receive a PlaceTrade call, they\u0026rsquo;ll store the trade in a SQL table. This table needs to be in the same SQL database as the one that the balances table is in, because we\u0026rsquo;ll need to use ACID transactions to alter both tables in an atomic way.\nThe SQL table for trades will look like this:\nid: string, a random, auto-generated string customer_id: string, the id of the customer making the trade stockTicker: string, the ticker symbol of the stock being traded type: string, either BUY or SELL quantity: integer (no fractional shares), the number of shares to trade status: string, the status of the trade; starts as PLACED created_at: timestamp, the time when the trade was created reason: string, the human-readable justification of the trade\u0026rsquo;s status The SQL table for balances will look like this:\nid: string, a random, auto-generated string customer_id: string, the id of the customer related to the balance amount: float, the amount of money that the customer has in USD last_modified: timestamp, the time when the balance was last modified 5. Trade-Execution Queue # With hundreds of orders placed every second, the trades table will be pretty massive. We\u0026rsquo;ll need to figure out a robust way to actually execute our trades and to update our table, all the while making sure of a couple of things:\nWe want to make sure that for a single customer, we only process a single BUY trade at any time, because we need to prevent the customer\u0026rsquo;s balance from ever reaching negative values. Given the nature of market orders, we never know the exact dollar value that a trade will get executed at in the exchange until we get a response from the exchange, so we have to speak to the exchange in order to know whether the trade can go through. We can design this part of our system with a Publish/Subscribe pattern. The idea is to use a message queue like Apache Kafka or Google Cloud Pub/Sub and to have a set of topics that customer ids map to. This gives us at-least-once delivery semantics to make sure that we don\u0026rsquo;t miss new trades. When a customer makes a trade, the API server writes a row to the database and also creates a message that gets routed to a topic for that customer (using hashing), notifying the topic\u0026rsquo;s subscriber that there\u0026rsquo;s a new trade.\nThis gives us a guarantee that for a single customer, we only have a single thread trying to execute their trades at any time.\nSubscribers of topics can be rings of 3 workers (clusters of servers, essentially) that use leader election to have 1 master worker do the work for the cluster (this is for our system\u0026rsquo;s high availability)\u0026ndash;the leader grabs messages as they get pushed to the topic and executes the trades for the customers contained in the messages by calling the exchange. As mentioned above, a single customer\u0026rsquo;s trades are only ever handled by the same cluster of workers, which makes our logic and our SQL queries cleaner.\nAs far as how many topics and clusters of workers we\u0026rsquo;ll need, we can do some rough estimation. If we plan to execute millions of trades per day, that comes down to about 10-100 trades per second given open trading hours during a third of a day and non-uniform trading patterns. If we assume that the core execution logic lasts about a second, then we should have roughly 10-100 topics and clusters of workers to process trades in parallel.\n~100,000 seconds per day (3600 * 24) ~1,000,000 trades per day trades bunched in 1/3rd of the day --\u0026gt; (1,000,000 / 100,000) * 3 = ~30 trades per second 6. Trade-Execution Logic # The subscribers (our workers) are streaming / waiting for messages. Imagine the following message were to arrive in the topic queue:\n{\u0026#34;customerId\u0026#34;: \u0026#34;c1\u0026#34;} The following would be pseudo-code for the worker logic:\n// We get the oldest trade that isn\u0026#39;t in a terminal state. trade = SELECT * FROM trades WHERE customer_id = \u0026#39;c1\u0026#39; AND (status = \u0026#39;PLACED\u0026#39; OR status = \u0026#39;IN PROGRESS\u0026#39;) ORDER BY created_at ASC LIMIT 1; // If the trade is PLACED, we know that it\u0026#39;s effectively // ready to be executed. We set it as IN PROGRESS. if trade.status == \u0026#34;PLACED\u0026#34; { UPDATE trades SET status = \u0026#39;IN PROGRESS\u0026#39; WHERE id = trade.id; } // In the event that the trade somehow already exists in the // exchange, the callback will do the work for us. if exchange.TradeExists(trade.id) { return; } // We get the balance for the customer. balance = SELECT amount FROM balances WHERE customer_id = \u0026#39;c1\u0026#39;; // This is the callback that the exchange will execute once // the trade actually completes. We\u0026#39;ll define it further down // in the walkthrough. callback = ... exchange.Execute( trade.stockTicker, trade.type, trade.quantity, max_price = balance, callback, ) 7. Exchange Callback # Below is some pseudo code for the exchange callback:\nfunction exchange_callback(exchange_trade) { if exchange_trade.status == \u0026#39;FILLED\u0026#39; { BEGIN TRANSACTION; trade = SELECT * FROM trades WHERE id = database_trade.id; if trade.status \u0026lt;\u0026gt; \u0026#39;IN PROGRESS\u0026#39; { ROLLBACK; pubsub.send({customer_id: database_trade.customer_id}); return; } UPDATE balances SET amount -= exchange_trade.amount WHERE customer_id = database_trade.customer_id; UPDATE trades SET status = \u0026#39;FILLED\u0026#39; WHERE id = database_trade.id; COMMIT; } else if exchange_trade.status == \u0026#39;REJECTED\u0026#39; { BEGIN TRANSACTION; UPDATE trades SET status = \u0026#39;REJECTED\u0026#39; WHERE id = database_trade.id; UPDATE trades SET reason = exchange_trade.reason WHERE id = database_trade.id; COMMIT; } pubsub.send({customer_id: database_trade.customer_id}); return http.status(200); } 8. System Diagram # Final Systems Architecture\n"},{"id":13,"href":"/tech-book/docs/systemdesign-tips/design-amazon/","title":"Design Amazon","section":"SystemDesign-Tips","content":" 1. Gathering System Requirements # As with any systems design interview question, the first thing that we want to do is to gather system requirements; we need to figure out what system we\u0026rsquo;re building exactly.\nWe\u0026rsquo;re designing the e-commerce side of the Amazon website, and more specifically, the system that supports users searching for items on the Amazon home page, adding items to cart, submitting orders, and those orders being assigned to relevant Amazon warehouses for shipment.\nWe need to handle items going out of stock, and we\u0026rsquo;ve been given some guidelines for a simple \u0026ldquo;stock-reservation\u0026rdquo; system when users begin the checkout process.\nWe have access to two smart services: one that handles user search queries and one that handles warehouse order assignment. It\u0026rsquo;s our job to figure out how these services fit into our larger design.\nWe\u0026rsquo;ll specifically be designing the system that supports amazon.com (i.e., Amazon\u0026rsquo;s U.S. operations), and we\u0026rsquo;ll assume that this system can be replicated for other regional Amazon stores. For the rest of this walkthrough, whenever we refer to \u0026ldquo;Amazon,\u0026rdquo; we\u0026rsquo;ll be referring specifically to Amazon\u0026rsquo;s U.S. store.\nWhile the system should have low latency when searching for items and high availability in general, serving roughly 10 orders per second in the U.S., we\u0026rsquo;ve been told to focus mostly on core functionality.\n2. Coming Up With A Plan # We\u0026rsquo;ll tackle this system by first looking at a high-level overview of how it\u0026rsquo;ll be set up, then diving into its storage components, and finally looking at how the core functionality comes to life. We can divide the core functionality into two main sections:\nThe user side. The warehouse side. We can further divide the user side as follows:\nBrowsing items given a search term. Modifying the cart. Beginning the checkout process. Submitting and canceliing orders. 3. High-Level System Overview # Within a region, user and warehouse requests will get round-robin-load-balanced to respective sets of API servers, and data will be written to and read from a SQL database for that region.\nWe\u0026rsquo;ll go with a SQL database because all of the data that we\u0026rsquo;ll be dealing with (items, carts, orders, etc.) is, by nature, structured and lends itself well to a relational model.\n4. SQL Tables # We\u0026rsquo;ll have six SQL tables to support our entire system\u0026rsquo;s storage needs.\nItems\nThis table will store all of the items on Amazon, with each row representing an item.\nitemId: uuid name: string description: string price: integer currency: enum other\u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; Carts\nThis table will store all of the carts on Amazon, with each row representing a cart. We\u0026rsquo;ve been told that each user can only have a single cart at once.\ncartId: uuid customerId: uuid items: []{itemId, quantity} \u0026hellip; \u0026hellip; \u0026hellip; Orders\nThis table will store all of the orders on Amazon, with each row representing an order.\n| orderId: uuid |\tcustomerId: uuid\t| orderStatus: enum |\titems: []{itemId, quantity} |\tprice: integer |\tpaymentInfo: PaymentInfo\t| shippingAddress: string\t| timestamp: datetime\t| other\u0026hellip; | | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; | | \u0026hellip; |\t\u0026hellip; |\t\u0026hellip; |\t\u0026hellip; |\t\u0026hellip; |\t\u0026hellip; |\t\u0026hellip; |\t\u0026hellip; |\t\u0026hellip;|\nAggregated Stock\nThis table will store all of the item stocks on Amazon that are relevant to users, with each row representing an item. See the Core User Functionality section for more details.\nitemId: uuid stock: integer \u0026hellip; \u0026hellip; Warehouse Orders\nThis table will store all of the orders that Amazon warehouses get, with each row representing a warehouse order. Warehouse orders are either entire normal Amazon orders or subsets of normal Amazon orders.\nwarehouseOrderId: uuid parentOrderId: uuid warehouseId: uuid orderStatus: enum items: []{itemId, quantity} shippingAddress: string \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; Warehouse Stock\nThis table will store all of the item stocks in Amazon warehouses, with each row representing an {item, warehouse} pairing. The physicalStock field represents an item\u0026rsquo;s actual physical stock in the warehouse in question, serving as a source of truth, while the availableStock field represents an item\u0026rsquo;s effective available stock in the relevant warehouse; this stock gets decreased when orders are assigned to warehouses. See the Core Warehouse Functionality section for more details.\nitemId: uuid warehouseId: uuid physicalStock: integer availableStock: integer \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; 5. Core User Functionality # GetItemCatalog(search)\nThis is the endpoint that users call when they\u0026rsquo;re searching for items. The request is routed by API servers to the smart search-results service, which interacts directly with the items table, caches popular item searches, and returns the results.\nThe API servers also fetch the relevant item stocks from the aggregated_stock table.\nUpdateCartItemQuantity(itemId, quantity)\nThis is the endpoint that users call when they\u0026rsquo;re adding or removing items from their cart. The request writes directly to the carts table, and users can only call this endpoint when an item has enough stock in the aggregated_stock table.\nBeginCheckout() \u0026amp; CancelCheckout()\nThese are the endpoints that users call when they\u0026rsquo;re beginning the checkout process and cancelling it. The BeginCheckout request triggers another read of the aggregated_stock table for the relevant items. If some of the items in the cart don\u0026rsquo;t have enough stock anymore, the UI alerts the users accordingly. For items that do have enough stock, the API servers write to the aggregated_stock table and decrease the relevant stocks accordingly, effectively \u0026ldquo;reserving\u0026rdquo; the items during the duration of the checkout. The CancelCheckout request, which also gets automatically called after 10 minutes of being in the checkout process, writes to the aggregated_stock table and increases the relevant stocks accordingly, thereby \u0026ldquo;unreserving\u0026rdquo; the items. Note that all of the writes to the aggregated_stock are ACID transactions, which allows us to comfortably rely on this SQL table as far as stock correctness is concerned.\nSubmitOrder(), CancelOrder(), \u0026amp; GetMyOrders()\nThese are the endpoints that users call when they\u0026rsquo;re submitting and cancelling orders. Both the SubmitOrder and CancelOrder requests write to the orders table, and CancelOrder also writes to the aggregated_stock table, increasing the relevant stocks accordingly (SubmitOrder doesn\u0026rsquo;t need to because the checkout process already has). GetMyOrders simply reads from the orders table. Note that an order can only be cancelled if it hasn\u0026rsquo;t yet been shipped, which is knowable from the orderStatus field.\n6. Core Warehouse Functionality # On the warehouse side of things, we\u0026rsquo;ll have the smart order-assignment service read from the orders table, figure out the best way to split orders up and assign them to warehouses based on shipping addresses, item stocks, and other data points, and write the final warehouse orders to the warehouse_orders table.\nIn order to know which warehouses have what items and how many, the order-assignment service will rely on the availableStock of relevant items in the warehouse_stock table. When the service assigns an order to a warehouse, it decreases the availableStock of the relevant items for the warehouse in question in the warehouse_stock table. These availableStock values are re-increased by the relevant warehouse if its order ends up being cancelled.\nWhen warehouses get new item stock, lose item stock for whatever reason, or physically ship their assigned orders, they\u0026rsquo;ll update the relevant physicalStock values in the warehouse_stock table. If they get new item stock or lose item stock, they\u0026rsquo;ll also write to the aggregated_stock table (they don\u0026rsquo;t need to do this when shipping assigned orders, since the aggregated_stock table already gets updated by the checkout process on the user side of things).\n7. System Diagram # Final Systems Architecture\n"},{"id":14,"href":"/tech-book/docs/systemdesign-tips/design-slack/","title":"Design Slack","section":"SystemDesign-Tips","content":" 1. Gathering System Requirements # As with any systems design interview question, the first thing that we want to do is to gather system requirements; we need to figure out what system we\u0026rsquo;re building exactly.\nWe\u0026rsquo;re designing the core communication system behind Slack, which allows users to send instant messages in Slack channels.\nSpecifically, we\u0026rsquo;ll want to support:\nLoading the most recent messages in a Slack channel when a user clicks on the channel. Immediately seeing which channels have unread messages for a particular user when that user loads Slack. Immediately seeing which channels have unread mentions of a particular user, for that particular user, when that user loads Slack, and more specifically, the number of these unread mentions in each relevant channel. Sending and receiving Slack messages instantly, in real time. Cross-device synchronization: if a user has both the Slack desktop app and the Slack mobile app open, with an unread channel in both, and if they read this channel on one device, the second device should immediately be updated and no longer display the channel as unread. The system should have low latencies and high availability, catering to a single region of roughly 20 million users. The largest Slack organizations will have as many as 50,000 users, with channels of the same size within them.\nThat being said, for the purpose of this design, we should primarily focus on latency and core functionality; availability and regionality can be disregarded, within reason.\n2. Coming Up With A Plan # We\u0026rsquo;ll tackle this system by dividing it into two main sections:\nHandling what happens when a Slack app loads. Handling real-time messaging as well as cross-device synchronization. We can further divide the first section as follows:\nSeeing all of the channels that a user is a part of. Seeing messages in a particular channel. Seeing which channels have unread messages. Seeing which channels have unread mentions and how many they have. 3. Persistent Storage Solution \u0026amp; App Load # While a large component of our design involves real-time communication, another large part of it involves retrieving data (channels, messages, etc.) at any given time when the Slack app loads. To support this, we\u0026rsquo;ll need a persistent storage solution.\nSpecifically, we\u0026rsquo;ll opt for a SQL database since we can expect this data to be structured and to be queried frequently.\nWe can start with a simple table that\u0026rsquo;ll store every Slack channel.\nChannels\nid (channelId): uuid orgId: uuid name: string description: string \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; Then, we can have another simple table representing channel-member pairs: each row in this table will correspond to a particular user who is in a particular channel. We\u0026rsquo;ll use this table, along with the one above, to fetch a user\u0026rsquo;s relevant when the app loads.\nChannel Members\nid: uuid orgId: uuid channelId: uuid userId: uuid \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; We\u0026rsquo;ll naturally need a table to store all historical messages sent on Slack. This will be our largest table, and it\u0026rsquo;ll be queried every time a user fetches messages in a particular channel. The API endpoint that\u0026rsquo;ll interact with this table will return a paginated response, since we\u0026rsquo;ll typically only want the 50 or 100 most recent messages per channel.\nAlso, this table will only be queried when a user clicks on a channel; we don\u0026rsquo;t want to fetch messages for all of a user\u0026rsquo;s channels on app load, since users will likely never look at most of their channels.\nHistorical Messages\nid: uuid orgId: uuid channelId: uuid senderId: uuid sentAt: timestamp body: string mentions: List \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; In order not to fetch recent messages for every channel on app load, all the while supporting the feature of showing which channels have unread messages, we\u0026rsquo;ll need to store two extra tables: one for the latest activity in each channel (this table will be updated whenever a user sends a message in a channel), and one for the last time a particular user has read a channel (this table will be updated whenever a user opens a channel).\nLatest Channel Timestamps\nid: uuid orgId: uuid channelId: uuid lastActive: timestamp \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; Channel Read Receipts\nid: uuid orgId: uuid channelId: uuid userId: uuid lastSeen: timestamp \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; For the number of unread user mentions that we want to display next to channel names, we\u0026rsquo;ll have another table similar to the read-receipts one, except this one will have a count of unread user mentions instead of a timestamp. This count will be updated (incremented) whenever a user tags another user in a channel message, and it\u0026rsquo;ll also be updated (reset to 0) whenever a user opens a channel with unread mentions of themself.\nUnread Channel-User-Mention Counts\nid: uuid orgId: uuid channelId: uuid userId: uuid count: int \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; 4. Load Balancing # For all of the API calls that clients will issue on app load, including writes to our database (when sending a message or marking a channel as read), we\u0026rsquo;re going to want to load balance.\nWe can have a simple round-robin load balancer, forwarding requests to a set of server clusters that will then handle passing requests to our database.\n5. \u0026ldquo;Smart\u0026rdquo; Sharding # Since our tables will be very large, especially the messages table, we\u0026rsquo;ll need to have some sharding in place.\nThe natural approach is to shard based on organization size: we can have the biggest organizations (with the biggest channels) in their individual shards, and we can have smaller organizations grouped together in other shards.\nAn important point to note here is that, over time, organization sizes and Slack activity within organizations will change. Some organizations might double in size overnight, others might experience seemingly random surges of activity, etc.. This means that, despite our relatively sound sharding strategy, we might still run into hot spots, which is very bad considering the fact that we care about latency so much.\nTo handle this, we can add a \u0026ldquo;smart\u0026rdquo; sharding solution: a subsystem of our system that\u0026rsquo;ll asynchronously measure organization activity and \u0026ldquo;rebalance\u0026rdquo; shards accordingly. This service can be a strongly consistent key-value store like Etcd or ZooKeeper, mapping orgIds to shards. Our API servers will communicate with this service to know which shard to route requests to.\n6. Pub/Sub System for Real-Time Behavior # There are two types of real-time behavior that we want to support:\nSending and receiving messages in real time. Cross-device synchronization (instantly marking a channel as read if you have Slack open on two devices and read the channel on one of them). For both of these functionalities, we can rely on a Pub/Sub messaging system, which itself will rely on our previously described \u0026ldquo;smart\u0026rdquo; sharding strategy.\nEvery Slack organization or group of organizations will be assigned to a Kafka topic, and whenever a user sends a message in a channel or marks a channel as read, our previously mentioned API servers, which handle speaking to our database, will also send a Pub/Sub message to the appropriate Kafka topic.\nThe Pub/Sub messages will look like:\n{ \u0026#34;type\u0026#34;: \u0026#34;chat\u0026#34;, \u0026#34;orgId\u0026#34;: \u0026#34;AAA\u0026#34;, \u0026#34;channelId\u0026#34;: \u0026#34;BBB\u0026#34;, \u0026#34;userId\u0026#34;: \u0026#34;CCC\u0026#34;, \u0026#34;messageId\u0026#34;: \u0026#34;DDD\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2020-08-31T01:17:02\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;this is a message\u0026#34;, \u0026#34;mentions\u0026#34;: [\u0026#34;CCC\u0026#34;, \u0026#34;EEE\u0026#34;] }, { \u0026#34;type\u0026#34;: \u0026#34;read-receipt\u0026#34;, \u0026#34;orgId\u0026#34;: \u0026#34;AAA\u0026#34;, \u0026#34;channelId\u0026#34;: \u0026#34;BBB\u0026#34;, \u0026#34;userId\u0026#34;: \u0026#34;CCC\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2020-08-31T01:17:02\u0026#34; } We\u0026rsquo;ll then have a different set of API servers who subscribe to the various Kakfa topics (probably one API server cluster per topic), and our clients (Slack users) will establish long-lived TCP connections with these API server clusters to receive Pub/Sub messages in real time.\nWe\u0026rsquo;ll want a load balancer in between the clients and these API servers, which will also use the \u0026ldquo;smart\u0026rdquo; sharding strategy to match clients with the appropriate API servers, which will be listening to the appropriate Kafka topics.\nWhen clients receive Pub/Sub messages, they\u0026rsquo;ll handle them accordingly (mark a channel as unread, for example), and if the clients refresh their browser or their mobile app, they\u0026rsquo;ll go through the entire \u0026ldquo;on app load\u0026rdquo; system that we described earlier.\nSince each Pub/Sub message comes with a timestamp, and since reading a channel and sending Slack messages involve writing to our persistent storage, the Pub/Sub messages will effectively be idempotent operations.\n7. System Diagram # Final Systems Architecture\n"},{"id":15,"href":"/tech-book/docs/networking-tips/dns/","title":"DNS Overview","section":"Networking Tips","content":" Intro # DNS (Domain Name System) allows you to interact with devices on the Internet without having to remember long strings of numbers. DNS is designed to be used in both the ways like as a TCP or as a UDP. It converts to TCP when it is not able to communicate on UDP.\nWhat is the Need for DNS? # Every host is identified by the IP address but remembering numbers is very difficult for people also the IP addresses are not static therefore a mapping is required to change the domain name to the IP address. So DNS is used to convert the domain name of the websites to their numerical IP address.\nTypes of Domain # Domain Hierarchy # TLD (Top-Level Domain) is the rightmost part of a domain name. The TLD for geeksforgeeks.com is “.com”. TLDs are divided into two categories: gTLDs (generic top-level domains) and ccTLDs (country code top-level domains). Historically, the purpose of a common top-level domain (gTLD) was to inform users of the purpose of the domain name; For example, a.com would be for business purposes, .org for organization, .edu for education, and .gov for the government. And a country code top-level domain (ccTLD) was used for geographic purposes, such as .ca for Canadian sites, .co.uk for UK sites, and so on. As a result of the high demand, many new gTLDs have emerged, including.online,.club,.website,.biz, and many others.\nSLD(Second-Level Domain): The .org component of geeksforgeeks.org is the top-level domain, while geeksforgeeks is the second-level domain. Second-level domains can only contain a-z 0-9 and hyphens and are limited to 63 characters and TLDs when registering a domain name (may not start or end with hyphens or contain consecutive hyphens).\nSubdomain: A period is used to separate a subdomain from a second-level domain. For example, the admin part is a subdomain named admin.geeksforgeeks.org. A subdomain name, like a second-level domain, is restricted to 63 characters and can only contain the letters a-z, 0-9, and hyphens (cannot begin or end with hyphens or consecutive hyphens).To create longer names, you can use multiple subdomains separated by periods, such as mailer.servers.geeksforgeeks.org. However, the maximum length should not exceed 253 characters. You can create as many subdomains as you want for your domain name.\nDNS Record Types: However, DNS is not just for websites, and there are many other types of DNS records as well. We’ll go through some of the most common ones you’re likely to encounter.\nA Record – For example, 104.26.10.228 is an IPv4 address that these entries resolve to. AAAA Record – For example, 2506:4700:20::681a:bc6 resolves to an IPv6 address. CNAME Record – For example, the subdomain name of Geeksforgeeks’s online shop is marketing.geeksforgeeks.org, which gives a CNAME record of marketing.shopify.com. To determine the IP address, another DNS request will be sent to marketing.shopify.com. MX Record – These records point to the servers that handle the email for the domain you are looking for. For example, the MX record response for geeksforgeeks.com would look like alt1.aspmx.l.google.com. There is also a priority sign on these documents. It instructs the client in which order to try the servers. This is useful when the primary server fails and the email needs to be sent to a backup server. TXT Record – TXT records are text fields that can be used to store any text-based data. TXT records can be used for a variety of things, but one of the most common is to identify the server that has the authorization to send an email on behalf of the domain (this can help in the fight against spam and fake email). is). They can also be used to verify domain ownership when registering for third-party services. How Does DNS Work? # What Are The Steps in a DNS Lookup? # References:\nDNS "},{"id":16,"href":"/tech-book/docs/networking-tips/ecmp/","title":"ECMP Load Balancing","section":"Networking Tips","content":" Intro # ECMP Hashing # ECMP is classified into per-flow load balancing and per-packet load balancing.\nPer-flow load balancing can ensure the packet sequence and ensure that the same data flow is forwarded according to the routing entry with the same next hop and different data flows are forwarded according to routing entries with different next hops.\nWhen multiple routes are installed in the routing table, a hash is used to determine which path a packet follows.\nHashes on the following fields:\nIP protocol Ingress interface Source IPv4 or IPv6 address Destination IPv4 or IPv6 address Further, hashes on these additional fields:\nSource MAC address Destination MAC address Ethertype VLAN ID For TCP/UDP frames, also hashes on:\nSource port Destination port Per-packet load balancing improves ECMP bandwidth utilization and evenly load balances traffic among equal-cost routes, but it cannot prevent out-of-order packet delivery. To resolve this issue, the device or terminal that receives traffic must support reassembly of out-of-order packets.\nThe following per-packet load balancing modes are supported: Random mode: A route is randomly selected among multiple equal-cost routes to forward packets. When the IP address and MAC address of known unicast packets remain unchanged, configure random per-packet load balancing. Round-robin mode: Each equal-cost route is used to forward packets in turn. When known unicast packets have a similar length, configure round-robin per-packet load balancing.\nEth-Trunk/Bundle Load Balancing # Ethernet link aggregation, also known as Eth-Trunk, bundles multiple physical links into a logical link to increase link bandwidth. The bundled links back up each other, increasing reliability.\nEth-Trunk load balancing is classified into per-packet load balancing and per-flow load balancing.\nPer-packet load balancing improves Eth-Trunk bandwidth utilization, but it cannot prevent out-of-order packet delivery. To resolve this issue, the device or terminal that receives traffic must support reassembly of out-of-order packets. The following per-packet load balancing modes are supported: Random mode: The outbound interface of packets is selected randomly based on the time when packets reach the Eth-Trunk. When the IP address and MAC address of known unicast packets remain unchanged, configure random per-packet load balancing. Round-robin mode: Each member interface of an Eth-Trunk forwards traffic in turn. When known unicast packets have a similar length, configure round-robin per-packet load balancing. Per-flow load balancing ensures the packet sequence and ensures that the same data flow is forwarded through the same physical link and different data flows are forwarded through different physical links. Table 1-2 describes the per-flow load balancing modes for different types of packets. How Do I Solve the Hash Polarization Problem? # Hash polarization, also known as hash imbalance, indicates that traffic is unevenly load balanced after being hashed twice or more. This situation is common when hash operations are performed across devices multiple times. For example, a device performs ECMP hashing and forwards traffic to two or more connected devices, which then perform ECMP or Eth-Trunk hashing again. Hash polarization may also occur if the outbound interfaces of ECMP routes are multiple Eth-Trunk interfaces on the same device. The implementation of the hash function on switches heavily depends on chips. Therefore, hash polarization may occur if switches using the same type of chips are located at adjacent network layers. If both Eth-Trunk hashing and ECMP hashing exist on the same device, hash polarization may also occur. Therefore, if ECMP or Eth-Trunk hashing is deployed on a multi-layer network, consider the risk of hash polarization.\nSuggestions # If load imbalance or hash polarization occurs during traffic forwarding, you can adjust the hash algorithm on the device to resolve the problem.\nHash algorithm: is configured by specifying the hash-mode hash-mode-id parameter. Seed value: is configured by specifying the seed seed-data parameter. If devices from multiple vendors exist on the network, you are advised to configure the same seed value for these devices. Offset: is configured by specifying the universal-id universal-id parameter. Typically, one hash algorithm corresponds to one offset. When devices from multiple vendors exist on the network, you are advised to configure the same offset for these devices. Resilient Hashing # When a next hop fails or is removed from an ECMP pool, the hashing or hash bucket assignment can change. For deployments where there is a need for flows to always use the same next hop, like TCP anycast deployments, this can create session failures.\nResilient hashing is an alternate mechanism for managing ECMP groups. The ECMP hash performed with resilient hashing is exactly the same as the default hashing mode. Only the method in which next hops are assigned to hash buckets differs — they’re assigned to buckets by hashing their header fields and using the resulting hash to index into the table of 2^n hash buckets. Since all packets in a given flow have the same header hash value, they all use the same flow bucket.\nReferences:\nhttps://docs.nvidia.com/networking-ethernet-software/cumulus-linux-43/Layer-3/Routing/Equal-Cost-Multipath-Load-Sharing-Hardware-ECMP/ https://support.huawei.com/enterprise/en/doc/EDOC1100086965 "},{"id":17,"href":"/tech-book/docs/systemdesign-tips/google-drive/","title":"Google Drive - Design","section":"SystemDesign-Tips","content":" 1. Gathering System Requirements # As with any systems design interview question, the first thing that we want to do is to gather system requirements; we need to figure out what system we\u0026rsquo;re building exactly.\nWe\u0026rsquo;re designing the core user flow of the Google Drive web application. This consists of storing two main entities: folders and files. More specifically, the system should allow users to create folders, upload and download files, and rename and move entities once they\u0026rsquo;re stored. We don\u0026rsquo;t have to worry about ACLs, sharing entities, or any other auxiliary Google Drive features.\nWe\u0026rsquo;re going to be building this system at a very large scale, assuming 1 billion users, each with 15GB of data stored in Google Drive on average. This adds up to approximately 15,000 PB of data in total, without counting any metadata that we might store for each entity, like its name or its type.\nWe need this service to be Highly Available and also very redundant. No data that\u0026rsquo;s successfully stored in Google Drive can ever be lost, even through catastrophic failures in an entire region of the world.\n2. Coming Up With A Plan # It\u0026rsquo;s important to organize ourselves and to lay out a clear plan regarding how we\u0026rsquo;re going to tackle our design. What are the major, distinguishable components of our how system?\nFirst of all, we\u0026rsquo;ll need to support the following operations:\nFor Files\nUploadFile DownloadFile DeleteFile RenameFile MoveFile For Folders\nCreateFolder GetFolder DeleteFolder RenameFolder MoveFolder Secondly, we\u0026rsquo;ll have to come up with a proper storage solution for two types of data:\nFile Contents: The contents of the files uploaded to Google Drive. These are opaque bytes with no particular structure or format. Entity Info: The metadata for each entity. This might include fields like entityID, ownerID, lastModified, entityName, entityType. This list is non-exhaustive, and we\u0026rsquo;ll most likely add to it later on. Let\u0026rsquo;s start by going over the storage solutions that we want to use, and then we\u0026rsquo;ll go through what happens when each of the operations outlined above is performed.\n3. Storing Entity Info # To store entity information, we can use key-value stores. Since we need high availability and data replication, we need to use something like Etcd, Zookeeper, or Google Cloud Spanner (as a K-V store) that gives us both of those guarantees as well as consistency (as opposed to DynamoDB, for instance, which would give us only eventual consistency).\nSince we\u0026rsquo;re going to be dealing with many gigabytes of entity information (given that we\u0026rsquo;re serving a billion users), we\u0026rsquo;ll need to shard this data across multiple clusters of these K-V stores. Sharding on entityID means that we\u0026rsquo;ll lose the ability to perform batch operations, which these key-value stores give us out of the box and which we\u0026rsquo;ll need when we move entities around (for instance, moving a file from one folder to another would involve editing the metadata of 3 entities; if they were located in 3 different shards that wouldn\u0026rsquo;t be great). Instead, we can shard based on the ownerID of the entity, which means that we can edit the metadata of multiple entities atomically with a transaction, so long as the entities belong to the same user.\nGiven the traffic that this website needs to serve, we can have a layer of proxies for entity information, load balanced on a hash of the ownerID. The proxies could have some caching, as well as perform ACL checks when we eventually decide to support them. The proxies would live at the regional level, whereas the source-of-truth key-value stores would be accessed globally.\n4. Storing File Data # When dealing with potentially very large uploads and data storage, it\u0026rsquo;s often advantageous to split up data into blobs that can be pieced back together to form the original data. When uploading a file, the request will be load balanced across multiple servers that we\u0026rsquo;ll call \u0026ldquo;blob splitters\u0026rdquo;, and these blob splitters will have the job of splitting files into blobs and storing these blobs in some global blob-storage solution like GCS or S3 (since we\u0026rsquo;re designing Google Drive, it might not be a great idea to pick S3 over GCS :P).\nOne thing to keep in mind is that we need a lot of redundancy for the data that we\u0026rsquo;re uploading in order to prevent data loss. So we\u0026rsquo;ll probably want to adopt a strategy like: try pushing to 3 different GCS buckets and consider a write successful only if it went through in at least 2 buckets. This way we always have redundancy without necessarily sacrificing availability. In the background, we can have an extra service in charge of further replicating the data to other buckets in an async manner. For our main 3 buckets, we\u0026rsquo;ll want to pick buckets in 3 different availability zones to avoid having all of our redundant storage get wiped out by potential catastrophic failures in the event of a natural disaster or huge power outage.\nIn order to avoid having multiple identical blobs stored in our blob stores, we\u0026rsquo;ll name the blobs after a hash of their content. This technique is called Content-Addressable Storage, and by using it, we essentially make all blobs immutable in storage. When a file changes, we simply upload the entire new resulting blobs under their new names computed by hashing their new contents.\nThis immutability is very powerful, in part because it means that we can very easily introduce a caching layer between the blob splitters and the buckets, without worrying about keeping caches in sync with the main source of truth when edits are made\u0026ndash;an edit just means that we\u0026rsquo;re dealing with a completely different blob.\n5. Entity Info Structure # Since folders and files will both have common bits of metadata, we can have them share the same structure. The difference will be that folders will have an is_folder flag set to true and a list of children_ids, which will point to the entity information for the folders and files within the folder in question. Files will have an is_folder flag set to false and a blobs field, which will have the IDs of all of the blobs that make up the data within the relevant file. Both entities can also have a parent_id field, which will point to the entity information of the entity\u0026rsquo;s parent folder. This will help us quickly find parents when moving files and folders.\n`File Info` `{` `blobs: [\u0026#39;blob_content_hash_0\u0026#39;, \u0026#39;blob_content_hash_1\u0026#39;],` `id: \u0026#39;some_unique_entity_id\u0026#39;` `is_folder: false,` `name: \u0026#39;some_file_name\u0026#39;,` `owner_id: \u0026#39;id_of_owner\u0026#39;,` `parent_id: \u0026#39;id_of_parent\u0026#39;,` `}` `Folder Info` `{` `children_ids: [\u0026#39;id_of_child_0\u0026#39;, \u0026#39;id_of_child_1\u0026#39;],` `id: \u0026#39;some_unique_entity_id\u0026#39;` `is_folder: true,` `name: \u0026#39;some_folder_name\u0026#39;,` `owner_id: \u0026#39;id_of_owner\u0026#39;,` `parent_id: \u0026#39;id_of_parent\u0026#39;,` `} `\n6. Garbage Collection # Any change to an existing file will create a whole new blob and de-reference the old one. Furthermore, any deleted file will also de-reference the file\u0026rsquo;s blobs. This means that we\u0026rsquo;ll eventually end up with a lot of orphaned blobs that are basically unused and taking up storage for no reason. We\u0026rsquo;ll need a way to get rid of these blobs to free some space.\nWe can have a Garbage Collection service that watches the entity-info K-V stores and keeps counts of the number of times every blob is referenced by files; these counts can be stored in a SQL table.\nReference counts will get updated whenever files are uploaded and deleted. When the reference count for a particular blob reaches 0, the Garbage Collector can mark the blob in question as orphaned in the relevant blob stores, and the blob will be safely deleted after some time if it hasn\u0026rsquo;t been accessed.\n7. End To End API Flow # Now that we\u0026rsquo;ve designed the entire system, we can walk through what happens when a user performs any of the operations we listed above.\nCreateFolder is simple; since folders don\u0026rsquo;t have a blob-storage component, creating a folder just involves storing some metadata in our key-value stores.\nUploadFile works in two steps. The first is to store the blobs that make up the file in the blob storage. Once the blobs are persisted, we can create the file-info object, store the blob-content hashes inside its blobs field, and write this metadata to our key-value stores.\nDownloadFile fetches the file\u0026rsquo;s metadata from our key-value stores given the file\u0026rsquo;s ID. The metadata contains the hashes of all of the blobs that make up the content of the file, which we can use to fetch all of the blobs from blob storage. We can then assemble them into the file and save it onto local disk.\nAll of the Get, Rename, Move, and Delete operations atomically change the metadata of one or several entities within our key-value stores using the transaction guarantees that they give us.\n8. System Diagram # Final Systems Architecture\n"},{"id":18,"href":"/tech-book/docs/manageability/why-grpc-on-http2/","title":"gRPC on HTTP/2","section":"Manageability","content":" Intro # tbd\nReferences:\nHTTP/2: Smarter at scale gRPC on HTTP/2: Engineering a robust, high performance protocol "},{"id":19,"href":"/tech-book/docs/networking-tips/ip-fragmentation/","title":"IP Fragmentation - IPv4 \u0026 IPv6","section":"Networking Tips","content":" Intro # Like IPv4, IPv6 fragmentation divides an IPv6 packet into smaller packets to facilitate transmission across networks with a smaller Maximum Transmission Unit (MTU). Unlike IPv4, fragmentation is not mandatory in IPv6, as all networks support an MTU of at least 1280 bytes.\nUnlike IPv4, IPv6 relies on the source device instead of intermediary routers for fragmentation Unlike IPv4, an IPv6 router does not fragment a packet unless it is the packet’s source. Intermediate nodes (routers) do not fragment. You will see how an IPv6 device fragments packets when it is the source of the packet with the use of extension headers. An IPv6 router drops packets too large for the egress interface and sends an ICMPv6 Packet Too Big message back to the source. Packet Too Big messages include the link’s MTU size in bytes so the source can resize the packet. Therefore, using the largest packet size supported by all the links from the source to the destination is preferable. Path MTUs (PMTUs) are used for this purpose. Path MTU Discovery: # In addition, IPv6 nodes can use the Path MTU Discovery (PMTUD) mechanism to dynamically determine the maximum MTU size along the path to a destination. PMTUD sends packets with the “Don’t Fragment” (DF) flag set and progressively reduces the packet size until a smaller MTU is found. Once the maximum MTU size is determined, the source node can adjust its packet size accordingly to avoid fragmentation.\nWhile IPv6 fragmentation is a valuable mechanism for ensuring packet delivery over networks with smaller MTU sizes, it should be used sparingly. Minimizing the need for fragmentation through proper MTU configuration and utilizing PMTUD can help improve network performance and reliability. ICMP and ICMPv6 # The Internet Control Messaging Protocol ( ICMP ) was initially introduced to aid network troubleshooting by providing tools to verify end-to-end reachability. ICMP also reports back errors on hosts. Unfortunately, due to its nature and lack of built-in security, it quickly became a target for many attacks. For example, an attacker can use ICMP REQUESTS for network reconnaissance.\nICMP’s lack of inherent security opened it up to some vulnerabilities. This results in many security teams blocking all ICMP message types, which harms useful ICMP features such as Path MTU. ICMP for v4 and v6 are entirely different. Unlike ICMP for IPv4, ICMPv6 is an integral part of v6 communication, and ICMPv6 has features required for IPv6 operation. For this reason, it is not possible to block ICMPv6 and all its message types. ICMPv6 is a legitimate part of V6; you must select what you can filter. ICMPv6 should not be completely filtered.\nFragmentation is normal # Fragmentation is a normal process on packet-switched networks. It occurs when a large packet is received, and the corresponding outbound interface’s maximum transmission unit (MTU) size is too small. Fragmentation dissects the IP packet into smaller packets before transmission. The receiving host performs fragment reassembly and passes the complete IP packet up the protocol stack.\nFragmentation is an IP process; TCP and other layers above IP are not involved. Reassembly is intended in the receiving host, but in practice, it may be done by an intermediate router. For example, network address translation (NAT) may need to reassemble fragments to translate data streams. This is where some differences between fragmentation in IPv4 and IPv6 are apparent.\nSo, in summary. IP fragmentation occurs at the Internet Protocol (IP) layer. Packets are fragmented to pass through a link with a smaller maximum transmission unit (MTU) than the original packet size. The receiving host then reassembles fragments.\nImpact on networks # In networks with multiple parallel paths, technologies such as LAG and CEF split traffic according to a hash algorithm. All packets from the same flow are sent out on the same path to minimize packet reordering. IP fragmentation can cause excessive retransmissions when fragments encounter packet loss. This is because reliable protocols such as TCP must retransmit all fragments to recover from the loss of a single fragment.\nFragmentation in IPv4 and IPv6 # Fragmentation in IPv6 is splitting a single inbound IP datagram into two or more outbound IP datagrams. The IP header is copied from the original IP datagram into the fragments. With IPv6, special bits are set in the fragments’ IPv6 headers to indicate that they are not complete IP packets. In the case of IPv6, we use IPv6 extension headers. Then, the payload is spread across the fragments. In IPv4, fragmentation is done whenever required, at the destination or routers, whereas, in IPv6, only the source, not the routers, is supposed to do fragmentation. This can only be done when the source knows the Maximum Transmission Unit (MTU) path. The IPv6 “do not fragment” bit is always 1, whereas the case is not the same in IPv4, and the ‘More fragment’ bit is the only flag in the fragmentation header, which is one bit. Two bits are reserved for future use, as shown in the picture below. The following diagram displays the Internet Protocol Version 6 Fragmentation Header.\nIPv6 fragmentation example # In an IPv6 world, the IPv6 header length is limited to 40 bytes, yet the IPv4 header has a max of 60. The primary IPv6 header remains a fixed size. IPv6 has the concept of extension headers to add optional IP layer information. Special handling with IPv4 was controlled by “IP options,” but there are no IP options in IPv6. All options are moved to different types of extension headers. The following IPv6 extension headers are currently defined.\nRouting – Extended routing, like IPv4 loose source route Fragmentation – Fragmentation and reassembly Authentication – Integrity and authentication, security Encapsulation – Confidentiality Hop-by-Hop Option – Special options that require hop-by-hop processing Destination Options – Optional information to be examined by the destination node The IPv6 fragment header # With IPv4, all this was contained in the IPv4 header. There is no separate fragment header in IPv4, and the fragment fields in the IPv4 header are moved to the IPv6 fragment header. The “Don’t fragment” bit (DF) is removed, and intermediate routers are not allowed to fragment. They only permitted end stations to create and reassemble fragments (RFC 2460), not intermediate routers. The design decision stems from the performance hit that fragmentation imposes on nodes.\nRouters are no longer required to perform packet fragmentation and reassembly, making dropped packets more significant than the router’s interface MTU. Instead, IPv6 hosts perform PMTU to determine the maximum packet size for the entire path. When a packet hits an interface with a smaller MTU, the routers send back an ICMPv6 type 2 error, known as Packet Too Big, to the sending host. The sending host receives the error message, reduces the size of the sending packet, and tries again.\nReferences:\nIPv6 Fragmentation "},{"id":20,"href":"/tech-book/docs/networking-tips/ip-tos-dscp/","title":"IP Precedence And TOS | DSCP","section":"Networking Tips","content":" Intro # 8 Bits of Type of Service in IP Header.\nIP Precedence # RFC791/RFC1349 Interpretation\n** Bits ** 7-5 IP Precedence\n111\tNetwork Control\n110\tInternetwork Control\n101\tCritic/ECP\n100\tFlash Override\n011\tFlash\n010\tImmediate\n001\tPriority\n000\tRoutine\nBits\n4 (1 = Low Delay; 0 = Normal Delay)\n3 (1 = High Throughput; 0 = Normal Throughput)\n2 (1 = High Reliability; 0 = Normal Reliability)\n1 (1 = Minimise monetary cost (RFC 1349))\n0 (Must be 0)\nTOS | DSCP (Differentiated Services Code Point) # RFC 2474 (Differentiated Services) Interpretation\nBits\n7-2\tDSCP\n1-0\tECN (Explicit Congestion Notification)\nDefault Forwarding (DF) PHB # Typically best-effort traffic The recommended DSCP for DF is 0\nExpedited Forwarding (EF) PHB # Dedicated to low-loss, low-latency traffic The recommended DSCP for EF is 101110B (46 or 2E(hex))\nAssured Forwarding (AF) PHB # Gives assurance of delivery under prescribed conditions\nDrop probability Class 1 Class 2 Class 3 Class 4 Low AF11 (DSCP 10) 001010 AF21 (DSCP 18) 010010 AF31 (DSCP 26) 011010 AF41 (DSCP 34) 100010 Medium AF12 (DSCP 12) 001100 AF22 (DSCP 20) 010100 AF32 (DSCP 28) 011100 AF42 (DSCP 36) 100100 High AF13 (DSCP 14) 001110 AF23 (DSCP 22) 010110 AF33 (DSCP 30) 011110 AF43 (DSCP 38) 100110 Class Selector PHBs # This maintains backward compatibility with the IP precedence field.\nService class DSCP Name DSCP Value IP precedence Examples of application Standard CS0 (DF) 0 0 (000) Low-priority data CS1 8 1 (001) File transfer (FTP, SMB) Network operations, administration and management (OAM) CS2 16 2 (010) SNMP, SSH, Ping, Telnet, syslog Broadcast video CS3 24 3 (011) RTSP broadcast TV, treaming of live audio and video events, video surveillance,video-on-demand Real-time interactive CS4 32 4 (100) Gaming, low priority video conferencing Signaling CS5 40 5 (101) Peer-to-peer (SIP, H.323, H.248), NTP Network control CS6 48 6 (110) Routing protocols (OSPF, BGP, ISIS, RIP) Reserved for future use CS7 56 7 (111) DF= Default Forwarding\nPHB == Per-Hop-Behavior\nCS: Class Selector (RFC 2474) AFxy: Assured Forwarding (x=class, y=drop precedence) (RFC2597) EF: Expedited Forwarding (RFC 3246) References:\nhttps://en.wikipedia.org/wiki/Differentiated_services https://bogpeople.com/networking/dscp.shtml "},{"id":21,"href":"/tech-book/docs/networking-tips/traceroute/","title":"Linux traceroute tool","section":"Networking Tips","content":" Intro # The linux traceroute tool is been great use for network engineer for their troubleshooting. The first version of this tool has been introduced 25 years ago. It\u0026rsquo;s relying on the TTL field of the ip-header, keep on incrementing starting on TTL as 1. Along the way, each router decreases the TTL by one, and when it hits zero, the router sends back an ICMP \u0026rsquo;time exceeded\u0026rsquo; message, revealing its identity. The modern network has evloved over time. Now a ways, it has always the multi-path support for destinations, NAT, etc. Hence, traceroute needs to enhanced to accomodate these use-cases.\nThere are two implementations emerged. lets go little deeper for both the implementations.\nparis-traceroute (https://paris-traceroute.net/about/) dublin-traceroute (https://dublin-traceroute.net/) Paris Traceroute # This maintain a per-flow session information for load-balancer to work. Other than this, it also maintain tcp, udp and icmp packet mode. Also, it sends multiple probe with different session information, which helps to find different packet path for same destination.\nDublin Traceroute # This is Paris-traceroute + NAT awareness.\nReferences:\nParis Traceroute Dublin Traceroute The Power of Paris Traceroute for Modern Load-Balanced Networks "},{"id":22,"href":"/tech-book/docs/algorithms/medium/","title":"Medium Complexity","section":"Algorithms","content":" Medium Complexity # Number Swapper: Write a function to swap a number in place (that is, without temporary variables). Hints - with just addition/substruction arithmatic, XOR.\nTic Tac Win: Design an algorithm to figure out if someone has won a game of tic-tac-toe.\nHashing # Two Sum: Find a pair in array whose sum equals to the target input: [10,4,1,3,2] Target: 7 Output: [4,3]\nHint - insert in hash and then start searching from first element, find the difference with the target and find the hash.\nLogest consecutive sequence: find length of longest consecutive sequence from given array input: [10, 4, 20, 1,3,2] Output: [1,2,3,4]\nSliding Window # Two Sum: find a pair whose sum is equal to target\ninput: [10,4,1,3,2] Target: 7 Output: [4,3]\nHint - sort, start from bengining and end at the same time\nTraping Water: Given an arrary of height of bars (width = 1) calculate the amount of water trapped.\ninput: 5, 2, 3, 4, 1, 3 Output = 5\nHint - use sliding window.\nRecursion # Combination Sum: Return the list of numbers from given array whose sum = target\nGenerate all pairs of valid parenthesis\nInput: 2\nOutput: {\n​\t()(),\n​\t(())\n​\t}\nBinary Tree # Height of the binary tree\nHints - use recursion\nDiameter of the binary tree\nThe diameter of the binary tree is the length of the longest path between any two nodes in the tree.\nHints - use recursion\nConvert to the Sum Tree: Convert it such that every node\u0026rsquo;s value is equal to sum of its left and right sub tree.\nHints - use recursion\nMaximum path sum in binary tree\nLowest common ancestor in a binary tree\n"},{"id":23,"href":"/tech-book/docs/networking-tips/mlag/","title":"Multi Chassis Link Aggregation Basics","section":"Networking Tips","content":" Intro # Link Aggregation Basics # Link aggregation is an ancient technology that allows you to bond multiple parallel links into a single virtual link (link aggregation group – LAG). With parallel links being replaced by a single virtual link, STP detects no loops and all the physical links can be fully utilized.\nI was also told that link aggregation makes your bridged network more stable. Every link state change triggers a Topology Change Notification in spanning tree, potentially sending a large part of your network through the STP listening-learning-forwarding state diagram. A link failure of a LAG member does not change the state of the aggregation group (unless it was the last working link in the group), and since STP rides on top of aggregated interfaces, it does not react to the failure.\nMulti-Chassis Link Aggregation(MLAG) # Imagine you could pretend two physical boxes use a single control plane and coordinated switching fabrics. The links terminated on two physical boxes would seem to terminate within the same control plane and you could aggregate them using LACP. Welcome to the wonderful world of Multi-Chassis Link Aggregation (MLAG).\nMLAG nicely solves two problems:\nWhen used in the network core, multiple links appear as a single link. No bandwidth is wasted due to STP loop prevention while you still retain almost full redundancy – just make sure you always read the smallprint to understand what happens when one of the two switches in the MLAG pair fails. When used between a server and a pair of top-of-rack switches, it allows the server to use the full aggregate bandwidth while still retaining resiliency against a link or switch failure. Multi-Chassis Link Aggregation Group (MC-LAG) # Overview # MC-LAG achieves similar redundancy and load balancing but is used primarily in service provider networks. Unlike MLAG, MC-LAG often uses a control plane protocol (e.g., ICCP - Inter-Chassis Control Protocol) to sync state information.\nArchitecture # Uses a control protocol (ICCP) to synchronize state between switches. Traffic forwarding decisions are synchronized between nodes. Unlike MLAG, which is mostly layer 2, MC-LAG may operate at Layer 3 for better scaling. Vendor-Specific Implementations # Vendor MC-LAG Implementation Key Features Juniper MC-LAG Uses ICCP for control-plane synchronization Nokia MC-LAG Supports both L2 and L3 redundancy Cisco (SP networks) MC-LAG Used in service provider routers (ASR, NCS series) Huawei MC-LAG Provides link aggregation for core networks Key Technical Differences: MLAG vs MC-LAG # Feature MLAG MC-LAG Primary Use Case Data center networks Service provider networks Interoperability Vendor-specific (Cisco vPC, Arista MLAG) More interoperable (Juniper, Nokia, Cisco SP) Control Plane No dedicated protocol; uses peer links Uses ICCP or proprietary control protocols Layer of Operation Layer 2 primarily Layer 2 \u0026amp; Layer 3 Failure Handling Uses peer-link recovery methods Uses ICCP for state synchronization Complexity Easier to configure More complex due to control-plane sync Scalability Works well for small to medium-sized networks Scales better in larger SP environments Link Aggregation Control Protocol (LACP) # LACP (Link Aggregation Control Protocol): a subcomponent of IEEE 802.3ad standard, provides a method to control the bundling of several physical ports together to form a single logical channel. LACP allows a network device to negotiate an automatic bundling of links by sending LACP packets to the peer.\nReferences:\nhttps://blog.ipspace.net/2010/10/multi-chassis-link-aggregation-basics.html https://community.fs.com/article/mlag-vs-stacking-vs-lacp.html "},{"id":24,"href":"/tech-book/docs/data-center/data-center-network-virtualization/","title":"Network Virtualization in Cloud Data Centers","section":"Data Center Tips","content":" Intro # Geographic Clusters of Data Centers # Data Center Interconnection (DCI) # Challenges of LAN Extension # TRILL # TRILL Architecture # TRILL Encapsulation Format # TRILL Features # GRE # GRE (Cont) # NVGRE # NVGRE (Cont) # NVO3 # NVO3 Terminology # NVO3 Terminology (Cont) # Current NVO Technologies # VXLAN # VXLAN Architecture # VXLAN Deployment Example # ![img|320x271](https://prasenjitmanna.com/tech-book/diagrams/dc-virtualization/VXLAN Deployment Example.png)\nVXLAN Encapsulation Format # ![img|320x271](https://prasenjitmanna.com/tech-book/diagrams/dc-virtualization/VXLAN Encapsulation Format.png)\nVXLAN Encapsulation Format (Cont) # Geneve # Geneve Frame Format # Geneve Frame Format (Cont) # LSO and LRO # Geneve Implementation Issues # Geneve Implementation Issues (Cont) # References:\nRaj Jain LAN Extension and Network Virtualization in Cloud Data Centers by Raj Jain Overlay Virtual Networking Explained by Ivan Pepelnjak(ip@ipSpace.net) "},{"id":25,"href":"/tech-book/docs/optical-knowledge/optical-breakout/","title":"Optical Transceiver(Grey) \u0026 Breakout Model","section":"Optical Knowledge","content":" Intro # There are two types of optics, grey and colored. In the grey optics, it has specific wavelength based on the pluggable types and they are used for short distances, while colored optics are designed for longer distance and it has more control on the frequency/wave-length configuration. In general, grey optics are used in router interface. This section covers mostly on Grey Optics.\nTransceiver Names: Your Guide to Decoding Fiber Optics\nHow to choose the right transceiver Fiber Connector Types - LC vs SC vs FC vs ST vs MTP vs MPO Everything You Need to Know About MTP® MPO Connectors\nHow to Breakout 100G to 4x25G\nHow to breakout 400G QSFP-DD DR4 to 4x 100G Multi-Mode Vs Single Mode Transceivers\nHow to Choose the Right Fiber Optic Cable For Your Optical Transceiver\nAsk an Engineer: Fiber Breakout Cable\nReferences: *\n"},{"id":26,"href":"/tech-book/docs/networking-tips/qos/","title":"QoS","section":"Networking Tips","content":" Intro # Traffic Shaping # The traffic shapers that we will look at here are commonly called token bucket shapers and are based on a token bucket algorithm which we will see can serve multiple purposes in achieving network QoS. We should note that frequently in networking literature such algorithms were also referred to as leaky bucket algorithms. In all but one case, that we will not be discussing here, these algorithms produce the same results. Use the token bucket and leaky bucket Wikipedia entries as a guide if you are reading older networking literature.\nThe basic token bucket traffic shaper is characterized by two parameters: a rate, r, and a bucket size, b. The rate can be in either bits or bytes per second and the bucket size can be in either bits or bytes.\nThe key pieces of the shaper are:\nToken generator with rate r. This requires some type of relatively accurate timing mechanism. The granularity of this mechanism heavily influences the accuracy to which r can be approximated.\nToken bucket with size b. This can be implemented with up/down counters or adders/subtractors in hardware or software.\nA buffer (queue) to hold packets\nLogic to control the release of packets based on packet size and current bucket fill.\nPolicing # A Single Rate Three Color Marker # A policer called a single rate three color marker (srTCM) is defined in RFC2697 an informational RFC. The single rate means we will use a single token generator with a specified rate parameter. By three color they mean that packets will be marked with by three levels of compliance (Green, Yellow, Red) from compliant to most out of compliance.\nThis particular policer is defined by three traffic parameters, an update algorithm, and marking criteria. The three traffic parameters are:\nCommitted Information Rate (CIR) measured in bytes of IP packets per second, i.e., it includes the IP header, but not link specific headers. This is the token rate.\nCommitted Burst Size (CBS) measured in bytes. This is the token bucket size associated with the rate.\nExcess Burst Size (EBS) measured in bytes. This is a bucket filled from the overflow of the first bucket hence the expression \u0026ldquo;excesses burst size\u0026rdquo;. This bucket allows us to save up tokens from periods of inactivity for later use. The CBS and EBS must be configured so that at least one of them is larger than 0.\nThere are two modes of operation for the srTCM one, called Color-Aware works with packets that have been previously marked by another policer in the network, the other mode called Color-Blind works with packets that have not been previously marked. The Color-Blind mode of operation is shown in below.\nTwo Rate Three Color Marker # Another policer called a two rate three color marker (trTCM) is defined in RFC2698, an informational RFC. The two rate means we will use a two token generators each with a specified rate parameter. Once again the three colors Green, Yellow, Red indicate the level of compliance which gets translated into packet drop precedence when the packets get marked. The parameters for this policer are: the Peak Information Rate (PIR), the Peak Burst Size (PBS), the Committed Information Rate (CIR), and the Committed Burst Size (CBS). All use units similar to that of the srTCM. The initialization, update, and marking behavior for the Color-Blind mode is shown below.\nReferences:\nhttps://www.grotto-networking.com/BBQoS.html "},{"id":27,"href":"/tech-book/docs/networking-tips/spine-leaf-arch/","title":"Spine-leaf Architecture Basics","section":"Networking Tips","content":" Intro # What Is Spine-leaf Architecture? # The spine-leaf architecture consists of only two layers of switches: spine and leaf switches. The spine layer consists of switches that perform routing and work as the core of the network. The leaf layer involves access switches that connect to servers, storage devices, and other end-users. This structure helps data center networks reduce hop count and reduce network latency.\nIn the spine-leaf architecture, each leaf switch is connected to each spine switch. With this design, any server can communicate with any other server, and there is no more than one interconnected switch path between any two leaf switches.\nWhy Use Spine-leaf Architecture? # The spine-leaf architecture has become a popular data center architecture, bringing many advantages to the data center, such as scalability, network performance, etc. The benefits of spine-leaf architecture in modern networks are summarized here in five points.\nIncreased redundancy: The spine-leaf architecture connects the servers with the core network, and has higher flexibility in hyper-scale data centers. In this case, the leaf switch can be deployed as a bridge between the server and the core network. Each leaf switch connects to all spine switches, which creates a large non-blocking fabric, increasing the level of redundancy and reducing traffic bottlenecks.\nIncreased bandwidth: The spine-leaf architecture can effectively avoid traffic congestion by applying protocols or techniques such as transparent interconnection of multiple links (TRILL) and shortest path bridging (SPB). The spine-leaf architecture can be Layer 2 or Layer 3, so uplinks can be added to the spine switch to expand inter-layer bandwidth and reduce oversubscription to secure network stability.\nImprove scalability: The spine-leaf architecture has multiple links that can carry traffic. The addition of switches will improve scalability and help enterprises to expand their business later.\nReduced expenses: A spine-leaf architecture increases the number of connections each switch can handle, so data centers require fewer devices to deploy. Many data center networks employ spine-leaf architecture to minimize costs.\nMinimal latency and congestion: By limiting the maximum number of hops to two between any source and destination nodes, we establish a more direct traffic path, enhancing overall performance and mitigating bottlenecks. The only exception is when the destination is on the same leaf switch.\nSpine-leaf vs. Traditional Three-Tier Architecture # The main difference between spine-leaf architecture and 3-tier architecture lies in the number of network layers, and the traffic they transform is north-south or east-west traffic.\nAs shown in the following figure, the traditional three-tier network architecture consists of three layers: core, aggregation and access. The access switches are connected to servers and storage devices, the aggregation layer aggregates the access layer traffic, provides redundant connections at the access layer, and the core layer provides network transmission. But this three-layer topology is usually designed for north-south traffic and uses the STP protocol, supporting up to 100 switches. In the case of continuous expansion of network data, this will inevitably result in port blockage and limited scalability.\nThe spine-leaf architecture is to add east-west traffic parallelism to the north-south network architecture of the backbone, fundamentally solving the bottleneck problem of the traditional three-tier network architecture. It increases the exchange layer under the access layer, and the data transmission between two nodes is completed directly at this layer, thereby diverting backbone network transmission. Compared with traditional three-tier architecture, the spine-leaf architecture provides a connection through the spine with a single hop between leaves, minimizing any latency and bottle necks. In spine-leaf architectures, the switch configuration is fixed so that no network changes are required for a dynamic server environment.\nHow to Design Spine-leaf Architecture? # Before designing a spine-leaf architecture, you need to figure out some important and relevant considerations, especially the oversubscription rate and the size of the spine switch. Surely, we have also given a detailed example for your reference.\nDesign Considerations of Spine-leaf Architecture # Oversubscription rate: It is the contention rate when all devices are sending traffic at the same time. It can be measured in the north/south direction (traffic entering/leaving the data center) and in the east/west direction (traffic between devices within the data center). The most appropriate oversubscription ratio for modern network architectures is 3:1 or less, which is measured and delineated as a ratio between upstream bandwidth (to backbone switches) and downstream capacity (to servers/storage).\nFor example, a leaf switch has 48 x 10G ports for a total of 480Gb/s of port capacity. If you connect 4 x 40G uplink ports from each leaf switch to a 40G spine switch, it will have 160Gb/s of uplink capacity. The ratio is 480:160, or 3:1. However, data center uplinks are typically 40G or 100G and can be migrated over time from a starting point of 40G (Nx 40G) to 100G (Nx 100G). It is important to note that the uplink should always run faster than the downlink to not have port link blockage.\nLeaf and spine sizing: The maximum number of leaf switches in the topology is determined by the port density of the spine switches. And the number of spine switches will be governed by the combination of the required throughput between the leaf switches, the number of redundant/ECMP (equivalent multipath) paths, and their port density. So the number of spine-leaf switches and port density need to be taken into account to prevent network problems.\nLayer 2 or Layer 3 design: A two-tier spine-leaf fabric can be built at either Layer 2 (configuring VLANs) or Layer 3 (subnetting). Layer 2 designs need to provide maximum flexibility, allowing VLANs to span anywhere and MAC addresses to migrate anywhere. Layer 3 designs need to provide the fastest convergence times and maximum scale with fan-out ECMP supporting up to 32 or more active spine switches.\nReferences:\nhttps://community.fs.com/article/leaf-spine-with-fs-com-switches.html "},{"id":28,"href":"/tech-book/docs/networking-tips/tcp-congestion/","title":"TCP Congestion Control","section":"Networking Tips","content":" Intro # TCP is a protocol that is used to transmit information from one computer on the internet to another, and is the protocol I’ll be focused on in this post. What distinguishes TCP from other networking protocols is that it guarantees 100% transmission. This means that if you send 100kb of data from one computer to another using TCP, all 100kb will make it to the other side. This property of TCP is very powerful and is the reason that many network applications we use, such as the web and email are built on top of it. The way TCP is able to accomplish this goal of trasmitting all the information that is sent over the wire is that for every segment of data that is sent from party A to party B, party B sends an “acknowledgement” segment back to party A indicating that it got that message.\nWhen does congestion happen? # Congestion is problem in computer networks because at the end of the day, information transfer rates are limited by physical channels like ethernet cables or cellular links, and on the internet, many individual devices are connected to these links.\nDetour: What is a link? # Before I dive into what some solutions to this problem are, I want to be a little bit more specific about the properties of links. There are three important details to know about a link:\ndelay (milliseconds) - the time it takes for one packet to get from the beginning to the end of a link bandwidth (megabits/second) - the number of packets that can get through the link in a second queue - the size of the queue for storing packets waiting to be sent out if a link is full, and the strategy for managing that queue when it hits its capacity Using the analogy of the link as a pipe, you can think of the delay as the length of the pipe, and the bandwidth as the circumference of the pipe. An important statistic about a link is the bandwidth-delay product (BDP). If senders are sending more bytes than the BDP, the link’s queue will fill and eventually start dropping packets.\nApproaches # There are two main indicators: packet loss and increased round trip times for packets. When congestion happens, queues on links begin to fill up, and packets get dropped. If a sender notices packet loss, it’s a pretty good indicator that congestion is occuring. Another consequence of queues filling up though is that if packets are spending more time in a queue before making it onto the link, the round trip time, which measures the time from when the sender sends a segment out to the time that it receives an acknowledgement, will increase. While today there are congestion control schemes that take into account both of these indicators, in the original implementations of congestion control, only packet loss was used.\nTherefore, in addition to being able to avoid congestion, congestion control approaches need to be able to “explore” the available bandwidth.\nControl-Based Algorithms # The Congestion Window # A key concept to understand about any congestion control algorithm is the concept of a congestion window. The congestion window refers to the number of segments that a sender can send without having seen an acknowledgment yet. If the congestion window on a sender is set to 2, that means that after the sender sends 2 segments, it must wait to get an acknowledgment from the receiver in order to send any more. The congestion window is often referred to as the “flight size”, because it also corresponds to the number of segments “in flight” at any given point in time.\nThe higher the congestion window, more packets you’ll be able to get across to the receiver in the same time period. To understand this intuitively, if the delay on the network is 88ms, and the congestion window is 10 segments, you’ll be able to send 10 segments for every round trip (88*2 = 176 ms), and if it’s 20 segments, you’ll be able to send 20 segments in the same time period.\nOf course, the risk with raising the congestion window too high is that it will lead to congestion. The goal of a congestion control algorithm, then, is to figure out the right size congestion window to use.\nFrom a theoretical perspective, the right size congestion window to use is the bandwidth-delay product of the link, which as we discussed earlier is the full capacity of the link. The idea here is that if the congestion window is equal to the BDP of the link, it will be fully utilized, and not cause congestion.\nTCP Tahoe # TCP Tahoe is a congestion control scheme that was invented back in the 80s, when congestion was first becoming a problem on the internet. The algorithm itself is fairly simple, and grows the congestion window in two phases.\nPhase 1 # Slow Start: The algorithm begins in a state called “slow start”. In Slow Start, the congestion window grows by 1 every time an acknowledgement is received. This effectively doubles the congestion window on every round trip. If the congestion window is 4, four packets will be in flight at once, and when each of those packets is acknowledged, the congestion window will increase by 1, resulting in a window of size 8. This process continues until the congestion window hits a value called the “Slow Start Threshold” ssthresh. This is a configurable number.\nPhase 2 # Congestion Avoidance: Once the congestion window has hit the ssthresh, it moves from “slow start” into congestion avoidance mode. In congestion avoidance, the congestion window increases by 1 on every round trip. So if the congestion window is 4, the window will increase to 5 after all four of those packets in flight have been acknowledged. This increases the window much more slowly.\nIf Tahoe detects that a packet is lost, it will resend the packet, the slow start threshold is updated to be half the current congestion window, the congestion window is set back to 1, and the algorithm goes back to slow start.\nDetecting Packet Loss \u0026amp; Fast Retransmit # There are two ways that a TCP sender could detect that a packet is lost.\nThe sender “times out”. The sender puts a timeout on every packet that is sent out into the wild, and when that timeout is hit without that packet having been acknowledged, it resends the packet and sets the congestion window to 1.\nThe receiver sends back “duplicate acks”. In TCP, receivers only acknowledge packets that are sent in order. If a packet is sent out of order, it will send out an acknowledgement for the last packet it saw in order. So, if a receiver has received segments 1,2, and 3, and then receives segment #5, it will ack segment #3 again, because #5 came in out of order. In Tahoe, if a sender receives 3 duplicate acks, it considers a packet lost. This is considered “Fast Retransmit”, because it doesn’t wait for the timeout to happen.\nThere are a number of issues with this approach though, which is why it is no longer used today. In particular, it takes a really long time, especially on higher bandwidth networks, for the algorithm to actually take full advantage of the available bandwidth. This is because the window size grows pretty slowly after hitting the slow start threshold. Another issue is that packet loss doesn’t necessarily mean that congestion is occuring–some links, like WiFi, are just inherently lossy. Reacting drastically by cutting the window size to 1 isn’t necessarily always appropriate. A final issue is that this algorithm uses packet loss as the indicator for whether there’s congestion. If the packet loss is happening due to congestion, you are already too late–the window is too high, and you need to let the queues drain.\nTCP CUBIC # This algorithm was implemented in 2005, and is currently the default congestion control algorithm used on Linux systems. Like Tahoe, it relies on packet loss as the indicator of congestion. However, unlike Tahoe, it works far better on high bandwidth networks, since rather than increasing the window by 1 on every round trip, it uses, as the name would suggest, a cubic function to determine what the window size should be, and therefore grows much more quickly.\nAvoidance-Based Algorithms # BBR(Bottleneck Bandwidth and RTT) - (Bufferbloat) # This is a very recent algorithm developed by Google, and unlike Tahoe or CUBIC, uses delay as the indicator of congestion, rather than packet loss. The rough thinking behind this is that delays are a leading indicator of congestion–they occur before packets actually start getting lost. Slowing down the rate of sending before the packets get lost ends up leading to higher throughput.\nActive Queue Management # Random Early Detection # Each router is programmed to monitor its own queue length and, when it detects that congestion is imminent, to notify the source to adjust its congestion window. RED, invented by Sally Floyd and Van Jacobson in the early 1990s.\nRED is most commonly implemented such that it implicitly notifies the source of congestion by dropping one of its packets. The source is, therefore, effectively notified by the subsequent timeout or duplicate ACK. In case you haven’t already guessed, RED is designed to be used in conjunction with TCP, which currently detects congestion by means of timeouts (or some other means of detecting packet loss such as duplicate ACKs). As the “early” part of the RED acronym suggests, the gateway drops the packet earlier than it would have to, so as to notify the source that it should decrease its congestion window sooner than it would normally have. In other words, the router drops a few packets before it has exhausted its buffer space completely, so as to cause the source to slow down, with the hope that this will mean it does not have to drop lots of packets later on.\nhow RED decides when to drop a packet and what packet it decides to drop. To understand the basic idea, consider a simple FIFO queue. Rather than wait for the queue to become completely full and then be forced to drop each arriving packet (the tail drop policy), we could decide to drop each arriving packet with some drop probability whenever the queue length exceeds some drop level. This idea is called early random drop. The RED algorithm defines the details of how to monitor the queue length and when to drop a packet.\nExplicit Congestion Notification || IP \u0026amp; TCP Flags # While TCP’s congestion control mechanism was initially based on packet loss as the primary congestion signal, it has long been recognized that TCP could do a better job if routers were to send a more explicit congestion signal. That is, instead of dropping a packet and assuming TCP will eventually notice (e.g., due to the arrival of a duplicate ACK), any AQM algorithm can potentially do a better job if it instead marks the packet and continues to send it along its way to the destination. This idea was codified in changes to the IP and TCP headers known as Explicit Congestion Notification (ECN), as specified in RFC 3168.\nSpecifically, this feedback is implemented by treating two bits in the IP TOS field as ECN bits. One bit is set by the source to indicate that it is ECN-capable, that is, able to react to a congestion notification. This is called the ECT bit (ECN-Capable Transport). The other bit is set by routers along the end-to-end path when congestion is encountered, as computed by whatever AQM algorithm it is running. This is called the CE bit (Congestion Encountered).\nIn addition to these two bits in the IP header (which are transport-agnostic), ECN also includes the addition of two optional flags to the TCP header. The first, ECE (ECN-Echo/Experienced), communicates from the receiver to the sender that it has received a packet with the CE bit set. The second, CWR (Congestion Window Reduced) communicates from the sender to the receiver that it has reduced the congestion window.\nBeyond TCP # Datacenters (DCTCP) # There have been several efforts to optimize TCP for cloud datacenters, where Data Center TCP was one of the first. There are several aspects of the datacenter environment that warrant an approach that differs from more traditional TCP. These include:\nRound trip time for intra-DC traffic are small; Buffers in datacenter switches are also typically small; All the switches are under common administrative control, and thus can be required to meet certain standards; A great deal of traffic has low latency requirements; That traffic competes with high bandwidth flows. It should be noted that DCTCP is not just a version of TCP, but rather, a system design that changes both the switch behavior and the end host response to congestion information received from switches.\nThe central insight in DCTCP is that using loss as the main signal of congestion in the datacenter environment is insufficient. By the time a queue has built up enough to overflow, low latency traffic is already failing to meet its deadlines, negatively impacting performance. Thus DCTCP uses a version of ECN to provide an early signal of congestion. But whereas the original design of ECN treated an ECN marking much like a dropped packet, and cut the congestion window in half, DCTCP takes a more finely-tuned approach. DCTCP tries to estimate the fraction of bytes that are encountering congestion rather than making the simple binary decision that congestion is present. It then scales the congestion window based on this estimate. The standard TCP algorithm still kicks in should a packet actually be lost. The approach is designed to keep queues short by reacting early to congestion while not over-reacting to the point that they run empty and sacrifice throughput.\nThe key challenge in this approach is to estimate the fraction of bytes encountering congestion. Each switch is simple. If a packet arrives and the switch sees the queue length (K) is above some threshold; e.g., K \u0026gt; (RTT * C) / 7\nwhere C is the link rate in packets per second, then the switch sets the CE bit in the IP header. The complexity of RED is not required.\nThe receiver then maintains a boolean variable for every flow, which we’ll denote DCTCP.CE, and sets it initially to false. When sending an ACK, the receiver sets the ECE (Echo Congestion Experienced) flag in the TCP header if and only if DCTCP.CE is true. It also implements the following state machine in response to every received packet:\nIf the CE bit is set and DCTCP.CE=False, set DCTCP.CE to True and send an immediate ACK.\nIf the CE bit is not set and DCTCP.CE=True, set DCTCP.CE to False and send an immediate ACK.\nOtherwise, ignore the CE bit.\nHTTP Performance (QUIC) # HTTP has been around since the invention of the World Wide Web in the 1990s and from its inception it has run over TCP. HTTP/1.0, the original version, had quite a number of performance problems due to the way it used TCP, such as the fact that every request for an object required a new TCP connection to be set up and then closed after the reply was returned. HTTP/1.1 was proposed at an early stage to make better use of TCP. TCP continued to be the protocol used by HTTP for another twenty-plus years.\nIn fact, TCP continued to be problematic as a protocol to support the Web, especially because a reliable, ordered byte stream isn’t exactly the right model for Web traffic. In particular, since most web pages contain many objects, it makes sense to be able to request many objects in parallel, but TCP only provides a single byte stream. If one packet is lost, TCP waits for its retransmission and successful delivery before continuing, while HTTP would have been happy to receive other objects that were not affected by that single lost packet. Opening multiple TCP connections would appear to be a solution to this, but that has its own set of drawbacks including a lack of shared information about congestion across connections.\nOther factors such as the rise of high-latency wireless networks, the availability of multiple networks for a single device (e.g., Wi-Fi and cellular), and the increasing use of encrypted, authenticated connections on the Web also contributed to the realization that the transport layer for HTTP would benefit from a new approach. The protocol that emerged to fill this need was QUIC.\nQUIC originated at Google in 2012 and was subsequently developed as a proposed standard at the IETF. It has already seen a solid amount of deployment—it is in most Web browsers, many popular websites, and is even starting to be used for non-HTTP applications. Deployability was a key consideration for the designers of the protocol. There are a lot of moving parts to QUIC—its specification spans three RFCs of several hundred pages—but we focus here on its approach to congestion control, which embraces many of the ideas we have seen to date in this book.\nLike TCP, QUIC builds congestion control into the transport, but it does so in a way that recognizes that there is no single perfect congestion control algorithm. Instead, there is an assumption that different senders may use different algorithms. The baseline algorithm in the QUIC specification is similar to TCP NewReno, but a sender can unilaterally choose a different algorithm to use, such as CUBIC. QUIC provides all the machinery to detect lost packets in support of various congestion control algorithms.\nMultipath Transport # While the early hosts connected to the Internet had only a single network interface, it is common these days to have interfaces to at least two different networks on a device. The most common example is a mobile phone with both cellular and WiFi interfaces. Another example is datacenters, which often allocate multiple network interfaces to servers to improve fault tolerance. Many applications use only one of the available networks at a time, but the potential exists to improve performance by using multiple interfaces simultaneously. This idea of multipath communication has been around for decades and led to a body of work at the IETF to standardize extensions to TCP to support end-to-end connections that leverage multiple paths between pairs of hosts. This is known as Multipath TCP (MPTCP).\nA pair of hosts sending traffic over two or more paths simultaneously has implications for congestion control. For example, if both paths share a common bottleneck link, then a naive implementation of one TCP connection per path would acquire twice as much share of the bottleneck bandwidth as a standard TCP connection. The designers of MPTCP set out to address this potential unfairness while also realizing the benefits of multiple paths. The proposed congestion control approach could equally be applied to other transports such as QUIC.\nReferences:\nhttps://tcpcc.systemsapproach.org/aqm.html https://squidarth.com/rc/programming/networking/2018/07/18/intro-congestion "},{"id":29,"href":"/tech-book/docs/networking-tips/tcp-data-transfer/","title":"TCP Data Transfer","section":"Networking Tips","content":" Intro # TCP Connection Establishment Handshake # Sync The initiator that is establishing a connection with a target generates a random sequence number (5,045 for this example) and sends a TCP packet with its sync flag set to 1 and its sequenceNumber set to the just-defined sequence number. Sync/Ack Upon receipt of the TCP Sync packet from the initiator, the target sets its ack number value to the received sequenceNumber + 1 (5,046 in this example). The target responds by setting its own sequence number to a random value (17,764 in this example) and sending a TCP packet whose sync and ack flags are both set to 1 and whose sequenceNumber is set to the just-defined sequence number value and whose ackNumber is set to the target’s just-set ack number. Ack Upon receipt of the TCP Sync/Ack packet, the initiator sets its ack number to the received sequenceNumber + 1. The initiator then sends a TCP Ack packet to the target whose ack flag is set to 1 and whose sequenceNumber and ackNumber are set to the initiator’s corresponding internal values. TCP Connection Termination Handshake # The steps in a four-way TCP connection termination handshake are thus.\nInitiator Finish One side of the connection (the initiator) sends a TCP packet to its connection partner whose finish bit is set to 1. Target Ack or Finish/Ack The target of the initial termination message responds by sending a TCP packet to the initiator whose ack bit is set to 1. This acknowledges that the initiator-to-target data connection is closed and that the initiator will no longer send TCP data packets to the target. The target responds by sending two TCP packets to the initiator: one whose ack bit is set to 1 and one whose finish bit is set to 1. If the target has no more data to send to the initiator, it may combine these two TCP packets into a single TCP packet where both ack and finish are set to 1. If the target does have more data to send, the connection may remain “half open” until the data transmissions are complete. Initiator Ack Finally the initiator sends a TCP packet whose ack bit is set to 1 to complete the connection termination process. Reliable Data Delivery # For a data connection to be reliable, three types of problems must be detected and corrected. These are: * out-of-order data * missing data * duplicated data\nTCP’s sequence numbers address all three of these problems.\nTCP Sequence Number Behavior # Briefly: The initial sequence number is chosen randomly. As the TCP sender transmits bytes to the receiver, each TCP packet includes a sequenceNumber value that indicates the cumulative sequence number of the last byte of each packet. The TCP receiver responds by transmitting TCP Ack packets whose ackNumber value identifies the highest-numbered byte received without any gaps from the first byte of the TCP session. Sequence numbers roll over through zero upon exceeding the maximum value that a 32-bit number can hold.\nTCP Data Reorder # You can see the presence of sequenceNumber in each packet means that a TCP receiver can position each received packet within a TCP reorder buffer where the original sequence of data bytes may be reassembled. Let’s consider a simple scenario where the initial sequence number is 0. If the first packet received by the TCP receiver has a sequenceNumber value of 1,000 and the packet has a TCP data payload of 1,000 bytes, then this packet neatly fits within the buffer at offset 0. The next packet received by the TCP receiver has a sequenceNumber value of 3,000 and carries 1,000 bytes of TCP data. The receiver subtracts the data byte count from the received sequenceNumber value to arrive at a sequence number of 2,000 for this packet’s first data byte. Thus, the data is written to the buffer starting at offset 2,000, leaving a 1,000-byte gap between the first and second received packets. Finally, a TCP packet with 1,000 data bytes and a sequenceNumber of 2,000 is received, meaning the sequence number of the first byte of this packet is 1,000. This third packet’s data is written into the 1,000-byte gap between the first two packets’ data, completing the in-order data transfer. Missing Data Retransmission # Using the data reordering example from above as a starting point, let’s assume that the TCP packet whose sequenceNumber value is 2,000 wasn’t simply delivered out of order, but was lost due to any of a variety of the usual things that happen on networks. From the TCP receiver’s perspective, the order in which the TCP data packets are received and its responses to the data packets are the same regardless of whether a packet was lost in transmission (and later retransmitted) or simply delivered out of order by the network. In response to the receipt of the first TCP data packet, the TCP receiver transmits a TCP Ack packet back to the sender with an ackNumber value of 1,000, indicating that is has successfully received bytes 0 through 999. The second TCP data packet received by the TCP receiver indicates that a gap exists from byte 1,000 through 1,999. To inform the TCP sender of this, the TCP receiver transmits another TCP Ack packet with the exact same ackNumber value of 1,000. This informs the TCP sender that, while the first 1,000 bytes have been successfully received, the next packet (and possibly more) is currently missing. This duplicate acknowledge (the sender has already received an acknowledge for the first 1,000 bytes) motivates the sender to retransmit the first unacknowledged packet (i.e., the packet whose sequenceNumber is 2,000). The TCP sender, in turn, keeps track of the TCP Ack messages that have been received and maintains an awareness of the highest-numbered data byte that’s been successfully transmitted. Every unacknowledged byte is subject to retransmission. Re-transmissions and acknowledges may also get lost. Hence, TCP maintains timers to detect these cases and automatically retransmit either data or Ack packets as necessary to keep the process moving along.\nDuplicate Data Discard # Duplicate data may be received by a TCP receiver if a packet that is assumed missing is simply delayed and delivered out of order. The TCP receiver may request another copy of the packet through the use of a TCP Ack message before the delayed data packet is received. If both copies of the data packet are eventually successfully received, then the receiver must deal with redundant data. If a TCP receiver receives a TCP data packet whose sequenceNumber corresponds to a data packet that had previously been received (regardless of whether or not it had been acknowledged), the TCP receiver may either discard the packet or simply overwrite the data in its receive buffer with the newly received data. Since the data is the same, the data in the buffer does not change.\nSelective Acknowledge # If a TCP data packet is lost before arriving at a receiver, but that missing packet is followed by several successfully received TCP data packets, the receiver has no effective means for alerting the sender to just retransmit the single missing packet. Selective acknowledgment (defined by RFC 2018 and known colloquially as SACK) is an optional extension to TCP that addresses this specific, common scenario.\nReferences:\n"},{"id":30,"href":"/tech-book/docs/ai-ml-dc/2-ai-deep-learning-basics/","title":"Deep Learning Basics | Artificial Neuron","section":"Data Center Networking for AI Clusters","content":" Content # Introduction Artificial Neuron Weighted Sum for Pre-Activation Value ReLU Activation Function for Post-Activation Bias Term S-Shaped Functions – TANH and SIGMOID Network Impact Summary Introduction # Artificial Intelligence (AI) is a broad term for solutions that aim to mimic the functions of the human brain. Machine Learning (ML), in turn, is a subset of AI, suitable for tasks like simple pattern recognition and prediction. Deep Learning (DL), the focus of this section, is a subset of ML that leverages algorithms to extract meaningful patterns from data. Unlike ML, DL does not necessarily require human intervention, such as providing structured, labeled datasets (e.g., 1,000 bird images labeled as “bird” and 1,000 cat images labeled as “cat”). DL utilizes layered, hierarchical Deep Neural Networks (DNNs), where hidden and output layers consist of computational units, artificial neurons, which individually process input data. The nodes in the input layer pass the input data to the first hidden layer without performing any computations, which is why they are not considered neurons or computational units. Each neuron calculates a pre-activation value (z) based on the input received from the previous layer and then applies an activation function to this value, producing a post-activation output (ŷ) value. There are various DNN models, such as Feed-Forward Neural Networks (FNN), Convolutional Neural Networks (CNN), and Recurrent Neural Networks (RNN), each designed for different use cases. For example, FNNs are suitable for simple, structured tasks like handwritten digit recognition using the MNIST dataset [1], CNNs are effective for larger image recognition tasks such as with the CIFAR-10 dataset [2], and RNNs are commonly used for time-series forecasting, like predicting future sales based on historical sales data. To provide accurate predictions based on input data, neural networks are trained using labeled datasets. The MNIST (Modified National Institute of Standards and Technology) dataset [1] contains 60,000 training and 10,000 test images of handwritten digits (grayscale, 28x28 pixels). The CIFAR-10 [2] dataset consists of 60,000 color images (32x32 pixels), with 50,000 training images and 10,000 test images, divided into 10 classes. The CIFAR-100 dataset [3], as the name implies, has 100 image classes, with each class containing 600 images (500 training and 100 test images per class). Once the test results reach the desired level, the neural network can be deployed to production.\nFigure 1-1: Deep Learning Introduction.\nArtificial Neuron # As we saw in the previous section, DNNs leverage a hierarchical, layered, role-based (input layer, n hidden layers, and output layer) network model, where each layer consists of artificial neurons. Neurons in different layers may be fully or partially connected to neurons in the other layers, depending on the DNN’s network model. However, neurons within the same layer are not connected.\nThe logical structure and functionality of an artificial neuron aim to emulate those of a biological neuron. A biological neuron transmits electrical signals to its peer neuron via the output axon to the axon terminal, which connects to the dendrites of the peer neuron at a connection point called a synapse. A biological neuron may receive signals from multiple dendrites, and if the signal is strong enough, it activates the neuron, causing the cell body to trigger a signal through its output axon, which connects to another neuron, and the process continues.\nAn artificial neuron, as a computational unit, calculates the weighted sum (z = pre-activation value) of the input (x) received from the previous layer, adds a bias value (b), and applies an activation function to produce the output (ŷ = post-activation value). The weight value for the input can be loosely compared to a synapse since it represents a connection, input values are assigned with weight. The weights-to-neuron association, in turn, can be seen as analogous to dendrites. The computational processes (weighted sum of inputs, bias addition, and activation functions) represent the cell body, while the connections to other neurons can be compared to output axons. In Figure 1-2, bn refers to a biological neuron. From now on, \u0026ldquo;neuron\u0026rdquo; will refer to an artificial neuron.\nFigure 1-2: Construct of an Artificial Neuron.\nWeighted Sum for Pre-Activation Value # The lower part of Figure 1-2 depicts the mathematical formulas of a neuron. The pre-activation value z is the weighted sum of the inputs. Although the bias is not part of the input data, a neuron treats it as an input variable when calculating the weighted sum. Each input x has a corresponding weight w. The calculation process is straightforward: each input value is multiplied by its corresponding weight, and the results are summed to obtain the weighted sum z.\nThe capital Greek letter sigma ∑ in the formula indicates that we are summing a series of terms, which, in this case, are the input values multiplied by their respective weights. The letter n specifies how many terms are being summed (four pairs of inputs and weights), while the letter iii denotes the starting point of the summation (bias b0 and weight w0). The equation for the weighted sum is:\nz = b0w0 + x1w1 + x2w2 + x3w3. ReLU Activation Function for Post-Activation # Next, the process applies an activation function, ReLU (Rectified Linear Unit) [4] in our example, to the weighted sum z obtain the post-activation value ŷ. The output of the ReLU function is z if z is greater than zero (0); otherwise, the result is zero (0). This can be written as: ŷ = MAX (0, z) which selects the larger value between 0 and variable z. The figure 1-3 depicts the ReLU activation function.\nFigure 1-3: Construct of an Artificial Neuron.\nBased on the figure 1-3 we can use the mathematical definition below for ReLU:\nBias Term # In the example calculation above, imagine that all input values are zero. Without a bias term, the activation value will be zero, regardless of how large the weight parameters are. Therefore, the bias term allows the neuron to produce non-zero outputs, even when all input values are zero.\nS-Shaped Functions – TANH and SIGMOID # The ReLU function is a non-linear activation function. Naturally, there are other activation functions as well. The Hyperbolic Tangent (tanh) [5] and the logistic Sigmoid [6] functions are examples of S-shaped functions that are symmetric around zero. Figure 1-4 illustrates that as the positive z value increases, the tanh function approaches one (1), while as the negative z value decreases, it approaches -1. Thus, the range of the tanh function is from -1 to 1. Similarly, the sigmoid function\u0026rsquo;s S-curve is also symmetric around zero, but its range is from 0 to 1.\nFigure 1-4: Tanh and Sigmoid functions.\nHere are examples of both functions using the same pre-activation value of 3.02, as in the ReLU activation function example.\nNote, the ⅇ represents Euler’s Number ⅇ≈ 2.718. The symbol σ represents the sigmoid function.\nThe formula for tanh function is:\nThe tanh function for z = 3,02\nThe formula for sigmoid function is:\nThe sigmoid function for z = 3,02\nThe symbol σ represents sigmoid function.\nFor z=3.02, both the sigmoid and tanh functions return values close to 1, but the tanh function is slightly higher, approaching its maximum of 1 more quickly than the sigmoid.\nWhich activation function should we use? The ReLU function has largely replaced the tanh and sigmoid functions due to its simplicity, which reduces the required computation cycles. However, if tanh and sigmoid are used, tanh is typically applied in the hidden layers, while sigmoid is used in the output layer.\nBackpropagation Algorithm: Introduction # This chapter introduces the training model of a neural network based on the Backpropagation algorithm. The goal is to provide a clear and solid understanding of the process without delving deeply into the mathematical formulas, while still explaining the fundamental operations of the involved functions. The chapter also briefly explains why, and in which phases the training job generates traffic to the network, and why lossless packet transport is required. The Backpropagation algorithm is composed of two phases: the Forward pass (computation phase) and the Backward pass (adjustment and communication phase).\nIn the Forward pass, neurons in the first hidden layer calculate the weighted sum of input parameters received from the input layer, which is then passed to the neuron\u0026rsquo;s activation function. Note that neurons in the input layer are not computational units; they simply pass the input variables to the connected neurons in the first hidden layer. The output from the activation function of a neuron is then used as input for the connected neurons in the next layer, whether it is another hidden layer or the output layer. The result of the activation function in the output layer represents the model\u0026rsquo;s prediction, which is compared to the expected value (ground truth) using the error function. The output of the error function indicates the accuracy of the current training iteration. If the result is sufficiently close to the expected value, the training is complete. Otherwise, it triggers the Backward pass process.\nIn the Backward pass process, the Backpropagation algorithm first calculates the derivative of the error function. This derivative is then used to compute the error term for each neuron in the model. Neurons use their calculated error terms to determine how much and in which direction the current weight values must be fine-tuned. Depending on the model and the parallelization strategy, GPUs in multi-GPU clusters synchronize information during the Backpropagation process. This process affects network utilization.\nOur feedforward neural network, shown in Figure 2-1, has one hidden layer and one output layer. If we wanted our example to be a deep neural network, we would need to add additional layers, as the definition of \u0026ldquo;deep\u0026rdquo; requires two or more hidden layers. For simplicity, the input layer is not shown in the figure.\nWe have three input parameters connected to neuron-a in the hidden layer as follows:\nInput X1 = 0.2 \u0026gt; neuron-a via weight Wa1 = 0.1 Input X2 = 0.1 \u0026gt; neuron-a via weight Wa2 = 0.2 Input X3 = 0.4 \u0026gt; neuron-a via weight Wa3 = 0.3 Bias ba0 = 1.0 \u0026gt; neuron-a via weight Wa0 = 0.6 The bias term helps ensure that the neuron is active, meaning its output value is not zero.\nThe input parameters are treated as constant values, while the weight values are variables that will be adjusted during the Backward pass if the training result does not meet expectations. The initial weight values are our best guess for achieving the desired training outcome. The result of the weighted sum calculation is passed to the activation function, which provides the input for neuron-b in the output layer. We use the ReLU (Rectified Linear Unit) activation function in both layers due to its simplicity. There are other activation functions, such as hyperbolic tangent (tanh), sigmoid, and softmax, but those are outside the scope of this chapter.\nThe input values and weights for neuron-b are:\nNeuron-a activation function output f(af) \u0026gt; neuron-b via weight Wb1 Bias ba0 = 1.0 \u0026gt; neuron-b via weight Wa0 = 0.5 The output, Ŷ, from neuron-b represents our feedforward neural network\u0026rsquo;s prediction. This value is used along with the expected result, y, as input for the error function. In this example, we use the Mean Squared Error (MSE) error function. As we will see, the result of the first training iteration does not match our expected value, leading us to initiate the Backward pass process.\nIn the first step of the Backward pass, the Backpropagation algorithm calculates the derivative of the error function (MSE’). Neurons-a and b use this result as input to compute their respective error terms by multiplying MSE’ with the result of the activation function and the weight value associated with the connection to the next neuron. Note that for neuron-b, there is no next layer—just the error function—so the weight parameter is excluded from the error term calculation of neuron-b. Next, the error term value is multiplied by an input value and learning rate, and this adjustment value is added to the current weight.\nAfter completing the Backward pass, the Backpropagation algorithm starts a new iteration of the Forward pass, gradually improving the model\u0026rsquo;s prediction until it closely matches the expected value, at which point the training is complete.\nFigure 2-1: Backpropagation Algorithm.\nIntroduction # This chapter introduces the training model of a neural network based on the Backpropagation algorithm. The goal is to provide a clear and solid understanding of the process without delving deeply into the mathematical formulas, while still explaining the fundamental operations of the involved functions. The chapter also briefly explains why, and in which phases the training job generates traffic to the network, and why lossless packet transport is required. The Backpropagation algorithm is composed of two phases: the Forward pass (computation phase) and the Backward pass (adjustment and communication phase).\nIn the Forward pass, neurons in the first hidden layer calculate the weighted sum of input parameters received from the input layer, which is then passed to the neuron\u0026rsquo;s activation function. Note that neurons in the input layer are not computational units; they simply pass the input variables to the connected neurons in the first hidden layer. The output from the activation function of a neuron is then used as input for the connected neurons in the next layer. The result of the activation function in the output layer represents the model\u0026rsquo;s prediction, which is compared to the expected value (ground truth) using the error function. The output of the error function indicates the accuracy of the current training iteration. If the result is sufficiently close to the expected value (error function close to zero), the training is complete. Otherwise, it triggers the Backward pass process.\nAs the first step in the backward pass, the backpropagation algorithm calculates the derivative of the error function, providing the output error (gradient) of the model. Next, the algorithm computes the error term (gradient) for the neuron(s) in the output layer by multiplying the derivative of each neuron’s activation function by the model\u0026rsquo;s error term. Then, the algorithm moves to the preceding layer and calculates the error term (gradient) for its neuron(s). This error term is now calculated using the error term of the connected neuron(s) in the next layer, the derivative of each neuron’s activation function, and the value of the weight parameter associated with the connection to the next layer.\nAfter calculating the error terms, the algorithm determines the weight adjustment values for all neurons simultaneously. This computation is based on the input values, the adjustment values, and a user-defined learning rate. Finally, the backpropagation algorithm refines all weight values by adding the adjustment values to the initial weights. Once the backward pass is complete, the backpropagation algorithm starts a new iteration of the forward pass, gradually improving the model\u0026rsquo;s predictions until they closely match the expected values, at which point the training is complete.\nFigure 2-1: Backpropagation Overview.\nThe First Iteration - Forward Pass # Training a model often requires multiple iterations of forward and backward passes. In the forward pass, neurons in the first hidden layer calculate the weighted sum of input values, each multiplied by its associated weight parameter. These neurons then apply an activation function to the result. Neurons in subsequent layers use the activation output from previous layers as input for their own weighted sum calculations. This process continues through all the layers until reaching the output layer, where the activation function produces the model\u0026rsquo;s prediction.\nAfter the forward pass, the backpropagation algorithm calculates the error by comparing the model\u0026rsquo;s output with the expected value, providing a measure of accuracy. If the model\u0026rsquo;s output is close to the expected value, training is complete. Otherwise, the backpropagation algorithm initiates the backward pass to adjust the weights and reduce the error in subsequent iterations.\nNeuron-a Forward Pass Calculations # Weighted Sum # In Figure 2-2, we have an imaginary training dataset with three inputs and a bias term. Input values and their respective initial weight values are listed below: x1 = 0.2 , initial weight wa1 = 0.1 x2 = 0.1, initial weight wa2 = 0.2 x3 = 0.4 , initial weight wa3 = 0.3 ba0 = 1.0 , initial weight wa0 = 0.6 From the model training perspective, the input values are constant, unchageable values, while weight values are variables which will be refined during the backward pass process.\nThe standard way to write the weighted sum formula is: Where:\nn = 3 represents the number of input values (x1, x2, x3). Each input xi is multiplied by its respective weight wi, and the sum of these products is added to the bias term b. In this case, the equation can be explicitly stated as:\nWhich with our parameters gives:\nActivation Function # Neuron-a uses the previously calculated weighted sum as input for the activation function. We are using the ReLU function (Rectified Linear Unit), which is more popular than the hyperbolic tangent and sigmoid functions due to its simplicity and lower computational cost.\nThe standard way to write the ReLU function is:\nWhere:\nf(a) represents the activation function. z is the weighted sum of inputs.\nThe ReLU function returns the z if z \u0026gt; 0. Otherwise, it returns 0 if z ≤ 0.\nIn our example, the weighted sum za is 0.76, so the ReLU function returns:\nFigure 2-2: Activation Function for Neuron-a.\nNeuron-b Forward Pass Calculations # Weighted Sum # Besides the bias term value of 1.0, Neuron-b uses the result provided by the activation function of neuron-a as an input to weighted sum calculation. Input values and their respective initial weight values are listed below: This gives us:\nActivation Function # Just like Neuron-a, Neuron-b uses the previously calculated weighted sum as input for the activation function. Because the zb = 0.804 is greater than zero, the ReLU activation function f(b) returns:\nNeuron-b is in the output layer, so its activation function result y_b_ represents the prediction of the model. Figure 2-3: Activation Function for Neuron-b.\nError Function # To keep things simple, we have used only one training example. However, in real-life scenarios, there will always be more than one training example. For instance, a training dataset might contain 10 images of cats and 10 images of dogs, each having 28x28 pixels. Each image represents a training example, giving us a total of 20 training examples. The purpose of the error function is to provide a single error metric for all training examples. In this case, we are using the Mean Squared Error (MSE).\nWe can calculate the MSE using the formula below where the expected value y is 1.0 and the model’s prediction for the training example yb = 0.804. This gives an error metric of 0.019, which can be interpreted as an indicator of the model\u0026rsquo;s accuracy.\nThe result of the error function is not sufficiently close to the desired value, which is why this result triggers the backward pass process.\nFigure 2-4: Calculating the Error Function for Training Examples.\nBackward Pass # In the forward pass, we calculate the model’s accuracy using several functions. First, Neuron-a computes the weighted sum Σ(za ) by multiplying the input values and the bias term with their respective weights. The output, za, is then passed through the activation function f(a), producing ya. Neuron-b, in turn, takes ya and the bias term to calculate the weighted sum Σ(zb ). The activation function f(b) then uses zb to compute the model’s output, yb. Finally, the error function f(e) calculates the model’s accuracy based on the output.\nSo, dependencies between function can be seen as:\nThe backpropagation algorithm combines these five functions to create a new error function, enew(x), using function composition and the chain rule. The following expression shows how the error function relates to the weight parameter w1 used by Neuron-a:\nThis can be expressed using the composition operator (∘) between functions:\nNext, we use a method called gradient descent to gradually adjust the initial weight values, refining them to bring the model\u0026rsquo;s output closer to the expected result. To do this, we compute the derivative of the composite function using the chain rule, where we take the derivatives of:\nThe error function (e) with respect to the activation function (b). The activation function b with respect to the weighted sum (zb). The weighted sum (zb) with respect to the activation function (a). The activation function (a) with respect to weighted sum (za(w1)). In Leibniz’s notation, this looks like:\nFigure 2-5 illustrates the components of the backpropagation algorithm, along with their relationships and dependencies.\nFigure 2-5: The Backward Pass Overview.\nPartial Derivative for Error Function – Output Error (Gradient) # As a recap, and for illustrating that the prediction of the first iteration fails, Figure 2-6 includes the computation for the error function (MSE = 0.019). As a first step, we calculate the partial derivative of the error function. In this case, the partial derivative describes the rate of change of the error function when the input variable yb changes. The derivative is called partial when one of its input values is held constant (i.e., not adjusted by the algorithm). In our example, the expected value y is constant input. The result of the partial derivative of the error function describes how the predicted output should change yb to minimize the model’s error.\nWe use the following formula for computing the derivative of the error function:\nFigure 2-6: The Backward Pass – Derivative of the Error Function.\nThe following explanation is meant for readers interested in why there is a minus sign in front of the function.\nWhen calculating the derivative, we use the Power Rule. The Power Rule states that if we have a function f(x) = xn , then its derivative is f’(x) = n ⋅ xn-1. In our case, this applies to the error function:\nUsing the Power Rule, the derivative becomes:\nNext, we apply the chain rule by multiplying this result by the derivative of the inner function (y − yb), with respect to yb . Since y is treated as a constant (because it represents our target value, which doesn\u0026rsquo;t change during optimization), the derivative of (y − yb) with respect to yb is simply −1, as the derivative of − yb with respect to yb is −1, and the derivative of y (a constant) is 0.\nTherefore, the final derivative of the error function with respect to yb is:\nPartial Derivative for the Activation Function # After computing the output error, we calculate the derivative of the activation function f(b) with respect to zb . Neuron b uses the ReLU activation function, which states that if the input to the function is greater than 0, the derivative is 1; otherwise, it is 0. In our case, the result of the activation function f(b)=0.804, so the derivative is 1.\nError Term for Neurons (Gradient) # The error term (Gradient) for neuron-b is calculated by multiplying the output error, the partial derivative of the error function, by the derivative of the neuron\u0026rsquo;s activation function. This means that now we propagate the model\u0026rsquo;s error backward using it as a base value for finetuning the model accuracy (i.e., refining new weight values). This is why the term backward pass fits perfectly for the process.\nFigure 2-7: The Backward Pass – Error Term (Gradient) for Neuron-b.\nAfter computing the error term for Neuron-b, the backward pass moves to the preceding layer, the hidden layer, and calculates the error term for Neuron-a. The algorithm computes the derivative for the activation function f(a) = 1, as it did with the Neuron-b. Next, it multiplies the result by Neuron-b\u0026rsquo;s error term (-0.196) and the connected weight parameter , wb1 =0.4. The result -0.0784 is the error term for Neuron-a.\nFigure 2-8: The Backward Pass – Error Term (Gradient) for Neuron-a.\nWeight Adjustment Value # After computing error terms for all neurons in every layer, the algorithm simultaneously calculates the weight adjustment value for every weight. The process is simple, the error term is multiplied with an input value connected to weight and with learning rate (η). The learning rate balances convergence speed and training stability. We have set it to -0.6 for the first iteration. The learning rate is a hyper-parameter, meaning it is set by the user rather than learned by the model during training. It affects the behavior of the backpropagation algorithm by controlling the size of the weight updates. It is also possible to adjust the learning rate during training—using a higher learning rate at the start to allow faster convergence and lowering it later to avoid overshooting the optimal result. Weight adjustment value for weight wb1 and wa1 respectively:\nNote! It is not recommended to use a negative learning rate. I use it here because we get a good enough output for the second forward pass iteration.\nFigure 2-9: The Backward Pass – Weight Adjustment Value for Neurons.\nRefine Weights # As the last step, the backpropagation algorithm computes new values for every weight parameter in the model by simply summing the initial weight value and weight adjustment value.\nNew values for weight parameters wb1 and wa1 respectively:\nFigure 2-10: The Backward Pass – Compute New Weight Values.\nThe Second Iteration - Forward Pass # After updating all the weight values (wa0, wa1, wa2, and wa3 ), the backpropagation process begins the second iteration of the forward pass. As shown in Figure 2-11, the model output yb = 0.9982 is very close to the expected value y = 1.0. The new MSE = 0.0017, is much better than 0.019 computed in the first iteration.\nFigure 2-11: The Second Iteration of the Forward Pass.\nNetwork Impact # Figure 2-12 shows a hypothetical example of Data Parallelization, where our training data set is split into two batches, A and B, which are processed by GPU-A and GPU-B, respectively. The training model is the same on both GPUs: Fully-Connected, with two hidden layers of four neurons each, and one output neuron in the output layer.\nAfter computing a model prediction during the forward pass, the backpropagation algorithm begins the backward pass by calculating the gradient (output error) for the error function. Once computed, the gradients are synchronized between the GPUs. The algorithm then averages the gradients, and the process moves to the preceding layer. Neurons in the preceding layer calculate their gradient by multiplying the weighted sum of their connected neurons’ averaged gradients and connected weight with the local activation function’s partial derivative. These neuron-based gradients are then synchronized over connections. Before the process moves to the preceding layer, gradients are averaged. The backpropagation algorithm executes the same process through all layers. If packet loss occurs during the synchronization, it can ruin the entire training process, which would need to be restarted unless snapshots were taken. The cost of losing even a single packet could be enormous, especially if training has been ongoing for several days or weeks. Why is a single packet so important? If the synchronization between the gradients of two parallel neurons fails due to packet loss, the algorithm cannot compute the average, and the neurons in the preceding layer cannot calculate their gradient. Besides, if the connection, whether the synchronization happens over NVLink, InfiniBand, Ethernet (RoCE or RoCEv2), or wireless connection, causes a delay, the completeness of the training slows down. This causes GPU under-utilization which is not efficient from the business perspective.\nFigure 2-12: Backward Pass – Gradient Synchronization and Averaging.\nTo be conntinued\u0026hellip;\nReferences:\nhttps://nwktimes.blogspot.com/2024/09/ai4ne-ch1-artificial-neuron.html https://nwktimes.blogspot.com/2024/10/ai-for-network-engineers.html "},{"id":31,"href":"/tech-book/docs/ai-ml-dc/2-1-large-language-models/","title":"Large Language Models (LLM)","section":"Data Center Networking for AI Clusters","content":" Large Language Models (LLM) - Part 1/2: Word Embedding # Introduction # This chapter introduces the basic operations of Transformer-based Large Language Models (LLMs), focusing on fundamental concepts rather than any specific LLM, such as OpenAI’s GPT (Generative Pretrained Transformer).The chapter begins with an introduction to tokenization and word embeddings, which convert input words into a format the model can process. Next, it explains how the transformer component leverages decoder architecture for input processing and prediction. This chapter has two main goals. First, it explains how an LLM understands the context of a word. For example, the word “clear” can be used as a verb (Please, clear the table.) or as an adjective (The sky was clear.), depending on the context. Second, it discusses why LLMs require parallelization across hundreds or even thousands of GPUs due to the large model size, massive datasets, and the computational complexity involved.\nTokenizer and Word Embedding Matrix # As a first step, we import a vocabulary into the model. The vocabulary used for training large language models (LLMs) typically consists of a mix of general and domain-specific terms, including basic vocabulary, technical terminology, academic and formal language, idiomatic expressions, cultural references, as well as synonyms and antonyms. Each word and character is stored in a word lookup table and assigned a unique token. This process is called tokenization.\nMany LLMs use Byte Pair Encoding (BPE), which splits words into subword units. For example, the word \u0026ldquo;unhappiness\u0026rdquo; might be broken down into \u0026ldquo;un,\u0026rdquo; \u0026ldquo;happi,\u0026rdquo; and \u0026ldquo;ness.\u0026rdquo; BPE is widely used because it effectively balances vocabulary size and tokenization efficiency, particularly for handling rare words and sub-words. For simplicity, we use complete words in all our examples.\nFigure 7-1 illustrates the relationship between words in the vocabulary and their corresponding tokens. Token values start from 2 because token 0 is reserved for padding and token 1 for unknown words.\nEach token, representing a word, is mapped to a Word Embedding Vector, which is initially assigned random values. The collection of these vectors forms a Word Embedding Matrix. The dimensionality of each vector determines how much contextual information it can encode.\nFor example, consider the word “clear.” A two-dimensional vector may distinguish it as either an adjective or a verb but lacks further contextual information. By increasing the number of dimensions, the model can capture more context and better understand the meaning of the word. In the sentence “The sky was clear,” the phrase “The sky was” suggests that \u0026ldquo;clear\u0026rdquo; is an adjective. However, if we extend the sentence to “She decided to clear the backyard of junk,” the word \u0026ldquo;clear\u0026rdquo; now functions as a verb. More dimensions allow the model to utilize surrounding words more effectively for next-word prediction. For instance, GPT-3 uses 12,288-dimensional vectors. Given a vocabulary size of 50,000 words used by GPT-3, the Word Embedding Matrix has dimensions of 12,288 × 50,000, resulting in 614,400,000 parameters.\nThe context size, defined as the sequence length of vectors, determines how many preceding words the model considers when predicting the next word. In GPT-3, the context size is 2,048 tokens.\nFigure 7-1: Tokenization and Word Embedding Matrix.\nWord Embedding # As a first step, when we feed input words into a Natural Language Processing (NLP) model, we must convert them into a format the model can understand. This is a two-step process:\nTokenization – Each word is assigned a corresponding token from a lookup table. Word Embedding – These token IDs are then mapped to vectors using a word embedding lookup table. To keep things simple, Figure 7-2 uses two-dimensional vectors in the embedding matrix. Instead of complete sentences, we use words, which can be categorized into four groups: female, male, adult, and child.\nThe first word, \u0026ldquo;Wife,\u0026rdquo; appears in the lookup table with the token value 2. The corresponding word vector in the lookup table for token 2 is [-4.5, -3.0]. Note that Figure 7-2 represents vectors as column vectors, whereas in the text, I use row vectors—but they contain the same values.\nThe second word, \u0026ldquo;Mother,\u0026rdquo; is assigned the token 3, which is associated with the word vector [-2.5, +3.0], and so on.\nFigure 7-2: Word Tokenization and Word Embedding.\nIn Figure 7-3, we have a two-dimensional vector space divided into four quadrants, representing gender (male/female) and age (child/adult). Tokenized words are mapped into this space.\nAt the start of the first iteration, all words are placed randomly within the two-dimensional space. During training, our goal is to adjust the word vector values so that adults are positioned on the positive side of the Y-axis and children on the negative side. Similarly, males are placed in the negative space of the X-axis, while females are positioned on the positive side.\nFigure 7-3: Words in the 2 Dimensional Vector Space in the Initial State.\nFigure 7-4 illustrates how words may be positioned after successful training. All words representing a male adult are placed in the upper-left quadrant (adult/male). Similarly, all other words are positioned in the two-dimensional vector space based on their corresponding age and gender.\nFigure 7-4: Words in the 2 Dimensional Vector Space After Training.\nIn addition to grouping similar words, such as \u0026ldquo;adult/female,\u0026rdquo; close to each other in an n-dimensional space, there should also be positional similarities between words in different quadrants. For example, if we calculate the Euclidean distance between the words Father and Mother, we might find that their distance is approximately 4.3. The same pattern applies to word pairs like Nephew-Niece, Brother-Sister, Husband-Wife, and Father-in-Law–Mother-in-Law.\nHowever, it is important to note that this example is purely theoretical. In practice, Euclidean distances in high-dimensional word embeddings are not fixed but vary depending on the training data and optimization process. The relationships between words are often better captured through cosine similarity rather than absolute Euclidean distances.\nFigure 7-5: Euclidean Distance.\nPositional Embeddings # Since input text often contains repeated words with different meanings depending on their position, an LLM must distinguish between them. To achieve this, the word embedding process in Natural Language Processing (NLP) incorporates a Positional Encoding Vector alongside the Word Embedding Vector, resulting in the final word representation.\nIn Figure 7-6, the sentence \u0026ldquo;The sky is clear, so she finally decided to clear the backyard\u0026rdquo; contains the word clear twice. Repeated words share the same token ID instead of receiving unique ones. In this example, the is assigned token ID 2, and clear is assigned 5. These token IDs are then mapped to vectors using a word embedding lookup table. However, without positional encoding, words with different meanings would share the same vector representation.\nFocusing on clear (token ID 5), it maps to the word embedding vector [+2.5, +1.0] from the lookup table. Since token IDs do not capture word position, identical words always receive the same embedding.\nPositional encoding is essential for capturing context and semantic meaning. As shown in Figure 7-6, each input word receives a Positional Encoding Vector (PE) in addition to its word embedding. PE can either be learned and adjusted during training or remain fixed. The final Word Embedding Vector is computed by combining both the Word Embedding Vector and Positional Encoding Vector.\nFigure 7-6: Tokenization – Positional Embedding Vector.\nCalculating the Final Word Embedding # Figure 7-2 presents the equations for computing the final word embedding by incorporating positional embeddings. There are three variables:\nPosition (pos) → The word’s position in the sentence. In our example, the first occurrence of clear is the fourth word, so pos = 4. Dimension (d) → The depth of the vector. We use a 2-dimensional vector, so d = 2. Index (i) → Specifies the axis of the vector: 0 for the x-axis and 1 for the y-axis. The positional embedding is computed using the following equations:\nx-axis: sin(pos/100002i/d), where i = 0 y-axis: cos(pos/100002i/d, where i = 1 For clear at position 4, with d = 2, the resulting 2D positional vector is [-0.8, +1.0]. This vector is then added to the input word embedding vector [+2.5, +1.0], resulting in the final word embedding vector [+1.7, +2.0].\nFigure 7-7 also shows the final word embedding for the second occurrence of clear, but the computation is omitted.\nFigure 7-7: Finals Word Embedding for the 4th Word.\nLarge Language Model (LLM) - Part 2/2: Transformer Architecture # Introduction # Sequence-to-sequence (seq2seq) language translation and Generative Pretrained Transformer (GPT) models are subcategories of Natural Language Processing (NLP) that utilize the Transformer architecture. Seq2seq models are typically using Long Short-Term Memory (LSTM) networks or encoder-decored based Transformers. In contrast, GPT is an autoregressive language model that uses decoder-only Transformer mechanism. The purpose of this chapter is to provide an overview of the decoder-only Transformer architecture.\nThe Transformer consists of stacks of decoder modules. A word embedding vector, a result of the word tokenization and embbeding, is fed as input to the first decoder module. After processing, the resulting context vector is passed to the next decodeer, and so on. After the final decoder, a softmax layer evaluates the output against the complete vocabulary to predict the next word. As an autoregressive model, the predicted word vector from the softmax layer is converted into a token before being fed back into the subsequent decoder layer. This process involves a token-to-word vector transformation prior to re-entering the decoder.\nEach decoder module consists of an attention layer, Add \u0026amp; Normalization layer and a feedforward neural network (FFNN). Rather than feeding the embedded word vector (i.e., token embedding plus positional encoding) directly into the decoder layers, the Transformer first computes the Query (Q), Key (K), and Value (V) vectors from the word vector. These vectors are then used in the self-attention mechanism. Initially, the query vector is multiplied by the key vectors using matrix multiplication. The result is then divided by the square root of the dimension of the key vectors (scaled dot product) to obtain the logits. The logits are processed by a softmax layer to compute probabilities. The SoftMax prediction results are multiplied with the value vectors to produce a context vector.\nBefore feeding the context vector into the feedforward neural network, it is summed with the original word embedding vector (which includes positional encoding) via a residual connection. Finally, the output is normalized using layer normalization. This normalized output is then passed as input to the FFNN, which computes the output. The basic architecture of the FFNN in the decoder is designed so that the input layer has as many neurons as the dimension of the context vector. The hidden layer, in turn, has four times as many neurons as the input layer, while the output layer has the same number of neurons as the input layer. This design guarantees that the output vector of the FFNN has the same dimension as the context vector. Like the attention block, the FFNN block also employs residual connections and normalization.\nFigure 7-8: Decoder-Only Transformer Architecture.\nQuery, Key and Value Vectors # As pointed out in the Introduction, the word embedding vector is not used as input to the first decoder. Instead, it is multiplied by pretrained Query, Key, and Value weight matrices. The result of this matrix multiplication, dot product, produces the Query, Key, and Value vectors, which are use as inputs, and are processed through the Transformer. Figure 7–9 show the basic workflow of this process.\nFigure 7-9: Query, Key, and Value Vectors.\nLet’s take a closer look at the process using numbers. After tokenizing the input words and applying positional encoding, we obtain a final 5-dimensional word matrix. To reduce computation cycles, the process reduces the dimension of the Query vector from 5 to 3. Because we want the Query vector to be three-dimensional, we use three 5-dimensional column vectors, each of which is multiplied by the word vector.\nFigure 7-10: Calculating the Query Vector.\nFigure 7–11 depicts the calculation process, where each component of the word vector is multiplied by its corresponding component in the Query weight matrix. The weighted sum of these results forms a three-dimensional Query vector. The Key and Value vectors are calculated using the same method.\nThe same Query, Key, and Value (Q, K, V) weight matrices are used across all words (tokens) within a single self-attention layer in a Transformer model. This ensures that each token is processed in the same way, maintaining consistency in the attention computations. However, each decoder layer in the Transformer has its own dedicated Q, K, and V weight matrices, meaning that every layer learns different transformations of the input tokens, allowing deeper layers to capture more abstract representations.\nFigure 7-11: Calculating the Query Vector.\nAttention Layer # Figure 7-12 depicts what happens in the first three components of the Attention layer after calculating the Query, Key, and Value vectors. In this example, we focus on the word “clear”, and try to predict the next word. Its Query vector is multiplied by its own Key vector as well as by all the Key vectors generated for the preceding words. Each multiplication produces its own score. Note that the score values shown in the figure are theoretical and are not derived from the actual Qv × Kv matrix multiplication; however, the remaining values are based on these calculations. Additionally, in our example, we use one-dimensional values (instead of actual vectors) to keep the figures and calculations simple. In reality, these are n-dimensional vectors.\nAfter the Qv × Kv matrix multiplication, the resulting scores are divided by the square root of the vector depth, yielding logits, i.e., the input values for the softmax function. The softmax function then computes the exponential of each logit (using Euler’s number, approximately 2.71828) and divides each result by the sum of all exponentials. For example, the value 3.16, corresponding to the first word, is divided by 482.22, resulting in a probability of 0.007. Note that the sum of the probabilities is 1.0. Softmax ensures that the attention scores sum to 1, making them interpretable as probabilities and helping the model decide which input tokens to focus on when generating an output. In our example, the token for the word “clear” has the highest probability at this stage. The word “decided” has the second highest probability score (0.210), which indicates that the semantics of “clear”, which has the highest probability score (0.665), can be interpreted as an verb answering the question: “What she decided to do?\nFigure 7-12: Attention Layer, the First Three Steps.\nNext, the SoftMax probabilities are multiplied by each token\u0026rsquo;s Value vector (matrix multiplication). The resulting vectors are then summed, producing the Context vector for the token associated with the word “clear.” Note that the components of the Value vectors are example values and are not derived from actual computations.\nFigure 7-13: Attention Layer, the Fourth Step.\nAdd \u0026amp; Normalization # As the final step, the Word vector, which includes positional encoding, is added to the context vector via a Residual Connection. The result is then passed through a normalization process, where the vector’s components are summed and divided by the vector’s dimension, yielding the mean (μ). This mean value is then used for standard deviation calculation: the mean is subtracted from each of the three vector components, and the results are squared. These squared values are then summed, divided by three (the vector’s dimension), and the square root of this result gives the final output vector [1.40, -0.55, -0.87] of the Add \u0026amp; Normalize layer. Figure 7-14: Add \u0026amp; Normalize Layer – Residual Connection and Layer Normalization.\nFeed Forward Neural Network # Within the decoder module, the feedforward neural network uses the output vector from the Add \u0026amp; Normalize layer as its input. In our example, the FFNN have one neuron in input layer for each component of the vector. This layer simply passes the input values to the hidden layer, where each neuron first calculates a weighted sum and then applies the ReLU activation function. In our example, the hidden layer contains nine neurons (three times the number of input neurons). The output from the hidden layer is then fed to the output layer, where the neurons again compute a weighted sum and apply the ReLU activation function. Note that in transformer-based decoders, the FFNN is applied to each token individually. This means that each token-related output from the attention layer is processed separately by the same FFNN model with shared weights, ensuring a consistent transformation of each token\u0026rsquo;s representation regardless of its position in the sequence.\nFigure 7-15: Fully Connected Feed Forward Neural Network (FFNN).\nThe final decoder output is computed in the Add \u0026amp; Normalize layer, similarly as Add \u0026amp; Normalize after the attention layer. This produces the decoder output, which is used as the input for the next decoder module. Figure 7-16: Add \u0026amp; Normalize Layer – Residual Connection and Layer Normalization.\nNext Word Probability Computation – SoftMax Function # The output of the last decoder module does not directly represent the next word. Instead, it must be transformed into a probability distribution over the entire vocabulary. First, the decoder output is passed through a weight matrix that maps it to a new vector, where each element corresponds to a word in the vocabulary. For example, in Figure 7-17 the vocabulary consists of 12 words. These words are tokenized and linked to their corresponding word embeddings vector. That said, the word embedding matrix serves as a weight matrix.\nFigure 7-17: Hidden State Vector and Word Embedding Matrix.\nFigure 7-18 illustrates how the decoder output vector (i.e., the hidden state h) is multiplied by all word embedding vectors to produce a new vector of logits. Figure 7-18: Logits Calculation – Dot Product of Hidden State and Complete Vocabulary.\nNext, the SoftMax function is applied to the logits. This function converts the logits into probabilities by exponentiating each logit and then normalizing by the sum of all exponentiated logits. The result is a probability distribution in which each value represents the likelihood of selecting a particular word as the next token.\nFigure 7-19: Probability Calculation - Adding Logits to SoftMax Function\nFinally, the word with the highest probability is selected as the next token. This token is then mapped back to its corresponding word using a token-to-word lookup. This initiates the next iteration, where the token is converted into its word embedding vector, and used together with positional encoding to create the actual word embedding for the next iteration.\nFigure 7-20: Word-to-Token, and Token-to-Word Embedding Process.\nIn theory, our simple example shows that the model can assign the highest probability to the correct word. For instance, by analyzing the position of the word “clear” relative to its preceding words, the model is able to infer the context. When the context implies that an action is directed toward a known target, the article “the” receives the highest probability score and is predicted as the next word.\nConclusion # We use pretty simple examples in this chapter. However, GPT-3, for example, is built on a deep Transformer architecture comprising 96 decoder blocks. Each decoder block is divided into three primary sub-layers:\nAttention Layer: This layer implements multi-head self-attention using four key weight matrices, one each for the query, key, and value projections, plus one for the output projection. Together, these matrices account for roughly 600 million trainable parameters per block.\nAdd \u0026amp; Normalize Layers: Each block employs two residual connections paired with layer normalization. The first Add \u0026amp; Normalize operation occurs immediately after the Attention Layer, and the second follows the Feed-Forward Neural Network (FFNN) layer. Although essential for stabilizing training, the parameters in each normalization step are relatively few, typically on the order of tens of thousands.\nFeed-Forward Neural Network (FFNN) Layer: The FFNN consists of two linear transformations with an intermediate expansion (usually about four times the model’s hidden size). This layer contributes approximately 1.2 billion parameters per block.\nAggregating the parameters from all 96 decoder blocks, and including additional parameters from the token embeddings, positional encodings, and other components, the entire GPT-3 model totals around 175 billion trainable parameters. This is why parallelism is essential: the training process must be distributed across multiple GPUs and executed according to a selected parallelization strategy. The second part of the book discusses about Parallelization.\nReferences:\nhttps://nwktimes.blogspot.com/2025/02/large-language-models-llm-part-12-word.html https://nwktimes.blogspot.com/2025/02/large-language-model-llm-part-22.html "},{"id":32,"href":"/tech-book/docs/ai-ml-dc/2-2-parallelism-strategies-in-deep-learning/","title":"Parallelism Strategies in Deep Learning","section":"Data Center Networking for AI Clusters","content":" Introduction # Figure 8-1 depicts some of the model parameters that need to be stored in GPU memory: a) Weight matrices associated with connections to the preceding layer, b) Weighted sum (z), c) Activation values (y), d) Errors (E), e) Local gradients (local ∇), f) Gradients received from peer GPUs (remote ∇), g) Learning rates (LR), and h) Weight adjustment values (Δw).\nIn addition, the training and test datasets, along with the model code, must also be stored in GPU memory. However, a single GPU may not have enough memory to accommodate all these elements. To address this limitation, an appropriate parallelization strategy must be chosen to efficiently distribute computations across multiple GPUs.\nThis chapter introduces the most common strategies include data parallelism, model parallelism, pipeline parallelism, and tensor parallelism.\nFigure 8-1: Overview of Neural Networks Parameters.\nData Parallelism # In data parallelization, each GPU has an identical copy of the complete model but processes different mini-batches of data. Gradients from all GPUs are averaged and synchronized before updating the model. This approach is effective when the model fits within a single GPU’s memory.\nIn Figure 8-2, the batch of training data is split into eight micro-batches. The first four micro-batches are processed by GPU A1, while the remaining four micro-batches are processed by GPU A2. Both GPUs share the same model, and their input data are processed through all layers to generate a model prediction. The computation during the forward pass does not involve network load traffic. After computing the model error, the backpropagation algorithm starts the backward pass. The first step involves calculating the derivative of the model error, which is synchronized across the GPUs. Next, the error is propagated backward to calculate neuron-based errors, which are then used to compute gradients for each weight parameter. These gradients are synchronized across the GPUs.\nThe backpropagation algorithm running on GPUs then sums the gradients and divides the result by the number of GPUs. This averaged result is also synchronized across GPUs. In this way, during the backward pass, both local gradients and averaged gradients are synchronized.\nIn our simple two-GPU example, this process does not generate excessive network traffic, although the GPUs can use 100% of their NICs (network interface cards) forwarding capacity. However, if hundreds or even thousands of GPUs are used, the network traffic becomes significantly larger.\nInter-GPU network communication within a single server (using PCIe, NVLink) or between multiple servers (over InfiniBand, Ethernet, wireless) requires packet forwarding with minimal latency and in a lossless manner. Minimal latency is required to keep the training time as short as possible, while lossless transport is essential because training will pause if even a single packet is lost during synchronization. In the worst-case scenario, if no snapshot of the training progress is taken, the entire training process must be restarted from the beginning. Training a Large Language Model can take months or more.\nNow, consider the electricity costs if training had already been running for two months and had to be restarted due to a single packet loss.\nPower Consumption Example:\nA single GPU consumes roughly 350W under full load. Total power consumption for 50,000 GPUs: 350 W×50,000=17,500,000 W=17.5 MW For two months (60 days = 1,440 hours) of training: 17.5 MW×1,440 hours=25,200 MWh=25,200,000 If electricity costs $0.10 per kWh, the total training cost will be: 25,200,000 kWh×0.10=2,520,000 USD Figure 8-2: Data Parallelism Overview.\nhttps://nwktimes.blogspot.com/2025/03/parallelism-strategies-in-deep-learning.html References:\n"},{"id":33,"href":"/tech-book/docs/ai-ml-dc/3-challenges-in-ai-fabric/","title":"Challenges in AI Fabric Design","section":"Data Center Networking for AI Clusters","content":" Intro # Figure 10-1 illustrates a simple distributed GPU cluster consisting of three GPU hosts. Each host has two GPUs and a Network Interface Card (NIC) with two interfaces. Intra-host GPU communication uses high-speed NVLink interfaces, while inter-host communication takes place via NICs over slower PCIe buses.\nGPU-0 on each host is connected to Rail Switch A through interface E1. GPU-1 uses interface E2 and connects to Rail Switch B. In this setup, inter-host communication between GPUs connected to the same rail passes through a single switch. However, communication between GPUs on different rails goes over three hops Rail–Spine–Rail switches.\nIn Figure 10-1, we use a data parallelization strategy where a training dataset is split into six micro-batches, which are distributed across the GPUs. All GPUs use the shared feedforward neural network model and compute local model outputs. Next, each GPU calculates the model error and begins the backward pass to compute neuron-based gradients. These gradients indicate how much, and in which direction, the weight parameters should be adjusted to improve the training result (see Chapter 2 for details).\nFigure 10-1: Rail-Optimized Topology.\nEgress Interface Congestions # After computing all gradients, each GPU stores the results in a local memory buffer and starts a communication phase where computed gradients are shared with other GPUs. During this process, the data (gradients) is being sent from one GPU and written to another GPU\u0026rsquo;s memory (RDMA Write operation). RDMA is explained in detail in Chapter 9.\nOnce all gradients have been received, each GPU averages the results (AllReduce) and broadcasts the aggregated gradients to the other GPUs. This ensures that all GPUs update their model parameters (weights) using the same gradient values. The Backward pass process and gradient calculation are explained in Chapter 2.\nFigure 10-2 illustrates the traffic generated during gradient synchronization from the perspective of GPU-0 on Host-2. Gradients from the local host’s GPU-1 are received via the high-speed NVLink interface, while gradients from GPUs in other hosts are transmitted over the backend switching fabric. In this example, all hosts are connected to Rail Switches using 200 Gbps fiber links. Since GPUs can communicate at line rate, gradient synchronization results in up to 800 Gbps of egress traffic toward interface E1 on Host-2, via Rail Switch A. This may cause congestion, and packet drops if the egress buffer on Rail Switch A or the ingress buffer on interface E1 is not deep enough to accommodate the queued packets.\nFigure 10-2: Congestion During Backward Pass.\nSingle Point of Failure # The training process of a neural network is a long-running, iterative task where GPUs must communicate with each other. The frequency and pattern of this communication depend on the chosen parallelization strategy. For example, in data parallelism, communication occurs during the backward pass, where GPUs synchronize gradients. In contrast, model parallelism and pipeline parallelism involve communication even during the forward pass, as one GPU sends activation results to the next GPU holding the subsequent layer. It is important to understand that communication issues affecting even a single GPU can delay or interrupt the entire training process. This makes the AI fabric significantly more sensitive to single points of failure compared to traditional data center fabrics.\nFigure 10-3 highlights several single points of failure that may occur in real-world environments. A host connection can become degraded or completely fail due to issues in the host, NIC, rail switch, transceiver, or connecting cable. Any of these failures can isolate a GPU. While this might not seem serious in large clusters with thousands of GPUs, as discussed in the previous section, even one isolated or failed GPU can halt the training process.\nProblems with interfaces, transceivers, or cables in inter-switch links can cause congestion and delays. Similar issues arise if a spine switch is malfunctioning. These types of failures typically affect inter-rail traffic but not intra-rail communication. A failure in a rail switch can isolate all GPUs connected to that rail, creating a critical point of failure for a subset of the GPU cluster.\nFigure 10-3: Single-Point Failures.\nHead-of-Line Blocking # In this example GPU clusters, NCCL (NVIDIA Collective Communications Library) has built a topology where gradients are first sent from GPU-0 to GPU-1 over NVLink, and then forwarded from GPU-1 to other GPU-1s via Rail switch B.\nHowever, this setup may lead to head-of-line blocking. This happens when GPU-1 is already busy sending its own gradients to the other GPUs, and now it also needs to forward GPU-0’s gradients. Since the PCIe and NIC bandwidth is limited, GPU-0’s traffic may need to wait in line behind GPU-1’s traffic. This queueing delay is called head-of-line blocking, and it can slow down the whole training process. The problem is more likely to happen when many GPUs rely on a single GPU or NIC for forwarding traffic to another rail. Even if only one GPU is overloaded, it can cause delays for others too.\nFigure 10-4: Head-of-Line Blocking.\nHash-Polarization with ECMP # First, when two GPUs open a Queue Pair (QP) between each other, all gradient synchronization traffic is typically sent over that QP. From the network point of view, this looks like one large flow between the GPUs. In deep learning training, gradient data can be hundreds of megabytes or even gigabytes, depending on the model size. So, when it is sent over one QP, the network sees it as a single high-bandwidth flow. This kind of traffic is often called an elephant flow, because it can take a big share of the link bandwidth. This becomes a problem when multiple large flows are hashed to the same uplink or spine port. If that happens, one link can get overloaded while others remain underused. This is one of the reasons we see hash polarization and head-of-line blocking in AI clusters. Hash polarization is a condition where the load-balancing hash algorithm used in ECMP (Equal-Cost Multi-Path) forwarding results in uneven distribution of traffic across multiple available paths.\nFor example, in Figure 10-5, GPU-0 in Host-1 and GPU-0 in Host-2 both send traffic to GPU-1 at a rate of 200 Gbps. The ECMP hash function in Rail Switch A selects the link to Spine 1 for both flows. This leads to a situation where one spine link carries 400 Gbps of traffic, while the other remains idle. This is a serious problem in AI clusters because training jobs generate large volumes of east-west traffic between GPUs, often at line rate. When traffic is unevenly distributed due to hash polarization, some links become congested while others are idle. This causes packet delays and retransmissions, which can slow down gradient synchronization and reduce the overall training speed. In large-scale clusters, even a small imbalance can have a noticeable impact on job completion time and resource efficiency.\nFigure 10-5: ECMP Hash-Polarization.\nThe rest of this book focuses on how these problems can be mitigated or even fully avoided. We will look at design choices, transport optimizations, network-aware scheduling, and alternative topologies that help improve the robustness and efficiency of the AI fabric.\nReferences:\nhttps://nwktimes.blogspot.com/2025/04/ai-for-network-engneers-challenges-in.html "},{"id":34,"href":"/tech-book/docs/ai-ml-dc/4-congestion-avoidance-in-ai-fabric/","title":"Congestion Avoidance in AI Fabric","section":"Data Center Networking for AI Clusters","content":" Congestion Avoidance in AI Fabric - Part I: Explicit Congestion Notification (ECN) # As explained in the preceding chapter, “Egress Interface Congestions,” both the Rail switch links to GPU servers and the inter-switch links can become congested during gradient synchronization. It is essential to implement congestion control mechanisms specifically designed for RDMA workloads in AI fabric back-end networks because congestion slows down the learning process and even a single packet loss may restart the whole training process.\nThis section begins by introducing Explicit Congestion Notification (ECN) and Priority-based Flow Control (PFC), two foundational technologies used in modern lossless Ethernet networks. ECN allows switches to mark packets, rather than dropping them, when congestion is detected, enabling endpoints to react proactively. PFC, on the other hand, offers per-priority flow control, which can pause selected traffic classes while allowing others to continue flowing.\nFinally, we describe how Datacenter Quantized Congestion Notification (DCQCN) combines ECN and PFC to deliver a scalable and lossless transport mechanism for RoCEv2 traffic in AI clusters.\nGPU-to-GPU RDMA Write Without Congestion # The figure 11-1 illustrates a standard Remote Direct Memory Access (RDMA) Write operation between two GPUs. This example demonstrates how GPU-0 on Host-1 transfers local gradients (∇₁ and ∇₂) from memory to GPU-0 on Host-2. Both GPUs use RDMA-capable NICs connected to Rail Switch A via 200 Gbps uplinks.\nThe RDMA Write operation proceeds through the following seven steps:\nTo initiate the data transfer, GPU-0 on Host-1 submits a work request to its RDMA NIC over the PCIe bus over the pre-established Queue Pair 0x123456. The RDMA NIC encodes the request by inserting the OpCode (RDMA Write) and Queue Pair Number (0x123456) into the InfiniBand Transport Header (IBTH). It wraps the IBTH and RETH (not shown in the figure) headers with Ethernet, IP, UDP, and Ethernet headers. The NIC sets the DSCP value to 24 and the ECN bits to 10 (indicating ECN-capable transport) in the IP header\u0026rsquo;s ToS octet. The DSCP value ensures that the switch can identify and prioritize RoCEv2 traffic. The destination UDP port is set to 4791 (not shown in the figure). Upon receiving the packet on interface Ethernet1/24, the Rail switch classifies the traffic as RoCEv2 based on the DSCP value of 24. The switch maps DSCP 24 to QoS-Group 3. QoS Group 3 uses egress priority queue 3, which is configured with bandwidth allocation and congestion avoidance parameters (WRED Min, WRED Max, and Drop thresholds) optimized for RDMA traffic. The packet count on queue 3 does not exceed the WRED minimum threshold, so packets are forwarded without modification. The RDMA NIC on Host-2 receives the packet, strips off the Ethernet, IP, and UDP headers, and processes the RDMA headers. The payload is delivered directly into the memory of GPU-0 on Host-2 without CPU involvement, completing the RDMA Write operation. Figure 11-1: Overview of Remote DMA operation Under Normal Condition.\nExplicit Congestion Notification -ECN # Because gradient synchronization requires a lossless network service, it is essential to have a proactive congestion detection system that can respond before buffer overflows occur. This system must also include a signalling mechanism that allows the receiver to request the sender to reduce its transmission rate or temporarily pause traffic when necessary.\nData Center Quantized Congestion Notification (DCQCN) is a congestion control scheme designed for RoCEv2 that leverages both Explicit Congestion Notification (ECN) and Priority Flow Control (PFC) for active queue management. This section focuses on ECN.\nIn IPv4, the last two bits (bits 6 and 7) of the ToS (Type of Service) byte are reserved for ECN marking. With two bits, four ECN codepoints are defined:\n00 – Not ECN-Capable Transport (Not-ECT) 01 – ECN-Capable Transport (ECT) 10 – ECN-Capable Transport (ECT) 11 – Congestion Experienced (CE)\nFigure 11-2 illustrates how ECN is used to prevent packet drops when congestion occurs on egress interface Ethernet2/24. ECN operates based on two queue thresholds: WRED Minimum and WRED Maximum. When the queue depth exceeds the WRED Minimum but remains below the WRED Maximum, the switch begins to randomly mark forwarded packets with ECN 11 (Congestion Experienced). If the queue depth exceeds the WRED Maximum, all packets are marked with ECN 11. The Drop Threshold defines the upper limit beyond which packets are no longer marked but instead dropped.\nIn Figure 11-2, GPU-0 on Host-1 transfers gradient values from its local memory to the memory of GPU-0 on Host-2. Although not shown in the figure for simplicity, other GPUs connected to the same rail switch also participate in the synchronization process with GPU-0 on Host-2. Multiple simultaneous elephant flows towards GPU-0 reach the rail switch, causing egress queue 3 on interface Ethernet2/24 to exceed the WRED Maximum threshold. As a result, the switch begins marking outgoing packets with ECN 11 (step 6), while still forwarding them to the destination GPU. Figure 11-2: Congested Egress Interface – ECN Congestion Experienced.\nAfter receiving a data packet marked with ECN 11, the destination RDMA NIC must inform the sender about network congestion. This is accomplished using a Congestion Notification Packet (CNP), which provides feedback at the RDMA transport layer. Upon detecting the ECN 11 in the IP header’s ToS bits of an incoming packet, the RDMA NIC on Host-2 generates a CNP message. It sets the OpCode in the IBTH header to 0x81, identifying the message as a CNP. Besides the FECN bit is set to 1, indicating that the NIC experienced congestion. The Queue Pair number used for the original memory copy (e.g., 0x123456) is reused, ensuring the feedback reaches the correct sender-side transport context. In the IP header, the DSCP field is set to 48, allowing switches to distinguish the CNP from standard RoCEv2 data traffic. When the CNP reaches the Rail switch (interface Ethernet2/24), it is classified based on DSCP 48, which is associated with CNP traffic in the QoS configuration. DSCP 48 maps the packet to QoS group 7, which is reserved for congestion feedback signaling. QoS group 7 is associated with strict-priority egress queue 7, ensuring that CNP packets are forwarded with the highest priority. This guarantees that congestion signals are not delayed behind other types of traffic. The switch forwards the CNP to the originating NIC on Host-1. Because of the strict-priority handling, the feedback arrives quickly even during severe congestion. Upon receiving the CNP, the sender-side RDMA NIC reduces its transmission rate for the affected Queue Pair by increasing inter-packet delay. This is achieved by holding outgoing packets longer in local buffers, effectively reducing traffic injection into the congested fabric. As transmission rates decrease, the pressure on the egress queue at the Rail switch’s interface Ethernet1/14 (connected to Host-2) is gradually relieved. Buffer occupancy falls below the WRED Minimum Threshold, ending ECN marking. Once congestion is fully cleared, the RDMA NIC slowly ramp up its transmission rate. This gradual marking strategy helps prevent sudden traffic loss and gives the sender time to react by adjusting its sending rate before the buffer overflows.\nFigure 11-3: Receiving NIC Generates CNP - Sender NIC Delay Transmit.\nNext post describes DSCP-Based Priority Flow Control (PFC) use cases and operation.\nCongestion Avoidance in AI Fabric - Part II: Priority Flow Control (PFC) # Priority Flow Control (PFC) is a mechanism designed to prevent packet loss during network congestion by pausing traffic selectively based on priority levels. While the original IEEE 802.1Qbb standard operates at Layer 2, using the Priority Code Point (PCP) field in Ethernet headers, AI Fabrics rely on Layer 3 forwarding, where traditional Layer 2-based PFC is no longer applicable. To extend lossless behavior across routed (Layer 3) networks, DSCP-based PFC is used.\nIn DSCP-based PFC, the Differentiated Services Code Point (DSCP) field in the IP header identifies the traffic class or priority. Switches map specific DSCP values to internal traffic classes and queues. If congestion occurs on an ingress interface and a particular priority queue fills beyond a threshold, the switch can send a PFC pause frame back to the sender switch, instructing it to temporarily stop sending traffic of that class—just as in Layer 2 PFC, but now triggered based on Layer 3 classifications.\nThis behavior differs from Explicit Congestion Notification (ECN), which operates at Layer 3 as well but signals congestion by marking packets instead of stopping traffic. ECN acts on the egress port, informing the receiver to notify the sender to reduce the transmission rate over time. In contrast, PFC acts immediately at the ingress port, pausing traffic flow in real time to avoid buffer overflows and packet drops.\nPFC relies on two thresholds to control flow: xOFF and xON. The xOFF threshold defines the point at which the switch generates a pause frame when a priority queue reaches a congested state. Once triggered, upstream devices halt transmission of that traffic class. The switch continuously monitors its buffer occupancy, and when the level drops below the xON threshold, it sends a PFC frame with a Quanta value of 0 for the affected priority. This signals the upstream device that it can resume transmission for that specific priority queue.\nA key requirement for PFC to function correctly is the provisioning of buffer headroom. The switch must reserve enough buffer space per priority class to accommodate in-flight traffic while the pause frame propagates to the sender and takes effect.\nDSCP-based PFC enables lossless packet delivery over routed networks, which is especially important for technologies like RoCEv2 (RDMA over Converged Ethernet v2), where even minimal packet loss can cause significant performance degradation.\nDSCP-Based PFC Process over a Layer 3 Routed Interface (Example Scenario) # This example illustrates how DSCP-based Priority Flow Control (PFC) operates across a routed Layer 3 fabric during congestion. We walk through a four-step process, beginning with buffer overflow and ending with traffic pausing on the correct priority queue.\nStep 1: Buffer Overflow on Rail Switch C (Egress to GPU-3, Host 3) # In a GPU cluster, multiple GPUs are sending high-throughput RDMA traffic to GPU on Host-3. In Figure 11-4 Rail Switch C is responsible for forwarding traffic toward GPU-3. The egress interface on Switch C (E12/24) that connects to GPU-3 becomes congested. Due to the overflow of egress queue 3, packets from ingress queue 3 on interface E3/24 cannot be placed into egress queue 3.\nStep 2: xOFF Threshold Exceeded # Priority queue 3 of has two configured thresholds:\nxOFF threshold: Triggers a pause when buffer usage exceeds this level. xON threshold: Triggers a resume when the buffer has drained sufficiently. Once priority queue 3 on ingress interface E3/24 exceeds its xOFF threshold, the switch takes immediate action to prevent packet loss by generating a PFC pause message targeted at the sender. The sender in this case is Spine Switch 1, which is sending traffic to Rail Switch C, over interface E3/24, for delivery to GPU-3.\nStep 3: Generating a PFC Pause Frame (MAC Control Frame) # To pause the sender, Rail Switch C generates an Ethernet MAC Control frame with:\nEthertype 0x8808: This indicates a MAC Control frame, used for pause-related operations (standardized in IEEE 802.3x). Inside this frame, a PFC opcode (0x0101) specifies it\u0026rsquo;s a Priority-based Pause (PFC) message. Class Enable Vector (CEV): This 8-bit field indicates which priority queues should be paused. Each bit corresponds to one of the 8 possible traffic classes (0–7). For example, if bit 3 is set to 1, it tells the sender to pause traffic for priority queue 3 only, while all other bits remain 0. In our case, the CEV is 0x1000. Note that the right-most bit represents queue 0. Quanta Field(s): For each enabled priority (bit set to 1), a corresponding quanta value is specified. This value defines the duration of the pause, measured in units of 512 bit times. For a 400 Gbps interface:\n1 bit time = 1 / 400,000,000,000 seconds ≈ 2.5 picoseconds 1 quanta = 512 × 2.5 ps = 1.28 nanoseconds If the pause quanta is set to maximum value 0xFFFF (65535), the pause duration is roughly 83.9 microseconds. This pause frame is sent back to the sender Spine Switch 1. Since the DSCP-based classification maps back to priority queue 3, and the switches share the same mapping, Spine Switch 1 will interpret this correctly.\nStep 4: Spine Switch 1 Pauses Transmission on Priority Queue 3 # Upon receiving the PFC frame on its ingress interface E3/24 connected to Rail Switch C, Spine Switch 1 examines the class enable vector.\nSince bit 3 is set, the switch knows to pause transmission of all frames mapped to priority queue 3 (DSCP value 24 in our example) on egress interface E3/24. Traffic for other priority queues continues unaffected. Spine Switch 1 holds off transmission of priority 3 traffic until it receives a subsequent PFC frame with quanta = 0, indicating “resume,” or a pause duration timeout occurs, after which the switch resumes sending unless another pause is received.\nFigure 11-4: Priority Flow Control – Pause Frame.\nThe following example shows how Priority Flow Control (PFC) events can cascade upstream when congestion persists in a routed Layer 3 fabric. This scenario builds on the earlier case, where Spine Switch 1 paused traffic to Rail Switch C. Now, we observe how that pause affects traffic originating from Rail Switches A and B.\nStep 5: Congestion on Spine Switch 1 Egress Queue to Rail Switch C # As described in the previous figure, Spine Switch 1 received a PFC frame from Rail Switch C and responded by pausing traffic on priority queue 3 on its egress interface E3/24 (towards Rail Switch C). Because this interface is no longer sending traffic, frames destined for GPU-3 via Rail Switch C begin to accumulate in Spine Switch 1’s egress queue 3. This build-up causes backpressure that impacts the ingress side of the switch.\nStep 6: xOFF Threshold Exceeded on Spine Switch 1 Ingress Interfaces # Spine Switch 1 receives incoming traffic from Rail Switch A (interface E1/24) and Rail Switch B (interface E2/24). Both switches are sending traffic mapped to priority queue 3 (e.g., DSCP 24). As the egress queue to Rail Switch C becomes full and cannot drain, the corresponding ingress buffers on interfaces E1/24 and E2/24 also begin to fill up, specifically for queue 3. Eventually, the xOFF thresholds on both ingress interfaces are exceeded, indicating that congestion is now impacting the reception of new packets on these ports.\nStep 7: Spine Switch 1 Sends PFC Pause Frames to Rail Switch A and B # To avoid dropping packets due to ingress buffer overflow, Spine Switch 1 generates PFC MAC Control frames on both E1/24 and E2/24. The class enable vector has bit 3 set, instructing the sender to pause traffic corresponding to priority queue 3. A suitable quanta value is included to define the pause duration. These control frames travel back to Rail Switch A and Rail Switch B respectively.\nStep 8: Rail Switches A and B Pause Queue 3 Traffic to Spine Switch 1 # Upon receiving the PFC frames, both Rail Switch A and Rail Switch B interpret the class enable vector and pause all traffic mapped to priority queue 3 (e.g., DSCP 24), still forwarding traffic on other priority queues unaffected. This marks the upstream propagation of congestion: a single bottleneck on the path to GPU-3 can trigger PFC reactions all the way back to multiple source switches.\nFigure 11-5: Priority Flow Control – Cascading Effect.\nSteps 9a – 14: Downstream Resume and Congestion Recovery # Figure 11-6 illustrates how the PFC-based congestion recovery process extends from Rail Switches A and B all the way to the GPU NICs, while simultaneously resolving the initial congestion at Rail Switch C.\nAs a result of the earlier PFC pause frames:\nRail Switch A and Rail Switch B have paused sending priority queue 3 traffic to Spine Switch 1. In turn, Spine Switch 1 has paused its own egress traffic toward Rail Switch C on interface E3/24. This pause allows queue 3 on Rail Switch C’s egress interface E12/24 (toward GPU-3) to drain, as no new traffic is arriving, and the GPU continues to consume incoming data.\nOnce the buffer utilization for priority queue 3 drops below the configured xON threshold, Rail Switch C initiates congestion recovery.\nIt sends a MAC Control Frame (Ethertype 0x8808) back to Spine Switch 1. The class enable vector has bit 3 set (indicating priority queue 3). The quanta value is set to 0, signaling that it is now safe to resume transmission. Upon receiving this resume message, Spine Switch 1 can begin sending traffic again on priority queue 3, restoring throughput toward GPU-3 and continuing the flow of RDMA traffic through the network. This recovery mechanism operates consistently across the entire AI fabric.\nFigure 11-6: Priority Flow Control – PCIe Bus Congested: Cascading Effect.\nLLDP with DCBX # PFC negotiation is performed using the Link Layer Discovery Protocol (LLDP), which carries Data Center Bridging eXchange (DCBX) Type-Length-Value (TLV) structures. At the time of writing, DCBX exists in two versions: IEEE and CEE. The IEEE mode (defined in 802.1Qbb and 802.1Qaz) is standards-based and supported by most modern data center switches from various vendors. This mode is also known as DCBXv2. Some older Cisco Nexus models support only the Cisco/Converged Enhanced Ethernet (CEE) mode. Capture 11-1 shows the packet format of a standards-based IEEE DCBX TLV within an LLDP message.\nCapture 11-1: PCF: LLDP with IEEE DBCXv2 TLV .\nCongestion Avoidance in AI Fabric - Part III: Data Center Quantized Congestion Notification (DCQCN) # Data Center Quantized Congestion Notification (DCQCN) is a hybrid congestion control method. DCQCN brings together both Priority Flow Control (PFC) and Explicit Congestion Notification (ECN) so that we can get high throughput, low latency, and lossless delivery across our AI fabric. In this approach, each mechanism plays a specific role in addressing different aspects of congestion, and together they create a robust flow-control system for RDMA traffic.\nDCQCN tackles two main issues in large-scale RDMA networks:\nHead-of-Line Blocking and Congestion Spreading: This is caused by PFC’s pause frames, which stop traffic across switches. Throughput Reduction with ECN Alone: When the ECN feedback is too slow, packet loss may occur despite the rate adjustments. DCQCN uses a two-tiered approach. It applies ECN early on to gently reduce the sending rate at the GPU NICs, and it uses PFC as a backup to quickly stop traffic on upstream switches (hop-by-hop) when congestion becomes severe.\nHow DCQCN Combines ECN and PFC # DCQCN carefully combines Explicit Congestion Notification (ECN) and Priority Flow Control (PFC) in the right sequence:\nEarly Action with ECN: When congestion begins to build up, the switch uses WRED thresholds (minimum and maximum) to mark packets. This signals the sender to gradually reduce its transmission rate. As a result, the GPU NIC slows down, and traffic continues flowing—just at a reduced pace—without abrupt pauses.\nBackup Action with PFC: If congestion worsens and the queue continues to grow, the buffer may reach the xOFF threshold. At this point, the switch sends PFC pause frames hop by hop to upstream devices. These devices respond by temporarily stopping traffic for that specific priority queue, helping prevent packet loss.\nResuming Traffic: Once the buffer has drained and the queue drops below the xON threshold, the switch sends a resume message (a PFC frame with a quanta value of 0). This tells the upstream device it can start sending traffic again.\nWhy ECN Must Precede xOFF # It is very important that the ECN thresholds (WRED minimum and maximum) are used before the xOFF threshold is reached for three main reasons:\nGraceful Rate Adaptation: Early ECN marking helps the GPU NIC (sender) reduce its transmission rate gradually. This smooth adjustment avoids sudden stops and leads to more stable traffic flows.\nAvoiding Unnecessary PFC Events: If the sender adjusts its rate early with ECN feedback, the buffers are less likely to fill up to the xOFF level. This avoids the need for abrupt PFC pause frames that can cause head-of-line blocking and backpressure on the network.\nMaintaining Fabric Coordination: With early ECN marking, the sender receives feedback before congestion becomes severe. While the ECN signal is not shared directly with other switches, the sender\u0026rsquo;s rate adjustment helps reduce overall pressure on the network fabric.\nWhat Happens If xOFF Is Reached Before ECN Marking? # Imagine that the ingress queue on Spine Switch 1 (from Rail Switch A) fills rapidly without ECN marking:\nSudden Pause: The buffer may quickly hit the xOFF threshold and trigger an immediate PFC pause.\nDownstream Effects: An abrupt stop in traffic from Rail Switch A leads to sudden backpressure. This can cause head-of-line blocking and disturb GPU communication, leading to performance jitter or instability at the application level.\nOscillations: When the queue finally drains and reaches the xON threshold, traffic resumes suddenly. This can cause recurring congestion and stop-and-go patterns that hurt overall performance.\nBy allowing ECN to mark packets early, the network gives the sender time to reduce its rate smoothly. This prevents abrupt stops and helps maintain a stable, efficient fabric.\nFigure 11 recaps how the example DCQCN process works:\nTime t1: (1) Traffic associated with priority queue 3 on Rail-1’s egress interface 1 crosses the WRED minimum threshold.\nTime t2: (2) Rail-1 begins randomly marking ECN bits as 11 on packets destined for GPU-0 on the Host-3.\nTime t3: (3) The RDMA NIC starts sending CNP messages to the sender GPU-1 on Host-1.\nTime t4: (4) In response to the CNP message, the sending GPU-0 on Host-1 reduces its transmission rate by holding packets longer in its egress queue. (5) At the same time, egress queue 3 on Rail-1 remains congested. (6) Since packets cannot be forwarded from ingress interface 2 to egress interface 1’s queue 3, ingress interface 3 also becomes congested, eventually crossing the PFC xOFF threshold.\nTime t5: (7) As a result, Rail-1 sends a PFC xOFF message to Spine-A over Inter-Switch Link 3. (8) In response, Spine-A halts forwarding traffic for the specified pause duration.\nTime t6: (9) Due to the forwarding pause, the egress queue of interface 3 on Spine-A becomes congested, which in turn (10) causes congestion on its ingress interface 2.\nTime t7: (11) The number of packets waiting in egress queue 3 on interface 1 of Rail-1 drops below the WRED minimum threshold. (12) This allows packets from the buffer of interface 3 to be forwarded.\nTime t8: (13) The packet count on ingress interface 3 of Rail-1 falls below the PFC xON threshold, triggering the PFC resume/unpause message to Spine-A. (14) Spine-A resumes forwarding traffic to Rail-1.\nAfter the PFC resume message is sent, Spine-A starts forwarding traffic again toward Rail-1. The congestion on Spine-A’s interface 3 gets cleared as packets leave the buffer. This also helps the ingress interface 2 on Spine-A to drain. On Rail-1, as interface 1 can now forward packets, queue 3 gets more room, and the flow to GPU-0 becomes smoother again.\nThe RDMA NIC on the sender GPU monitors the situation. Since there are no more CNP messages coming in, the GPU slowly increases its sending rate. At the same time, the ECN marking on Rail-1 stops, as queue lengths stay below the WRED threshold. Traffic flow returns to normal, and no more PFC pause messages are needed.\nThe whole system stabilizes, and data can move again without delay or packet loss.\nFigure 11-7: DCQCN: ECN and PFC Interaction .\nDCQCN Configuration # Figure 11-8 shows the six steps to enable DCQCN on a switch. The figure assumes that the RDMA NIC marks RoCEv2 traffic with DSCP 24.\nFirst, we classify the packets based on the DSCP value in the IPv4 header. Packets marked with DSCP 24 are identified as RoCEv2 packets, while packets marked with DSCP 48 are classified as CNP.\nAfter classification, we add an internal QoS label to the packets to place them in the correct output queue. The mapping between internal QoS labels and queues is fixed and does not require configuration.\nNext, we define the queue type, allocate bandwidth, and set ECN thresholds. After scheduling is configured, we enable PFC and set its threshold values. A common rule of thumb for the relationship between ECN and PFC thresholds is: xON \u0026lt; WRED Min \u0026lt; WRED Max \u0026lt; xOFF.\nTo apply these settings, we enable them at the system level. Finally, we apply the packet classification to the ingress interface and enable the PFC watchdog on the egress interface. Because PFC is a sub-TLV in the LLDP Data Unit (LLDPDU), both LLDP and PFC must be enabled on every inter-switch link.\nFigure 11-8: Applying DCQCN to Switch.\nStep 1: Packet Classification # The classification configuration is used to identify different types of traffic based on their DSCP values. In our example we have one for RoCEv2 traffic and another for Congestion Notification Packets (CNP). The “class-map type qos match-any ROCEv2” line defines a class map named “ROCEv2” that matches any packet marked with DSCP value 24, which is commonly used for RDMA traffic. Similarly, the “class-map type qos match-any CNP” defines another class map named “CNP” that matches packets marked with DSCP value 48, typically used for congestion signaling in RDMA environments. These class maps serve as the foundation for downstream policies, enabling differentiated handling of traffic types. Note that the names “ROCEv2” and “CNP” are not system-reserved; they are simply user-defined labels that can be renamed, as long, as the references are consistent throughout the configuration.\nclass-map type qos match-any ROCEv2 match dscp 24 class-map type qos match-any CNP match dscp 48 Example 11-1: Classification.\nStep 2: Internal QoS Label for Queueing # The marking configuration assigns internal QoS labels to packets that have already been classified. This is done using a policy map named QOS_CLASSIFICATION, which refers to the previously defined class maps. Within this policy, packets that match the “ROCEv2” class are marked with qos-group 3, and those matching the “CNP” class are marked with qos-group 7. Any other traffic that doesn\u0026rsquo;t fit these two categories falls into the default class and is marked with qos-group 0. These QoS groups are internal identifiers that the switch uses in later stages for queuing and scheduling, to decide how each packet should be treated. Just like class maps, the name of the policy map itself is user-defined and can be anything descriptive, provided it is correctly referenced in other parts of the configuration. policy-map type qos QOS_CLASSIFICATION class ROCEv2 set qos-group 3 class CNP set qos-group 7 class class-default set qos-group 0 Example 11-2: Marking.\nStep 3: Scheduling # The queuing configuration defines how traffic is scheduled and prioritized on the output interfaces, based on the internal QoS groups that were assigned earlier. This is handled by a policy map named “QOS_EGRESS_PORT,” which maps traffic to different hardware output queues. Each queue is identified by a class, such as c-out-8q-q7 (fixed names: 8q = eight queues, q7 = queue number 7). For example, queue 7 is configured with priority level 1, which gives it strict priority over all other traffic. Queue 3 is assigned bandwidth remaining percent 50, meaning that it is guaranteed half of the remaining bandwidth after strict-priority traffic has been serviced. In addition to bandwidth allocation, queue 3 includes congestion management features through the random-detect command. This enables Weighted Random Early Detection (WRED), a mechanism that helps avoid congestion by randomly mark packets as queue depth increases. The minimum-threshold and maximum-threshold define the WRED minimum and maximum values (from 150 KB to 3000 KB) at which packets begin marked. The drop-probability 7 determines the likelihood of packet mark when the maximum threshold is reached, with higher numbers indicating higher marking rates. The weight 0 setting controls how queue size is averaged. A weight of 0 means use instantaneous queue depth (no averaging). Finally, ecn enables Explicit Congestion Notification, allowing network devices to signal congestion without dropping packets, without the ecn option switch drops packet based on WRED min/max values. The remaining queues are configured with either zero percent of remaining bandwidth, effectively disabling them for general use, or with a share of the remaining bandwidth. This queuing policy ensures that RoCEv2 traffic receives adequate resources with congestion feedback, while CNP messages always get through with strict priority.\npolicy-map type queuing QOS\\_EGRESS\\_PORT class type queuing c-out-8q-q6 bandwidth remaining percent 0 ... class type queuing c-out-8q-q3 bandwidth remaining percent 50 random-detect minimum-threshold 150 kbytes maximum-threshold 3000 kbytes drop-probability 7 weight 0 ecn ... class type queuing c-out-8q-q7 priority level 1 Example 11-3: Queuing (Output Scheduling).\nStep 4: Enable PFC for Queue # The Network QoS configuration defines the low-level, hardware-based characteristics of traffic handling within the switch, such as enabling lossless behavior and setting the maximum transmission unit (MTU) size for each traffic class. In this example, the policy-map type network-qos qos_network is used to configure how traffic is handled inside the switch fabric. Under this policy, the class type network-qos c-8q-nq3 is associated with pause pfc-cos 3, which enables Priority Flow Control (PFC) on Class of Service (CoS) 3. This is critical for RoCEv2 traffic, which depends on a lossless transport layer. The MTU is also defined here, with bytes (jumbo frame) set for class 3 traffic.\npolicy-map type network-qos qos\\_network class type network-qos c-8q-nq3 mtu 9216 pause pfc-cos 3 Example 11-4: Queuing (Output Scheduling).\nPriority Flow Control Watchdog # The Priority Flow Control (PFC) watchdog is a mechanism that protects the network from traffic deadlocks caused by stuck PFC pause frames. In RDMA environments like RoCEv2, PFC is used to create lossless classes of traffic by pausing traffic flows instead of dropping packets. However, if a device fails to release the pause or a misconfiguration causes PFC frames to persist, traffic in the affected class can become permanently blocked, leading to what is called a \u0026ldquo;head-of-line blocking\u0026rdquo; condition. To mitigate this risk, the priority-flow-control watch-dog-interval on command enables the PFC watchdog feature. When enabled, the switch monitors traffic in each PFC-enabled queue for signs of persistent pause conditions. If it detects that traffic has been paused for too long, indicating a potential deadlock, it can take corrective actions, such as generating logs, resetting internal counters, or even discarding paused traffic to restore flow. priority-flow-control watch-dog-interval on Example 11-5: Priority Flow Control (PFC) Watchdog.\nStep 5: Bind and Apply QoS Settings # System-level QoS policies bind all the previously defined QoS components together and activate them across the switch. This is done using the system qos configuration block, which applies the appropriate policy maps globally. The service-policy type network-qos qos_network command activates the network-qos policy defined earlier, ensuring that MTU sizes and PFC configurations are enforced across the switch fabric. The command service-policy type queuing output QOS_EGRESS_PORT applies the queuing policy at the output interface level, enabling priority queuing, bandwidth allocation, and congestion management as traffic exits the switch. These system-level bindings are essential because, without them, the individual QoS policies, classification, marking, queuing, and fabric-level configuration, would remain inactive. By applying the policies under system qos, the switch is instructed to treat traffic according to the rules and priorities defined in each policy map. This final step ensures end-to-end consistency in QoS behavior, from ingress classification to fabric transport and egress scheduling, providing a complete and operational quality-of-service framework tailored for latency-sensitive, lossless applications like RoCEv2.\nsystem qos service-policy type network-qos qos\\_network service-policy type queuing output QOS\\_EGRESS\\_PORT Example 11-6: Priority Flow Control (PFC) Watchdog.\nStep 6: Interface-Level Configuration The interface-level configuration attaches the previously defined QoS policies and enables PFC-specific features for a given port. In our example, the configuration is applied to Ethernet2/24, but the same approach can be used for any interface where you need to enforce QoS and PFC settings. The first command, priority-flow-control mode auto, enables Priority Flow Control (PFC) on the interface in auto-negotiation mode. This means the interface will automatically negotiate PFC with its link partner, allowing for lossless traffic handling by pausing specific traffic classes instead of dropping packets. The priority-flow-control watch-dog command enables the PFC watchdog for this interface, which ensures that if any PFC pause frames are stuck or persist for too long, the watchdog will take corrective action to prevent a deadlock situation. This helps maintain the overall health of the network by preventing traffic congestion or blockages due to PFC-related issues. Lastly, the service-policy type qos input QOS_CLASSIFICATION command applies the QoS classification policy on incoming traffic, ensuring that packets are classified and marked according to their DSCP values as defined in the QOS_CLASSIFICATION policy. This classification enables downstream QoS treatment, including proper queuing, scheduling, and priority handling. interface Ethernet 2/24 priority-flow-control mode auto priority-flow-control watch-dog service-policy type qos input QOS\\_CLASSIFICATION Example 11-7: Interface Level Configuration.\nReferences:\nhttps://nwktimes.blogspot.com/2025/04/congestion-avoidance-ai-fabric-part-i.html https://nwktimes.blogspot.com/2025/04/congestion-avoidance-in-ai-fabric-part.html https://nwktimes.blogspot.com/2025/04/congestion-avoidance-in-ai-fabric-part_15.html "},{"id":35,"href":"/tech-book/docs/ai-ml-dc/5-load-balancing-in-ai-fabric/","title":"Load Balancing in AI Fabric","section":"Data Center Networking for AI Clusters","content":" AI for Network Engineers: Understanding Flow, Flowlet, and Packet-Based Load Balancing # Though BGP supports the traditional Flow-based Layer 3 Equal Cost Multi-Pathing (ECMP) traffic load balancing method, it is not the best fit for a RoCEv2-based AI backend network. This is because GPU-to-GPU communication creates massive elephant flows, which RDMA-capable NICs transmit at line rate. These flows can easily cause congestion in the backend network.\nIn ECMP, all packets of a single flow follow the same path. If that path becomes congested, ECMP does not adapt or reroute traffic. This leads to uneven bandwidth usage across the network. Some links become overloaded, while others remain idle. In AI workloads, where multiple high-bandwidth flows occur at the same time, this imbalance can degrade performance.\nDeep learning models rely heavily on collective operations like all-reduce, all-gather, and broadcast. These generate dense traffic patterns between GPUs, often at terabit-per-second speeds. If these flows are not evenly distributed, a single congested path can slow down the entire training job.\nThis chapter introduces two alternative load balancing methods to traditional Flow-Based with Layer 3 ECMP: 1) Flowlet-Based Load Balancing with Adaptive Routing, and 2) Packet-Based Load Balancing with Packet Spraying. Both aim to improve traffic distribution in RoCEv2-based AI backend networks, where conventional flow-based routing often leads to congestion and underutilized links. These advanced methods are designed to handle the unique traffic patterns of AI workloads more efficiently.\nRDMA WRITE Operation # Before we explore the load balancing solution, let’s first walk through a simplified example of how the RDMA WRITE memory copy operation works. In Figure 12-1, we have two GPU servers: Host 1 and Host 2, each with one GPU. By this point, the memory has already been allocated and registered, and the Queue Pair (QP) has been created on both sides, so the data transfer can begin.\nOn GPU-0 of Host 1, gradients are stored in memory regions highlighted in green, orange, and blue. Each colored section represents a portion of local memory that will be written to GPU-0 on Host 2. To transfer the data, the RDMA NIC on Host 1 splits the write operation into three flowlets (green, orange, and blue). Rather than sending the entire data block as a single continuous stream, each flowlet is treated as a segment of the same RDMA Write operation.\nRDMA Write First # The first message carries the RDMA Extended Transport Header (RETH) in its payload. This header tells the receiving RDMA NIC where in the remote memory the incoming data should be written. In our example, data from memory block 1B of GPU-0 on Host 1 is written to memory block 2C of GPU-0 on Host 2.\nThe RETH contains the R_Key, which gives permission to write to the remote memory region. It also includes the length of the data being transferred and the virtual address of the target memory location on Host 2.\nThe operation code in the InfiniBand Base Transport Header (IBTH) is set to RDMA Write First, indicating that this is the first message in the sequence. The IBTH also describes the Partition Key (interface identifier), the Destination Queue Pair number, and the Packet Sequence Number (PSN) that helps ensure packets are processed in the correct order.\nWhen this first packet arrives at Host 2, the RDMA NIC uses the Virtual Address information in the RETH header to write the payload directly into memory block 2C.\nFigure 12-1: RDMA WRITE First.\nRDMA Write Middle # The second message has the opcode RDMA Write Middle and PSN 2, which tells the receiver that this packet comes after the first one with PSN 1. The payload of this Flowlet is written right after the previous block, into memory block 2D on Host 2. The RDMA NIC ensures that the order is maintained based on PSNs, and it knows exactly where to place the data thanks to the original offset from the first packet.\nFigure 12-2: RDMA WRITE Middle.\nRDMA Write Last # The third message has the opcode RDMA Write Last, indicating that this is the final message in the sequence. With PSN 3, it follows directly after PSN 2. The payload in this packet is written into memory block 2E, which comes directly after 2D.\nFigure 12-3: RDMA WRITE Last.\nIn a multi-packet RDMA Write operation, each Flowlet represents a continuous block of data being transferred from the source GPU to the destination GPU. Data within packets must arrive in the correct order because only the first packet includes the full addressing information in the RDMA Extended Transport Header (RETH). This header tells the receiver where in memory the data should be written.\nPackets marked as RDMA Write Middle and RDMA Write Last depend on this information and must follow the sequence defined by the Packet Sequence Numbers (PSNs). If packets are delivered out of order, the receiving RDMA NIC cannot process them immediately. Instead, it must hold them in memory and wait for the missing earlier packets to arrive. This buffering increases memory usage and processing overhead. In high-speed environments, this can lead to performance degradation or even packet drops, especially when buffers fill up under heavy load.\nFlow-Based Load Balancing with Layer 3 ECMP # Figure 12-4 depicts the problem with flow-based load balancing when used in an AI fabric backend network. In our example, we have four hosts, each equipped with two GPUs: GPU-1 and GPU-2. The RDMA NICs connected to GPU-1s are linked to switch Rail-1, and the RDMA NICs connected to GPU-2s are linked to Rail-2. Traffic between NICs on Rail-1 and Rail-2 is forwarded through either Spine-1 or Spine-2.\nWe use a basic data parallelization strategy, where the training dataset is divided into mini-batches and distributed across all eight GPUs. To keep the example simple, Figure 12-4 only shows the all-reduce gradient synchronization flow from the GPU-1s on Hosts 1, 2, and 3 to the GPU-2 on Host 4. In real-world training, a full-mesh all-reduce operation takes place between all GPUs.\nAs a starting point, the GPU-1s on the three leftmost hosts begin the RDMA process to copy data from their memory to the memory of GPU-2 on Host 4. These GPU-1s are all connected to Rail-1. Instead of sending one large flow, the RDMA NICs split the data into flowlets, small bursts of data from the larger transfer. These flowlets arrive at the Rail-1 switch, where the 5-tuple L3 ECMP hash algorithm unfortunately selects the same uplink for all three flows. Since the switch cannot forward all the data at wire speed, it stores some of the packets in the buffer, causing congestion. Similar congestion may also occur at the spine switches. As explained earlier in Chapter 12, egress buffer overflow may trigger ECN (Explicit Congestion Notification) and PFC (Priority Flow Control) mechanisms to prevent packet loss. Figure 12-4: Layer 3 Load balancing.\nFlowlet-Based Load Balancing with Adaptive Routing # Adaptive routing is a dynamic method that actively monitors link utilization and reacts to network congestion in real time. In Figure 12-5, the 5-tuple hash algorithm initially selects the same uplink for all flowlets, just like in the previous example. However, once the utilization of the inter-switch link between Rail-1 and Spine-1 goes over threshold, the adaptive routing mechanism detects the increased load and starts redirecting some of the flowlets to an alternate, less congested path, through Spine-2.\nBy distributing the flowlets across multiple paths, adaptive routing helps to reduce buffer buildup and avoid potential packet drops. This not only improves link utilization across the fabric but also helps maintain consistent throughput for time-sensitive operations like RDMA-based gradient synchronization. In AI workloads, where delays or packet loss can slow down or even interrupt training, adaptive routing plays a critical role in maintaining system performance.\nFigure 12-5: Dynamic Flow Balancing.\nPacket-Based Load Balancing with Packet Spraying # Packet spraying is a load balancing method where individual packets from the same flow are distributed across multiple equal-cost paths. The idea is to use all available links evenly and reduce the chance of congestion on any single path.\nIn a RoCEv2-based AI backend network, however, packet spraying presents serious challenges. RoCEv2 relies on lossless and in-order packet delivery. When packets are sprayed over different paths, they can arrive out of order at the destination. This packet reordering can disrupt RDMA operations and reduce the overall performance of GPU-to-GPU communication.\nFigure 12-6: Packet Spraying: OpCode: RDMA Write First, Middle, and Last.\nRDMA Write Only # NVIDIA’s RDMA NICs starting from ConnectX-5 support the RDMA Write Only operation, where a RETH header is included in every packet. Figure 12-7 shows how the RDMA NIC uses the OpCode: RDMA Write Only in the IBTH header for each message. With this OpCode, every message also includes a RETH header, which holds information about the destination memory block reserved for the data carried in the payload. This allows the receiving RDMA NIC to write data directly to the correct memory location without relying on prior messages in the transfer sequence.\nRDMA Write Only, when combined with Packet-Based Load Balancing using Packet Spraying, brings significant benefits. Since each packet is self-contained and includes full memory addressing information, the network fabric can forward individual packets over different paths without worrying about packet ordering or context loss. This enables true flowlet or even per-packet load balancing, which helps spread traffic more evenly across available links, avoids hotspots, and reduces queuing delays. Figure 12-7: Packet Spraying: OpCode: TDMA Write Only.\nConfiguring Per-Packet Load Balancing on Cisco Nexus Switches # At the time of writing, Cisco Nexus 9000 Series Cloud Scale switches (9300-FX3, GX, GX2, and HX-TOR), starting from NX-OS Release 10.5(1)F, support Dynamic Load Balancing (DLB)—including flowlet-based and per-packet (packet spraying) load balancing. DLB is supported on Layer 3 physical interfaces in IP-routed and VXLAN fabrics for unicast IPv4 and IPv6 traffic.\nWhen DLB is enabled, egress QoS and access policies are not applied to flows using DLB. Similarly, TX SPAN configured on an egress interface does not capture DLB traffic. For hardware and software support details, refer to Cisco’s official documentation.\nExample 12-1 shows a basic configuration for enabling per-packet load balancing:\nswitch(config)# hardware profile dlb switch(config-dlb)# dlb-interface Eth1/1 switch(config-dlb)# dlb-interface Eth1/2 switch(config-dlb)# mac-address aa:bb:cc:dd:ee:ff switch(config-dlb)# mode per-packet Example 12-1: Configuring Per-Packet Load Balancing Packet Spraying.\nNote: The DLB MAC acts as a virtual next-hop MAC address. It’s not tied to any specific physical interface. This decouples the MAC from the physical path, allowing the switch to choose a different egress port for each packet. The same DLB MAC address must be configured on all participating switches. If you do not specify a DLB MAC, the default DLB MAC 00:CC:CC:CC:CC:CC is applied.\nReferences:\nhttps://nwktimes.blogspot.com/2025/04/ai-for-network-engineers-understanding.html "},{"id":36,"href":"/tech-book/docs/ai-ml-dc/6-backend-network-topologies-for-ai-fabric/","title":"Backend Network Topologies for AI Fabrics","section":"Data Center Networking for AI Clusters","content":" Backend Network Topologies for AI Fabrics # Although there are best practices for AI Fabric backend networks, such as Data Center Quantized Congestion Control (DCQCN) for congestion avoidance, rail-optimized routed Clos fabrics, and Layer 2 Rail-Only topologies for small-scale implementations, each vendor offers its own validated design. This approach is beneficial because validated designs are thoroughly tested, and when you build your system based on the vendor’s recommendations, you receive full vendor support and avoid having to reinvent the wheel.\nHowever, instead of focusing on any specific vendor’s design, this chapter explains general design principles for building a resilient, non-blocking, and lossless Ethernet backend network for AI workloads.\nBefore diving into backend network design, this chapter first provides a high-level overview of a GPU server based on NVIDIA H100 GPUs. The first section introduces a shared NIC architecture, where 8 GPUs share two NICs. The second section covers an architecture where each of the 8 GPUs has a dedicated NIC.\nShared NIC # Figure 13-1 illustrates a shared NIC approach. In this example setup, NVIDIA H100 GPUs 0–3 are connected to NVSwitch chips 1-1, 1-2, 1-3, and 1-4 on baseboard-1, while GPUs 4–7 are connected to NVSwitch chips 2-1, 2-2, 2-3, and 2-4 on baseboard-2. Each GPU connects to all four NVSwitch chips on its respective baseboard using a total of 18 NVLink 4 connections: 5 links to chip 1-1, 4 links to chip 1-2, 4 links to chip 1-3, and 5 links to chip 1-4.\nThe NVSwitch chips themselves are paired between the two baseboards. For example, chip 1-1 on baseboard-1 connects to chip 2-1 on baseboard-2 with four NVLink connections, chip 1-2 connects to chip 2-2, and so on. This design forms a fully connected crossbar topology across the entire system.\nThanks to this balanced pairing, GPU-to-GPU communication is very efficient whether the GPUs are located on the same baseboard or on different baseboards. Each GPU can achieve up to 900 GB/s of total GPU-to-GPU bandwidth at full NVLink 4 speed.\nFor inter-GPU server connection, GPUs are also connected to a shared NVIDIA ConnectX-7 200 GbE NIC through a PEX89144 PCIe Gen5 switch. Each GPU has a dedicated PCIe Gen5 x16 link to the switch, providing up to 64 GB/s of bidirectional bandwidth (32 GB/s in each direction) between the GPU and the switch. The ConnectX-7 (200Gbps) NIC is also connected to the same PCIe switch, enabling high-speed data transfers between remote GPUs and the NIC through the PCIe fabric.\nWhile each GPU benefits from a high-bandwidth, low-latency PCIe connection to the switch, the NIC itself has a maximum network bandwidth of 200 GbE, which corresponds to roughly 25 GB/s. Therefore, the PCIe switch is not a bottleneck; instead, the NIC’s available bandwidth must be shared among all eight GPUs. In scenarios where multiple GPUs are sending or receiving data simultaneously, the NIC becomes the limiting factor, and the bandwidth is divided between the GPUs.\nIn real-world AI workloads, however, GPUs rarely saturate both the PCIe interface and the NIC at the same time. Data transfers between the GPUs and the NIC are often bursty and asynchronous, depending on the training or inference pipeline stage. For example, during deep learning training, large gradients might be exchanged periodically, but not every GPU constantly sends data at full speed. Additionally, many optimizations like gradient compression, pipeline parallelism, and overlapping computation with communication further reduce the likelihood of sustained full-speed congestion.\nAs a result, even though the NIC bandwidth must be shared, the shared ConnectX-7 design generally provides sufficient network performance for typical AI workloads without significantly impacting training or inference times.\nIn high-performance environments, such as large-scale training workloads or GPU communication across nodes, this shared setup can become a bottleneck. Latency may increase under load, and data transfer speeds can slow down. Despite these challenges, the design is still useful in many cases. It is well-suited for development environments, smaller models, or setups where cost is a primary concern. If the workload does not require maximum GPU-to-network performance, sharing a NIC across GPUs can be a reasonable and efficient solution. However, for optimal performance and full support for technologies like GPUDirect RDMA, it is better to use a dedicated NIC for each GPU. Figure 13-1: Shared NIC GPU Server.\nNIC per GPU # Figure 13-2 builds on the shared NIC design from Figure 13-1 but takes a different approach. In this setup, each GPU has its own dedicated ConnectX-7 200 GbE NIC. All NICs are connected to the PCIe Gen5 switch, just like in the earlier setup, but now each GPU uses its own PCIe Gen5 x16 connection to a dedicated NIC. This design eliminates the need for NIC sharing and allows every GPU to use the full 64 GB/s PCIe bandwidth independently.\nThe biggest advantage of this design is in GPU-to-NIC communication. There is no bandwidth contention at the PCIe level, and each GPU can fully utilize RDMA and GPUDirect features with its own NIC. This setup improves network throughput and reduces latency, especially in multi-node training workloads where GPUs frequently send and receive large amounts of data over Ethernet. The main drawback of this setup is cost. Adding one NIC per GPU increases both hardware costs and power consumption. It also requires more switch ports and cabling, which may affect system design. Still, these trade-offs are often acceptable in performance-critical environments.\nThis overall design reflects NVIDIA’s DGX and HGX architecture, where GPUs are fully interconnected using NVLink and NVSwitch and each GPU is typically paired with a dedicated ConnectX or BlueField NIC to maximize network performance. In addition, this configuration is well suited for rail-optimized backend networks, where consistent per-GPU network bandwidth and predictable east-west traffic patterns are important.\nFigure 13-2: Dedicated NIC per GPU.\nBefore moving to the design sections, it is worth mentioning that the need for a high-performance backend network, and how it is designed, is closely related to the size of the neural networks being used. Larger models require more GPU memory and often must be split across multiple GPUs or even servers. This increases the need for fast, low-latency communication between GPUs, which puts more pressure on the backend network.\nFigure 13-3 shows a GPU server with 8 GPUs. Each GPU has 80 GB of memory, giving a total of 640 GB GPU memory. This kind of setup is common in high-performance AI clusters.\nThe figure also shows three examples of running large language models (LLMs) with different parameter sizes:\n8B model: This model has 8 billion parameters and needs only approximately 16 GB of memory. It fits on a single GPU if model parallelism is not required. 70B model: This larger model has 70 billion parameters and needs approximately 140 GB of memory. It cannot fit into one GPU, so it must use at least two GPUs. In this case, the GPUs communicate using intra-host GPU connections across NVLink. 405B model: This large model has 405 billion parameters and needs approximately 810 GB of memory. It does not fit into one server. Running this model requires at least 10 GPUs across multiple servers. The GPUs must use both intra-GPU connections inside a server and inter-GPU connections between servers. This figure highlights how model size directly affects memory needs, and the number of GPUs required. As models grow, parallelism and fast GPU interconnects become essential.\nFigure 13-3: Model Size and Required GPUs.\nDesign Scenarios # Single Rail Switch Design with Dedicated, Single-Port NICs per GPU # Figure 13-4 illustrates a single rail switch design. The switch interfaces are divided into three groups of eight 200 Gbps interface each. The first group of eight ports is reserved for Host-1, the second group for Host-2, and the third group for Host-3. Each host has eight GPUs, and each GPU is equipped with a dedicated, single-port NIC.\nWithin each group, ports are assigned to different VLANs to separate traffic into different logical rails. Specifically, the first port of each group belongs to the VLAN representing Rail-1, the second port belongs to Rail-2, and so on. This pattern continues across all three host groups.\nBenefits # Simplicity: The architecture is very easy to design, configure, and troubleshoot. A single switch and straightforward VLAN assignment simplify management. Cost-Effectiveness: Only one switch is needed, reducing capital expenditure (CapEx) compared to dual-rail or redundant designs. Less hardware also means lower operational expenditure (OpEx), including reduced power, cooling, and maintenance costs. Additionally, fewer devices translate to lower subscription-based licensing fees and service contract costs, further improving the total cost of ownership. Efficient Use of Resources: Ports are used efficiently by directly mapping each GPU’s NIC to a specific port on the switch, minimizing wasted capacity. Low Latency within the Rail: Since all communications stay within the same switch, latency is minimized, benefiting tightly-coupled GPU workloads. Sufficient for Smaller Deployments: In smaller clusters or test environments where absolute redundancy is not critical, this design is perfectly sufficient. Drawbacks # No Redundancy: A single switch creates a single point of failure. If the switch fails, all GPU communications are lost. Limited Scalability: Expanding beyond the available switch ports can be challenging. Adding more hosts or GPUs might require replacing the switch or redesigning the network. Potential Oversubscription: With all GPUs sending and receiving traffic through the same switch, there’s a risk of oversubscription, especially under heavy AI workload patterns where network traffic bursts are common. Difficult Maintenance: Software upgrades or hardware maintenance on the switch impact all connected hosts, making planned downtime more disruptive. Not Suitable for High Availability (HA) Requirements: Critical AI workloads, especially in production environments, often require dual-rail (redundant) networking to meet high availability requirements. This design would not meet such standards. Single rail designs are cost-efficient and simple but lack redundancy and scalability, making them best suited for small or non-critical AI deployments.\nFigure 13-4: Single Rail Switch Design: GPU with Single Port NIC.\nDual-Rail Switch Topology with Dedicated, Dual-Port NICs per GPU # In this topology, each host contains 8 GPUs, and each GPU has a dedicated dual-port NIC. The NICs are connected across two independent Rail switches equipped with 200 Gbps interfaces. This design ensures that every GPU has redundant network connectivity through separate switches, maximizing performance, resiliency, and failover capabilities.\nEach Rail switch independently connects to one port of each NIC, creating a dual-homed connection per GPU. To ensure seamless operations and redundancy, the two switches must logically appear as a single device to the host NICs, even though they are physically distinct systems.\nBenefits # High Availability: The failure of a single switch, link, or NIC port does not isolate any GPU, maintaining system uptime. Load Balancing: Traffic can be distributed across both switches, maximizing bandwidth utilization and reducing bottlenecks. Scalability: Dual-rail architectures can be extended easily to larger deployments while maintaining predictable performance and redundancy. Operational Flexibility: Maintenance can often be performed on one switch without service disruption. Drawbacks # Higher Cost: Requires two switches, twice the number of cables, and dual-port NICs, increasing CapEx and OpEx. Complexity: Managing a dual-rail environment introduces more design complexity due to Multi-Chassis Link Aggregation (MLAG). Increased Power and Space Requirements: Two switches and more cabling demand more rack space, power, and cooling. Challenges of Multi-Chassis Link Aggregation (MLAG) # To create a logical channel between dual-port NICs and two switches, the switches must be presented as a single logical device to each NIC. Multi-Chassis Link Aggregation (MLAG) is often used for this purpose. MLAG allows a host to see both switch uplinks as part of the same LAG (Link Aggregation Group).\nAnother solution is to assign the two NIC ports to different VLANs without bundling them into a LAG, though this approach may limit bandwidth utilization and redundancy benefits compared to MLAG.\nMLAG introduces several challenges:\nMAC Address Synchronization: Both switches must advertise the same MAC address to the host NICs, allowing the two switches to appear as a single device. Port Identification: A common approach to building MLAG is to use the same interface numbers on both switches. Therefore, the system must be capable of uniquely identifying each member link internally. Control Plane Synchronization: The two switches must exchange state information (e.g., MAC learning, link status) to maintain a consistent and synchronized view of the network. Failover Handling: The switches must detect failures quickly and handle them gracefully without disrupting existing sessions, requiring robust failure detection and recovery mechanisms. Vendor-Specific MLAG Solutions # The following list shows some of the vendor proprietary MLAG:\nCisco Virtual Port Channel (vPC): Cisco\u0026rsquo;s vPC allows two Nexus switches to appear as one logical switch to connected devices, synchronizing MAC addresses and forwarding state. Juniper Virtual Chassis / MC-LAG: Juniper offers Virtual Chassis and MC-LAG solutions, where two or more switches operate with a shared control plane, presenting themselves as a single switch to the host. Arista MLAG: Arista Networks implements MLAG with a simple peer-link architecture, supporting independent control planes while synchronizing forwarding state. NVIDIA/Mellanox MLAG: Mellanox switches also offer MLAG solutions, often optimized for HPC and AI workloads. Standards-Based Alternative: EVPN ESI Multihoming # Instead of vendor-specific MLAG, a standards-based approach using Ethernet Segment Identifier (ESI) Multihoming under BGP EVPN can be used. In this model:\nSwitches advertise shared Ethernet segments (ESIs) to the host over BGP EVPN. Hosts see multiple physical links but treat them as part of a logical redundant connection. EVPN ESI Multihoming allows for interoperable solutions across vendors, but typically adds more complexity to the control plane compared to simple MLAG setups. Figure 13-5: Dual Rail Switch Design: GPU with Dual-Port NIC.\nCross-Rail Communication over NVLink in Rail-Only Topologies # In the introduced single- and dual-rail topologies (Figures 13-4 and 13-5), each GPU is connected to a dedicated NIC, and each NIC connects to a specific Rail switch. However, there is no direct cross-rail connection between the switches themselves — no additional spine layer interconnecting the rails. As a result, if a GPU needs to send data to a destination GPU that belongs to a different rail, special handling is required within the host before the data can exit over the network.\nFor example, consider a memory copy operation where GPU-2 (connected to Rail 3) on Host-1 needs to send data to GPU-3 (connected to Rail 4) on Host-2. Since GPU-2’s NIC is associated with Rail 3 and GPU-3 expects data arriving over Rail 4, the communication path must traverse multiple stages:\nIntra-Host Transfer: The data is first copied locally over NVLink from GPU-2 to GPU-3 within Host-1. NVLink provides a high-bandwidth, low-latency connection between GPUs inside the same server. NIC Transmission: Once the data resides in GPU-3’s memory, it can be sent out through GPU-3’s NIC, which connects to Rail 4. Inter-Host Transfer: The packet travels over Rail 4 through one of the Rail switches to reach Host-2. Destination Reception: Finally, the data is delivered to GPU-3 on Host-2. This method ensures that each network link (and corresponding NIC) is used according to its assigned rail without needing direct switch-to-switch rail interconnects.\nTo coordinate and optimize such multi-step communication, NVIDIA Collective Communications Library (NCCL) plays a critical role. NCCL automatically handles GPU-to-GPU communication across multiple nodes and rails, selecting the appropriate path, initiating memory copies over NVLink, and scheduling transmissions over the correct NICs — all while maximizing bandwidth and minimizing latency. The upcoming chapter will explore NCCL in greater detail.\nFigure 13-6 illustrates how the upcoming topology in Figure 13-7 maps NIC-to-Rail connections, transitioning from a switch interface-based view to a rail-based view. Figure 13-6 shows a partial interface layout of a Cisco Nexus 9348D-GX2A switch and how its interfaces are grouped into different rails as follows:\nRail-1 Interfaces: 1, 4, 7, 10 Rail-2 Interfaces: 13, 16, 19, 22 Rail-3 Interfaces: 25, 28, 31, 34 Rail-4 Interfaces: 37, 40, 43, 46 Rail-5 Interfaces: 2, 5, 8, 11 Rail-6 Interfaces: 14, 17, 20, 23 Rail-7 Interfaces: 26, 29, 32, 35 Rail-8 Interfaces: 38, 41, 44, 47 However, a port-based layout becomes extremely messy when describing larger implementations. Therefore, the common practice is to reference the rail number instead of individual switch interface identifiers.\nFigure 13-6: Interface Block to Rail Mapping.\nFigure 13-7 provides an example showing how each NIC is now connected to a rail instead of being directly mapped to a specific physical interface. In this approach, each rail represents a logical group of physical interfaces, simplifying the overall design and making larger deployments easier to visualize and document.\nIn our example \u0026ldquo;Host-Segment\u0026rdquo; (an unofficial name), we have four hosts, each equipped with eight GPUs — 32 GPUs in total. Each GPU has a dedicated 200 Gbps dual-port NIC. All GPUs are connected to two rail switches over a 2 × 200 Gbps MLAG, providing 400 Gbps of transmission speed per GPU.\nFigure 13-7: Example Figure of Connecting 32 Dual-Port NICs 8 Rails on 2 Switches.\nFigure 13-8 shows how multiple Host-Segments can be connected. The figure illustrates a simplified two-tier, three-stage Clos fabric topology, where full-mesh Layer 3 links are established between the four Rail switches (leaf switches) and the Spine switches. The figure also presents the link capacity calculations. Each Rail switch has 32 × 100 Gbps connections to the hosts, providing a total downlink capacity of 3.2 Tbps.\nSince oversubscription is generally not preferred in GPU clusters — to maintain high performance and low latency — the uplink capacity from each Rail switch to the Spine layer must also match 3.2 Tbps. To achieve this, each Rail switch must have uplinks capable of an aggregate transfer rate of 3.2 Tbps. This can be implemented either by using native 800 Gbps interfaces or by forming a logical Layer 3 port channel composed of two 400 Gbps links per Spine connection. Additionally, Inter-Switch capacity can be increased by adding more switches in the Spine layer. This is one of the benefits of a Clos fabric: the capacity can be scaled without the need to replace 400 Gbps interfaces with 800 Gbps interfaces, for example.\nThis topology forms a Pod and supports 64 GPUs in total and provides a non-blocking architecture, ensuring optimal east-west traffic performance between GPUs across different Host-Segments.\nIn network design, the terms \u0026ldquo;two-tier\u0026rdquo; and \u0026ldquo;three-stage\u0026rdquo; Clos fabric describe different aspects of the same overall topology. \u0026ldquo;Two-tier\u0026rdquo; focuses on the physical switch layers (typically Leaf and Spine) and describes the depth of the topology, offering a hierarchy view of the architecture. Essentially, it\u0026rsquo;s concerned with how many switching layers are present. On the other hand, three-stage Clos describes the logical data path a packet follows when moving between endpoints: Leaf–Spine–Leaf. It focuses on how data moves through the network and the stages traffic flows through. Therefore, while a two-tier topology refers to the physical switch structure, a three-stage Clos describes the logical path taken by packets, which crosses through three stages: Leaf, Spine, and Leaf. These two perspectives are complementary, not contradictory, and together they provide a complete view of the Clos network design.\nFigure 13-8: AI fabric – Pod Design.\nFigure 13-9 extends the previous example by adding a second 64-GPU Pod, creating a larger multi-Pod architecture. To interconnect the two Pods, four Super-Spine switches are introduced, forming an additional aggregation layer above the Spine layer. Each Pod retains its internal two-tier Clos fabric structure, with Rail switches fully meshed to the Spine switches as described earlier. The Spine switches from both Pods are then connected northbound to the Super-Spine switches over Layer 3 links.\nDue to the introduction of the Super-Spine layer, the complete system now forms a three-tier, five-stage Clos topology. This design supports scalable expansion while maintaining predictable latency and high bandwidth between GPUs across different Pods. Similar to the Rail-to-Spine design, maintaining a non-blocking architecture between the Spine and Super-Spine layers is critical. Each Spine switch aggregates 3.2 Tbps of traffic from its Rail switches; therefore, the uplink capacity from each Spine to the Super-Spine layer must also be 3.2 Tbps.\nThis can be achieved either by using native 800 Gbps links or logical Layer 3 port channels composed of two 400 Gbps links per Super-Spine connection. All Spine switches are fully meshed with all Super-Spine switches to ensure high availability and consistent bandwidth. This architecture enables seamless east-west traffic between GPUs located in different Pods, ensuring that inter-Pod communication maintains the same non-blocking performance as intra-Pod traffic.\nFigure 13-9: AI fabric – Multi-Pod Design.\nIn this chapter, we focus mainly on different topology options, such as Single Rail with Single-Port GPU NIC, Dual Rail Switch with Dual-Port GPU NIC, Cross-Rail Over Layer 3 Clos fabric, and finally, Inter-Pod architecture. The next chapter will delve more in-depth into the technical solutions and challenges.\nReferences:\nhttps://nwktimes.blogspot.com/2025/04/ai-fabric-backend-network-topologies.html "},{"id":37,"href":"/tech-book/docs/ai-ml-dc/7-backend-network-in-gpu-fabric/","title":"Backend Network/Rail Designs","section":"Data Center Networking for AI Clusters","content":" Clos vs Rail-Optimised Design for Backend Network # CPU cluster with a Clos Network # Conventional networked clusters are designed to serve CPUheavy workloads using a multi-layer Clos network, illustrated in Figure 1. This architecture, known as a Fat-Tree network, is deeply studied in the system and networking communities. In a typical Fat-Tree-based cluster, each server is equipped with one NIC (40 Gbps to 400 Gbps), and K servers are arranged into racks connecting to Top-ofthe-Rack (ToR) switches. The ToR switches are then connected to the aggregation switches to provide connectivity across racks, forming a pod. Finally, the pods are interconnected with spine switches, allowing any-to-any communication across servers in a CPU cluster.\nRail-Optimised Network for GPU Clusters # In contrast, the rise of network-heavy ML workloads led to the dominance of GPU-centric clusters, where individual GPUs have dedicated NICs. Figure 2 illustrates the network architecture of a typical GPU cluster. Each GPU has two different communication interfaces: (i) An NVLink interface to support high-bandwidth but short-range interconnection and (ii) a conventional RDMA-enabled NIC. The NVLinks connect K GPUs to provide terabits of non-blocking any-to-any bandwidth in/out per GPU (7.68 Tbps for fourthgen NVLink, for instance). This group of GPUs with fast interconnect forms a high-bandwidth domain (HB domain). Traditionally, HB domains were restricted to a single server (e.g., DGX servers with K = 8 or 16 GPUs). However, recently, Nvidia announced the GH200 supercomputer interconnecting K = 256 Grace Hopper Superchips to form one HB domain across multiple racks.\nHowever, some LLMs take too long for a single HB domain to train, even with 256 GPUs. For instance, the PaLM540B model would take 117 days to finish on a GH200 supercomputer, assuming perfect GPU utilization. These models require parallelization across multiple HB domains.\nTo enable training an LLM across multiple HB domains, GPU cluster operators use RDMA-capable NICs to interconnect multiple HB domains together. The conventional network architecture to interconnect HB domains is called a rail-optimized network. In a rail-optimized architecture, GPUs within an HB domain are labeled from 1 to K. A rail is the set of GPUs with the same index (or rank) on different HB domains, interconnected with a rail switch. For instance, Figure 2 illustrates Rail 1 and Rail K in red and yellow color, respectively. These rail switches are subsequently connected to spine switches to form a full-bisection any-to-any Clos network topology. This network ensures any pair of GPUs in different HB domains can communicate at the network line rate (400 Gbps Infiniband network for GH200). For instance, traffic between GPU 1, Domain 1 and GPU 1, Domain 2 traverses through Rail Switch 1 only, while traffic between GPU 1, Domain 1 and GPU 2, Domain 2 goes through the respective rails and the spine switches.\nRail Designs in GPU Fabric # When building a scalable, resilient GPU network fabric, the design of the rail layer, the portion of the topology that interconnects GPU servers via Top-of-Rack (ToR) switches, plays a critical role. This section explores three different models: Multi-rail-per-switch, Dual-rail-per-switch, and Single-rail-per-switch. All three support dual-NIC-per-GPU designs, allowing each GPU to connect redundantly to two separate switches, thereby removing the Rail switch as a single point of failure.\nMulti-Rail-per-Switch # In this model, multiple small subnets and VLANs are configured per switch, with each logical rail mapped to a subset of physical interfaces. For example, a single 48-port switch might host four or eight logical rails using distinct Layer 2 and Layer 3 domains. Because all logical rails share the same physical device, isolation is logical. As a result, a hardware or software failure in the switch can impact all rails and their associated GPUs, creating a large failure domain.\nThis model is not part of NVIDIA’s validated Scalable Unit (SU) architecture but may suit test environments, development clusters, or small-scale GPU fabrics where hardware cost efficiency is a higher priority than strict fault isolation. From a CapEx perspective, multi-rail-per-switch is the most economical, requiring fewer switches. Figure 13-10 illustrates the multi-rail-per-switch architecture, where each rail is implemented as a separate VLAN-subnet pair mapped to a subset of switch ports. In the figure, interfaces 1–4 are assigned to subnet 10.0.1.0/28 and VLAN 101, while interfaces 5–8 are mapped to subnet 10.0.2.0/28 and VLAN 102. Each VLAN maintains its own MAC address table, learning GPU NIC MACs through ingress traffic. Although not shown in the figure, the Rail switch acts as the default gateway for all eight VLANs.\nThe figure also illustrates the BGP process when a Clos architecture with a spine layer is used to connect rail switches. All directly connected subnets are installed into the local Routing Information Base (RIB) as connected routes. These routes are then imported into the BGP Loc-RIB. Next, the routes pass through the BGP output policy engine, where they are aggregated into a single summary route: 10.0.1.0/24. This aggregate is placed into the BGP Adj-RIB-Out. When the BGP Update message is sent to a peer, the NEXT_HOP attribute is set accordingly.\nFigure 13-10: Multi-Rail per Switch.\nDual-Rail-per-Switch # While dual-rail-per-switch improves manageability and is easier to scale, it shares the same limitation: both logical rails reside within a single physical switch, so the failure domain remains large. A single switch failure or misconfiguration affects both rails and all associated GPUs. This design resembles the dual-rail concept used in scalable AI clusters, but NVIDIA’s SU approach calls for two separate physical switches per rail, which provides full physical isolation. Dual-rail-per-switch hits a middle ground in terms of CapEx and OpEx: fewer switches are required than in the single-rail model, and operational complexity is reduced compared to multi-rail. It’s often a good choice for intermediate-scale environments where some fault tolerance and cost control must be balanced. Figure 13-11 illustrates a dual-rail-per-switch design, where the switch interfaces are divided evenly between two separate rails. Rail 1 uses interfaces 1 through 16 and is assigned to subnet 10.0.1.0/25 (VLAN 101). Rail 2 uses interfaces 17 through 32 and is assigned to subnet 10.0.128.0/25 (VLAN 102). Each VLAN has its own MAC address table, and the rail switch serves as the default gateway for both. The individual /25 subnets are redistributed into the BGP process and summarized as 10.0.1.0/24 for advertisement toward the spine layer.\nFigure 13-11: Dual-Rail Switch.\nSingle-Rail-per-Switch # This model offers the highest level of physical isolation. Each switch forms a single rail, serving its connected GPU servers through one subnet and one VLAN. No logical separation is needed, as each rail is entirely independent in hardware. As a result, a switch failure affects only the GPU servers attached to that specific rail, yielding a small, predictable failure domain.\nThe design closely aligns with NVIDIA’s Scalable Unit (SU) architecture, in which each rack or rack group includes its own rail switch, and horizontal scaling is achieved by repeating modular, self-contained units.\nWhile this model demands the highest CapEx, due to the one-to-one mapping between switches and rails, it offers major operational advantages. Configuration is simpler, troubleshooting is faster, and the risk of cascading faults is minimized. There is no need for route summarization, or custom BGP redistribution logic. Over time, these benefits help drive down OpEx, particularly in large-scale or mission-critical GPU clusters.\nTo ensure optimal hardware utilization, it is important to align the number of GPU servers per rack with the switch’s port capacity. Otherwise, underutilized ports can lead to inefficiencies in infrastructure cost and resource planning.\nFigure 13-12 illustrates a simplified single-rail-per-switch topology. All interfaces from 1 to 32 operate within a single rail, configured with subnet 10.0.1.0/24 and VLAN 101. The rail switch serves as the default gateway, and because the full /24 subnet is used without subnetting, route summarization is not needed.\nFigure 13-12: Single-Rail Switch.\nAI Fabric Architecture Conclusion # Figure 13-13 illustrates one way to describe the overall architecture of an AI Fabric. It is divided into three domains. The first domain, called the Segment, includes GPU hosts and Rail switches. The second domain, the Pod, aggregates multiple segments using Spine switches. In cases where NCCL builds a topology where cross-rail inter-host traffic is first copied to the local GPU memory (located on the destination rail) and then sent over the GPU NIC to the remote GPU via the correct Rail switch, a Pod architecture with Spine switches may not be necessary. The third domain, multi-Pod, interconnects multiple pods using Super Spine switches, enabling large-scale AI Fabric deployments. Figure 13-10 also depicts global settings and properties shared across the AI Fabric backend network.\nSegment: GPU I/O Topology and Rail Switch Fabric Profile # GPU I/O Topology: Each GPU connects to the network through a NIC. You can either dedicate a NIC to each GPU or share one NIC among multiple GPUs. NICs may have single, dual, or quad ports and support speeds such as 100, 200, or 400 Gbps. The interconnect type can be InfiniBand, RoCEv2, or NVLink. A segment typically includes multiple hosts.\nRail Switch Fabric Profile: Rail switches connect directly to GPU hosts. Each rail handles a group of NIC ports. You can map rails one-to-one to switches for physical isolation or map multiple rails per switch for logical isolation. In the latter case, two or more rails can be mapped per switch depending on performance and capacity requirements. Rail switches are responsible for ingress packet classification and for mapping RoCEv2 traffic to the correct queues. Pod: Spine Switch Profile: # Spine switches aggregate multiple Rail switches, forming a Pod that consists of n segments. Spine switches enable cross-rail communication between GPUs. They use high-density, high-speed ports. When the Spine layer is used, the result is a 2-tier, 3-stage architecture.\nMulti-Pod: Super Spine Switch Profile # Super Spine switches provide inter-Pod connectivity. They are built with very high port density to support all connected Spine switches. When the Super Spine layer is used, the architecture becomes a 3-tier, 5-stage fabric.\nGlobal AI Fabric Profile # All layers are governed by the Global AI Fabric Profile. This profile defines the control plane (eBGP, iBGP, BGP EVPN), the data plane (Ethernet, VXLAN), Layer 3 ECMP strategies (flow-based, flowlet-based, or per-packet), congestion control mechanisms (ECN marking, PFC), inter-switch link monitoring (BFD), and global MTU settings.\nFigure 13-13: AI fabric Architecture Description.\nReferences # https://nwktimes.blogspot.com/2025/05/ai-for-network-engineers-rail-desings.html Optimized Network Architectures for Training Large Language Models With Billions of Parameters, (Mirror-Copy) YouTube - Between 0x2 Nerds - A Tale of Two Switches: ToRs vs. Rails w/Petr Lapukov "},{"id":38,"href":"/tech-book/posts/2022-01-18-first-doc/","title":"First Blog","section":"Blog","content":" Preface # This is my black board for my future technical book. There is no structure of this blog posts. Whenever I find a good technical literature, I am planning to add it here.\nFeedback is very important for any development cycle. Please drop a message at prasenjit.manna@gmail.com.\nThanks, Prasenjit Manna\n"},{"id":39,"href":"/tech-book/docs/5g/","title":"5G","section":"Example Site","content":" 5G Introduction # "},{"id":40,"href":"/tech-book/docs/algorithms/","title":"Algorithms","section":"Example Site","content":" Algorithms # In this sections, all the interesting algorithms will be classified into simple, medium and hard.\n"},{"id":41,"href":"/tech-book/docs/ai-ml-dc/","title":"Data Center Networking for AI Clusters","section":"Example Site","content":" Data Center Networking for AI Clusters # In this sections, all the data center topics will be classified into simple, medium and hard.\n"},{"id":42,"href":"/tech-book/docs/data-center/","title":"Data Center Tips","section":"Example Site","content":" Data Center Topics # In this sections, all the data center topics will be classified into simple, medium and hard.\n"},{"id":43,"href":"/tech-book/docs/manageability/","title":"Manageability","section":"Example Site","content":" Manageability Topics # In this sections, all the manageability topics will be classified into simple, medium and hard.\n"},{"id":44,"href":"/tech-book/docs/networking-tips/","title":"Networking Tips","section":"Example Site","content":" Networking Tips # In this sections, all the interesting networking tips will be classified into simple, medium and hard.\n"},{"id":45,"href":"/tech-book/docs/optical-knowledge/","title":"Optical Knowledge","section":"Example Site","content":" Optical Knowledge Topics # "},{"id":46,"href":"/tech-book/docs/programming-tips/","title":"Programming Tips","section":"Example Site","content":" Programming Tips # Bit Manipulation # XORing a bit with 1 always flips the bit, whereas XO Ring with O will never change it. Miscellaneous # Passing a 2D array to a C++ function\nThere are three ways to pass a 2D array to a function:\nThe parameter is a 2D array\nint array[10][10]; void passFunc(int a[][10]) { // ... } passFunc(array); The parameter is an array containing pointers\nint *array[10]; for(int i = 0; i \u0026lt; 10; i++) array[i] = new int[10]; void passFunc(int *a[10]) //Array containing pointers { // ... } passFunc(array); The parameter is a pointer to a pointer\nint **array; array = new int *[10]; for(int i = 0; i \u0026lt;10; i++) array[i] = new int[10]; void passFunc(int **a) { // ... } passFunc(array); "},{"id":47,"href":"/tech-book/docs/systemdesign-tips/","title":"SystemDesign-Tips","section":"Example Site","content":" System-Tips # In this sections, all the essential concents will be described.\nStorage # Disk - HDD(Hard-disk drive) and SSD(solid state drive). SSD is faster than HDD, hence costlier also. Persistent Storage. Memory - RAM (Random access momory). Volatile storage Latency and Throughput # Latency - Time it takes for a certain operation to complete, unit msec or sec.\nReading 1 MB from RAM: 250 us (0.25ms)\nReading 1 MB from SSD: 1,000 ps (1 ms)\nReading 1 MB from HDD: 20,000 is (20 ms)\nTransfer 1 MB over Network: 10,000 pus (10 ms)\nInter-Continental Round Trip: 150,000 ps (150 ms)\nThroughput - The number of operations that a system can handle properly per time unit. For instance the throughput of a sec measured in requests per second (RPS or QPS).\nAvailability # Availability - The odds of a particular server or service being up and running at any point in time, usually measured in percentages. A server that has 99% availability will be operational 99% of the time (this would be described as having two nines of availability).\nHigh Availability - Used to describe systems that have particularly high levels of availability, typically 5 nines or more; sometimes abbreviated \u0026ldquo;HA\u0026rdquo;,\nNines - Typically refers to percentages of uptime. For example, 5 nines of availability means an uptime of 99.999% of the time. Below are the downtimes expected per year depending on those 9s:\n99% (two 9s): 87.7 hours\n99.9% (three 9s): 8.8 hours\n99.99%: 52.6 minutes\n99.999%: 5.3 minutes\nCaching # Cache - A piece of hardware or software that stores data, typically meant to retrieve that data faster than otherwise. Caches are often used to store responses to network requests as well as results of computationally tong operations. Note that data in a cache can become stale if the main source of truth for that data (i.e., the main database behind the cache) gets updated and the cache doesn\u0026rsquo;t.\nCache Hit When requested data is found in a cache.\nCache Miss When requested data could have been found in a cache but isn\u0026rsquo;t. This is typically used to refer to a negative consequence of a system failure or of a poor design choice. For example: If a server goes down, our load balancer will have to forward requests to a new server, which will result in cache misses.\nCache Eviction Policy The policy by which values get evicted or removed from a cache. Popular cache eviction policies include LRU (least-recently used), FIFO (first in first out), and LFU (least-frequently used).\nContent Delivery Network A CDN is a third-party service that acts like a cache for your servers. Sometimes, web applications can be slow for users in a particular region if your servers are located only in another region. A CDN has servers all around the world, meaning that the latency to a CDN\u0026rsquo;s servers will almost always be far better than the latency to your servers. A CDN\u0026rsquo;s servers are often referred to as PoPs (Points of Presence). Two of the most popular CDNs are Cloudflare and Google Claud CDN.\nProxies # Forward Proxy A server that sits between a client and servers and acts on behalf of the client, typically used to mask the client\u0026rsquo;s identity (IP address), Note that forward proxies are often referred to as just proxies.\nReverse Proxy A server that sits between clients and servers and acts on behalf of the servers, typically used for logging, load balancing, or caching. | nginx @ Pronounced \u0026ldquo;engine X°\u0026ndash;not \u0026ldquo;N jinx”, Nginx is a very popular webserver that\u0026rsquo;s often used as a reverse proxy and load balancer. Learn more: https://www.nginx.com/\nLoad Balancer A type of reverse proxy that distributes traffic across servers, Load balancers can be found in many parts of a system, from the DNS layer all the way to the database layer. Learn more: https://www.nginx.com/\nServer-Selection Strategy How a load balancer chooses servers when distributing traffic amongst multiple servers. Commonly used strategies include roundrobin, random selection, performance-based selection (choosing the server with the best performance metrics, like the fastest response time or the least amount of traffic), and IP-based routing.\nDatabases # Relational Database A type of structured database in which data is stored following a tabular format; often supports powerful querying using SQL. Learn more: https://www.postgresql.org/\nNon-Relational Database In contrast with relational database (SQL databases), a type of database that is free of Imposed, tabular-like structure. Non-relational databases are often referred to as NoSQL databases,\nACID Transaction\nAtomicity: The operations that constitute the transaction will either all succeed or ail fail. There is no in-between state. Consistency: The transaction cannot bring the database to an invalid state. After the transaction is committed or rolled back, the rules for each record will still apply, and all future transactions will see the effect of the transaction. Also named Strong Consistency. isolation: The execution of multiple transactions concurrently will have the same effect as if they had been executed sequentially. Durability: Any committed transaction is written to non-volatile storage. It will not be undone by a crash, power loss, or network partition. Strong Consistency Strong Consistency usually refers to the consistency of ACID transactions, as opposed to Eventual Consistency.\nEventual Consistency A consistency mode which is unlike Strong Consistency. In this model, reads might return a view of the system that is stale. An eventually consistent datastore will give guarantees that the state of the database will eventuall reflect writes within a time period.\nNo-SQL Databases # Key-Value Store A Key-Value Store is a flexible NoSQL database that\u0026rsquo;s often used for caching and dynamic configuration. Popular aptions include DynamoDB, Etcd, Redis, and ZooKeeper,\netcd Etcd is a strongly consistent and highly available key-value store that\u0026rsquo;s often used to Implement leader election in a system, Learn more: https://etcd.io/\nRedis An in-memory key-value store. Does offer some persistent storage options but is typically used as a really fast, best-effort caching solution, Redis is also often used to implement rate limiting. Learn more: https://redis.io/\nZookeeper Zookeeper Is a strongly consistent, highly available key-value store. It\u0026rsquo;s often used to store important configuration of to perform leader election. Learn more: https://zookeeper.apache.org/\nDynamoDB An key-value store by AWS, this provides eventual consistency.\nBlob Storage Widely used kind of storage, in small and large scale systems. They don’t really count as databases per se, partially because they only allow the user to store and retrieve data based on the name of the blob. This is sort of like a key-value store but usually blob stores have different guarantees. They might be slower than KV stores but values can be megabytes large (or sometimes gigabytes large). Usually people use this to store things like large binaries, database snapshots, or images and other static assets that a website might have. Blob storage is rather complicated to have on premise, and only giant companies like Google and Amazon have infrastructure that supports it. So usually in the context of System Design interviews you can assume that you will be able to use GCS or S3. These are blob storage services hosted by Google and Amazon respectively, that cost money depending on how much storage you use and how often you store and retrieve blobs from that storage.\nGoogle Cloud Storage(GCS) - is a blob storage service provided by Google. Learn more: https://cloud.google.com/storage\nS3 - ls a blob storage service provided by Amazon through Amazon Web Services (AWS). Learn more: https://aws.amazon.com/s3/\nTime Series Database A TSDB is a special kind of database optimized for storing and analyzing time-indexed data: data points that specifically occur at a given moment in time. Examples of TSDBs are InfluxDB, Prometheus, and Graphite.\ninfluxDB - popular open-source time series database, Learn more; https://www.influxdata.com/\nPrometheus - A popular open source time series database, typically used for monitoring purposes. https://prometheus.io\nGraph Database A type of database that stores data following the graph data model. Data entries in a graph database can have explicitly defined relationships, much like nodes in a graph can have edges. Graph databases take advantage of their underlying graph structure to perform complex queries on deeply connected data very fast. Graph databases are thus often preferred to relational databases when dealing with systems where data points naturally form a graph and have multiple tevels of relationships—for example, social networks.\nNeo4j - a popular grpah DB, consists of nodes, relationships, propreties and labels. https://neo4j.com Spatial Database A type of database optimized for storing and querying spatial data like locations on a map. Spatial databases rely on spatial indexes fike quadtrees to quickly perform spatial queries like finding all locations in the vicinity of a region.\nReplication \u0026amp; Shrading # Replication - The act of duplicating the data from one database server to others. This is sometimes used to increase the redundancy of your system and tolerate regional failures for instance. Other times you can use replication to move data closer to your clients, thus decreasing latency of accessing specific data.\nSharding - Sometimes called data partitioning, sharding is the act of splitting a database into two or more pieces called shards and is typically done to increase the throughput of your database. Popular sharding strategies include:\nSharding based on a client\u0026rsquo;s region. Sharding based on the type of data being stored (e.g: user data gets stored in one shard, payments data gets stored In another shard) Sharding based on the hash of a column (only for structured data) Peer-To-Peer Networks # Peer-To-Peer Network A collection of machines referred to as peers that divide a workload between themselves to presumably complete the workload faster than would otherwise be possible. Peer-to-peer networks are often used in file-distribution systems.\nGossip Protocol When a set of machines talk to each other in a uncoordinated manner in a cluster to spread information through a system without requiring a central source of data. - ad °\nRate Limiting # Rate Limiting The act of limiting the number of requests sent to or from a system. Rate limiting is most often used to limit the number of incoming requests in order to prevent DoS attacks and can be enforced at the IP-address level, at the user-account level, or at the region level, for example. Rate limiting can also be implemented in tiers; for instance, a type of network request could be limited to 1 per second, 5 per 10 seconds, and 10 per minute.\nDoS Attack Short for “denial-of-service attack”, a DoS attack is an attack in which a malicious user tries to bring down or damage a system in order to render it unavailable to users. Much of the time, it consists of flooding it with traffic. Some DoS attacks are easily preventable with rate limiting, while others can be far trickier to defend against.\nDDoS Attack\nShort for “distributed denial-of-service attack\u0026rdquo;, a DDoS attack is a DoS attack in which the traffic flooding the target system comes from many different sources (like thousands of machines), making It much harder to defend against.\nRedis An in-memory key-value store. Does offer some persistent storage options but Is typically used as a really fast, best-effort caching solution, Redis is also often used to implement rate limiting. Learn more: https://redis.io/\nPublish/Subscribe Pattern # Publish/Subscribe Pattern Often shortened as Pub/Sub, the Publish/Subscribe pattern Is a popular messaging model that consists of publishers and subscribers. Publishers publish messages to special topics (sometimes called channels) without caring about or even knowing who will read those messages, and subscribers subscribe to topics and read messages coming through those topics. Pub/Sub systems often come with very powerful guarantees like at-least-once delivery, persistent storage, ordering of messages, and replayability of messages.\nApache Kafka A distributed messaging system created by Linkedin. Very useful when using the streaming paradigm as opposed to polling. Learn more: https://kafka.apache.org/\nCloud pub/sub A highly-scalable Pub/Sub messaging service created by Google, Guarantees at-least-once delivery of messages and supports “rewinding” in order to reprocess messages. Learn more: https://cloud.google.com/pubsub/\nMapReduce # MapReduce A popular framework for processing very large datasets in a distributed setting efficiently, quickly, and in a fault-tolerant manner. A MapReduce job is comprised of 3 main steps:\nthe Map step, which runs a map function on the various chunks of the dataset and transforms these chunks into intermediate key-value pairs. the Shuffle step, which reorganizes the intermediate key-value pairs such that pairs of the same key are routed to the same machine in the final step. the Reduce step, which runs a reduce function on the newly shuffled key-value pairs and transforms them into more meaningful data. The canonical example of a MapReduce use case is counting the number of occurrences of words in a large text file. When dealing with a MapReduce library, engineers and/or systems administrators only need to worry about the map and reduce functions, as well as their inputs and outputs. All other concerns, including the parallelization of tasks and the fault-tolerance of the MapReduce job, are abstracted away and taken care of by the MapReduce implementation. Distributed File System A Distributed Ale System is an abstraction over a (usually large) cluster of machines that allows them to act like one large file system. The two most popular implementations of a DFS are the Google File System (GFS) and the Hadoop Distributed File System (HDFS). Typically, DFSs take care of the classic availability and replication guarantees that can be tricky to obtain in a distributed-system setting, The overarching idea is that files are split into chunks of a certain size (4MB or 64MB, for instance), and those chunks are sharded across a large cluster of machines. A central control plane is in charge of deciding where each chunk resides, routing reads to the right nodes, and handling communication between machines, Olfferent DFS implementations have slightly different APIs and semantics, but they achieve the same common goal: extremely largescale persistent storage,\nHadoop A popular, open-source framework that supports MapReduce jobs and many other kinds of data-processing pipelines. Its central component is HDFS (Hadoop Distributed File System), on top of which other technologies have been developed. Learn more: https://hadoop.apache.org/\nSecurity And HTTPS # Symmetric Encryption A type of encryption that relies on only a single key to both encrypt and decrypt data. The key must be known to all parties involved in the communication and must therefore typically be shared between the parties at one point or another. Symmetric-key algorithms tend to be faster than their asymmetric counterparts. The most widely used symmetric-key algorithms are part of the Advanced Encryption Standard (AES).\nAsymmetric Encryption Also known as public-key encryption, asymmetric encryption relies on two keys—a public key and a private key—to encrypt and decrypt data. The keys are generated using cryptographic algorithms and are mathematically connected such that data encrypted with the public key can only be decrypted with the private key. While the private key must be kept secure to maintain the fidelity of this encryption paradigm, the public key can be openly shared. Asymmetric-key algorithms tend to be slower than their symmetric counterparts.\nAES Stands for Advanced Encryption Standard. AES is a widely used encryption standard that has three symmetric-key algorithms (AES-128, AES-192, and AES-256). Of note, AES is considered to be the \u0026ldquo;gold standard\u0026rdquo; in encryption and is even used by the U.S. National Security Agency to encrypt top secret information. .\nHTTPS The HyperText Transfer Protocol Secure is an extension of HTTP that\u0026rsquo;s used for secure communication online. It requires servers to have trusted certificates (usually SSL certificates) and uses the Transport Layer Security (TLS), a security protocol built on top of TCP, to encrypt data communicated between a client and a server. { TLs The Transport Layer Security is a security protocol over which HTTP runs in order to achieve secure communication online. \u0026ldquo;HTTP over TLS\u0026rdquo; Is also known as HTTPS.\nSSL Certificate A digital certificate granted to a server by a certificate authority. Contains the server\u0026rsquo;s public key, to be used as part of the TLS handshake process in an HTTPS connection. An SSL certificate effectively confirms that a public key belongs to the server claiming it belongs to them. SSL certificates are a crucial defense against man-in-the-middie attacks,\nCertificate Authority A trusted entity that signs digital certificates—namely, SSL certificates that are relied on in HTTPS connections.\nTLS Handshake The process through which a client and a server communicating over HTTPS exchange encryption-related information and establish a secure communication. The typical steps in a TLS handshake are roughly as follows:\nThe client sends a client hello string of random bytes—to the server. The server responds with a server hello another string of random bytes—as well as its SSL certificate, which contains its publle key. The client verifies that the certificate was issued by a certificate authority and sends a premaster secret—yet another string of random bytes, this time encrypted with the server\u0026rsquo;s public key—to the server. The client and the server use the client hello, the server helio, and the premaster secret to then generate the same symmetric encryption session keys, to be used to encrypt and decrypt all data communicated during the remainder of the connection. "}]