[{"id":0,"href":"/tech-book/docs/","title":"Example Site","section":"Introduction","content":" Introduction # Ferre hinnitibus erat accipitrem dixi Troiae tollens # Lorem markdownum, a quoque nutu est quodcumque mandasset veluti. Passim inportuna totidemque nympha fert; repetens pendent, poenarum guttura sed vacet non, mortali undas. Omnis pharetramque gramen portentificisque membris servatum novabis fallit de nubibus atque silvas mihi. Dixit repetitaque Quid; verrit longa; sententia mandat quascumque nescio solebat litore; noctes. Hostem haerentem circuit plenaque tamen.\nPedum ne indigenae finire invergens carpebat Velit posses summoque De fumos illa foret Est simul fameque tauri qua ad # Locum nullus nisi vomentes. Ab Persea sermone vela, miratur aratro; eandem Argolicas gener.\nMe sol # Nec dis certa fuit socer, Nonacria dies manet tacitaque sibi? Sucis est iactata Castrumque iudex, et iactato quoque terraeque es tandem et maternos vittis. Lumina litus bene poenamque animos callem ne tuas in leones illam dea cadunt genus, et pleno nunc in quod. Anumque crescentesque sanguinis progenies nuribus rustica tinguet. Pater omnes liquido creditis noctem.\nif (mirrored(icmp_dvd_pim, 3, smbMirroredHard) != lion(clickImportQueue, viralItunesBalancing, bankruptcy_file_pptp)) { file += ip_cybercrime_suffix; } if (runtimeSmartRom == netMarketingWord) { virusBalancingWin *= scriptPromptBespoke + raster(post_drive, windowsSli); cd = address_hertz_trojan; soap_ccd.pcbServerGigahertz(asp_hardware_isa, offlinePeopleware, nui); } else { megabyte.api = modem_flowchart - web + syntaxHalftoneAddress; } if (3 \u0026lt; mebibyteNetworkAnimated) { pharming_regular_error *= jsp_ribbon + algorithm * recycleMediaKindle( dvrSyntax, cdma); adf_sla *= hoverCropDrive; templateNtfs = -1 - vertical; } else { expressionCompressionVariable.bootMulti = white_eup_javascript( table_suffix); guidPpiPram.tracerouteLinux += rtfTerabyteQuicktime(1, managementRosetta(webcamActivex), 740874); } var virusTweetSsl = nullGigo; Trepident sitimque # Sentiet et ferali errorem fessam, coercet superbus, Ascaniumque in pennis mediis; dolor? Vidit imi Aeacon perfida propositos adde, tua Somni Fluctibus errante lustrat non.\nTamen inde, vos videt e flammis Scythica parantem rupisque pectora umbras. Haec ficta canistris repercusso simul ego aris Dixit! Esse Fama trepidare hunc crescendo vigor ululasse vertice exspatiantur celer tepidique petita aversata oculis iussa est me ferro.\n"},{"id":1,"href":"/tech-book/docs/algorithms/breadth-first-search/","title":"Breadth First Search","section":"Algorithms","content":" Breadth First Search # Intro # Hopefully, by this time, you\u0026rsquo;ve drunk enough DFS kool-aid to understand its immense power and seen enough visualization to create a call stack in your mind. Now let me introduce the companion spell Breadth First Search (BFS). The names are self-explanatory. While depth first search reaches for depth (child nodes) first before breadth (nodes in the same level/depth), breadth first search visits all nodes in a level before starting to visit the next level. While DFS uses recursion/stack to keep track of progress, BFS uses a queue (First In First Out). When we dequeue a node, we enqueue its children.\nBFS template # from collections import deque def bfs_by_queue(root): queue = deque([root]) # at least one element in the queue to kick start bfs while len(queue) \u0026gt; 0: # as long as there is an element in the queue node = queue.popleft() # dequeue for child in node.children: # enqueue children if OK(child): # early return if problem condition met return FOUND(child) queue.append(child) return NOT_FOUND "},{"id":2,"href":"/tech-book/docs/programming-tips/c++/","title":"C++ Tips","section":"Programming Tips","content":" C++ Tips # Templates # A template is a simple yet very powerful tool in C++. The simple idea is to pass the data type as a parameter so that we don’t need to write the same code for different data types. For example, a software company may need to sort() for different data types. Rather than writing and maintaining multiple codes, we can write one sort() and pass the datatype as a parameter.\nC++ adds two new keywords to support templates: ‘template’ and ‘typename’. The second keyword can always be replaced by the keyword ‘class’.\nFunction Templates # We write a generic function that can be used for different data types.\n#include \u0026lt;iostream\u0026gt; using namespace std; template \u0026lt;typename T\u0026gt; T myFunc (T x, T y) { return (x\u0026gt;y)? x:y; } int main () { cout \u0026lt;\u0026lt; myFunc\u0026lt;int\u0026gt;(3,7) \u0026lt;\u0026lt; endl; std::cout \u0026lt;\u0026lt; myFunc\u0026lt;char\u0026gt;(\u0026#39;g\u0026#39;, \u0026#39;e\u0026#39;) \u0026lt;\u0026lt; std::endl; return 0; } Class Templates # Class templates like function templates, class templates are useful when a class defines something that is independent of the data type. Can be useful for classes like LinkedList, BinaryTree, Stack, Queue, Array, etc.\n// C++ Program to implement // template Array class #include \u0026lt;iostream\u0026gt; using namespace std; template \u0026lt;typename T\u0026gt; class Array { private: T* ptr; int size; public: Array(T arr[], int s); void print(); }; template \u0026lt;typename T\u0026gt; Array\u0026lt;T\u0026gt;::Array(T arr[], int s) { ptr = new T[s]; size = s; for (int i = 0; i \u0026lt; size; i++) ptr[i] = arr[i]; } template \u0026lt;typename T\u0026gt; void Array\u0026lt;T\u0026gt;::print() { for (int i = 0; i \u0026lt; size; i++) cout \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; *(ptr + i); cout \u0026lt;\u0026lt; endl; } int main() { int arr[5] = { 1, 2, 3, 4, 5 }; Array\u0026lt;int\u0026gt; a(arr, 5); a.print(); return 0; } Reference: Templates in C++ with Examples\nArray # An array in C++ holds a fixed number of elements of a single type in contiguous memory locations.\nstd::string clothes[3]; // declare a string array of size 3 named clothes int numbers[5]; // declare an int array of size 5 named numbers By default, the values in C++ arrays are undetermined at the time of declaration (meaning that the values can be random). An array can be initialized to specific values by enclosing the values in curly braces. The number of values in {} should not exceed the size of the array. If the number of values inside the braces is less than the size of the array, the rest of the array will be set to the default value for their type, such as 0 for int and false for bool. If an array is initialized to empty curly braces, all elements in the array is set to their default value.\nint a[3] = { 3, 4, 5 }; // int array a = [3, 4, 5] int b[5] = { 1 }; // int array b = [1, 0, 0, 0, 0] int c[2] = { }; // int array c = [0, 0] An element in an array is accessed by its index in O(1) time with square brackets.\nstd::string clothes[5]; // declare an array of size 5 clothes[0] = \u0026#34;shirt\u0026#34;; // initialize first element clothes[1] = \u0026#34;dress\u0026#34;; // initialize second element std::cout \u0026lt;\u0026lt; clothes[0]; // print out \u0026#34;shirt\u0026#34; C++ arrays do not have a length attribute that lets you access its size easily. The common way to get the size of an array is by:\nint arr[5] = { 7, 3, 4, 6, 8 } std::cout \u0026lt;\u0026lt; sizeof(arr)/sizeof(arr[0]) \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; // print out 5 Since C++17, we can also retrieve the size of an array with std::size().\nint arr[] = { 6, 7, 8 } std::cout \u0026lt;\u0026lt; std::size(arr) \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; // print out 3 Vector # When you need to create a dynamically resizable array, C++ provides a useful sequence container vector. Like arrays, it provides efficient constant time element access.\nvector\u0026lt;data_type\u0026gt; vectorName; Commonly used Vector Functions\npush_back() – It is used to insert the elements at the end of the vector. pop_back() – It is used to pop or remove elements from the end of the vector. clear() – It is used to remove all the elements of the vector. empty() – It is used to check if the vector is empty. at(i) – It is used to access the element at the specified index ‘i’. front() – It is used to access the first element of the vector. back() – It is used to access the last element of the vector. erase() – It is used to remove an element at a specified position. size() - returns the size of the array resize() - resize the array with the new size vector_name[i] gets or sets item stored at index i, O(1) emplace_back(item) adds item to the end of the vector, amortized O(1) size() returns the size of the vector, O(1) The emplace_back() operation, which appends a new element to the end of the vector, runs in amortized constant time. A vector will resize itself when needed. A typical resizing implementation is that when the array is full, the array doubles in size, and the old content is copied over to the newer, larger array. The doubling takes O(n) time, but it happens rarely that the overall insertion still takes O(1) time.\nIn general, to iterate through an array, a for loop is easier to reason than while since there\u0026rsquo;s no condition to manage that could skip elements. In C++, there are two types of for loops: the simple for loop and the for-each loop. Use for-each loop if you don\u0026rsquo;t need to access the index since it avoids the possible error of messsing up the index.\nstd::vector\u0026lt;int\u0026gt; numbers{20, 6, 13, 5}; // simple for loop goes through indices so we fetch elements using indices for (int i = 0; i \u0026lt; numbers.size(); i++) { int number = numbers[i]; std::cout \u0026lt;\u0026lt; number \u0026lt;\u0026lt; std::endl; } // for-each loop directly fetches elements for (int number : numbers) { std::cout \u0026lt;\u0026lt; number \u0026lt;\u0026lt; std::endl; } Linked List # C++ doesn\u0026rsquo;t have a built-in singly linked list. Normally at an interview, you\u0026rsquo;ll be given a definition like this:\nstruct LinkedListNode { int val; LinkedListNode* next; } append to end is O(1) finding an element is O(N) Besides problems that specifically ask for linked lists, you don\u0026rsquo;t normally define and use linked list. If you need a list with O(1) append you can use a vector, and if you want O(1) for both prepend and append you can use deque.\nStack # C++ provides a container adaptor std::stack that works on the basis of LIFO (last-in-first-out).\npush: push(item) adds item to end of stack, O(1) pop: pop() removes item at the end of stack, O(1) size: size(), O(1) top: top() returns but doesn\u0026rsquo;t remove item at the end of stack, O(1) Stack is arguably the most magical data structure in computer science. In fact, with two stacks and a finite-state machine you can build a Turing Machine that can solve any problem a computer can solve.\nRecursion and function calls are implemented with stack behind the scene. We\u0026rsquo;ll discuss this in the recursion review module.\nQueue # We normally use std::deque when we need a queue.\nenqueue: push_back(item) inserts item at the end of the queue, O(1) dequeue: pop_front() removes and return item from the head of the queue, O(1) size: size(), O(1) peek: front() returns but don\u0026rsquo;t remove item at the head of the queue, O(1) In coding interviews, we see queues most often in breadth-first search. We\u0026rsquo;ll also cover monotonic deque where elements are sorted inside the deque that is useful in solving some advanced coding problems.\nHash Table # std::unordered_map in C++ implements hash table.\nat(key) gets item mapped to key if present, O(1) map[key] gets item mapped to key if present (returns 0 if not present), and inserts the key if not, O(N) map[key] = item sets item mapped to key, O(1) count(key) finds out if key is present or not, O(1) erase(key) removes item mapped to key if present, O(1) It\u0026rsquo;s worth mentioning that these are average case time complexity. A hash table\u0026rsquo;s worst time complexity is actually O(N) due to hash collision and other things. For the vast majority of the cases and certainly most coding interviews, the assumption of constant time lookup/insert/delete is valid.\nUse a hash table if you want to create a mapping from A to B. Many starter interview problems can be solved with hash tables.\nHash Set # std::unordered_set in C++ is useful in answering existence queries in constant time.\ncount(item) checks if item is in a set, O(1) insert(item) adds item to a set if it\u0026rsquo;s not already present, O(1) erase(item) removes item from a set if it\u0026rsquo;s present, O(1) size() gets the size of the set, O(1) Hash set is useful when you only need to know existence of a key. Example use cases include DFS and BFS on graphs.\nTree # Normally at an interview, you\u0026rsquo;d be given the following implementation for a binary tree:\nstruct Node { int val; Node* left; Node* right; Node(T val, Node* left = nullptr, Node* right = nullptr) : val{val}, left{left}, right{right} {} }; For n-nary trees:\n#include \u0026lt;vector\u0026gt; struct Node { int val; std::vector\u0026lt;Node*\u0026gt; children; Node(int val, std::vector\u0026lt;Node*\u0026gt; children = {}) : val{val}, children{children} {} }; Infinity # Infinity is useful when you want to initialize a variable that is greater or smaller than any value that your algorithm may want to compare with. std::numeric_limits\u0026lt;T\u0026gt; provides the maximum and minimum values of fundamental arithmatic types in C++.\n#include \u0026lt;limits\u0026gt; int max = std::numeric_limits\u0026lt;int\u0026gt;::max(); // 2147483648 int min = std::numeric_limits\u0026lt;int\u0026gt;::min(); // -2147483648 "},{"id":3,"href":"/tech-book/docs/algorithms/depth-first-search/","title":"Depth First Search","section":"Algorithms","content":" Depth First Search # Intro # The pre-order traversal of a tree is DFS.\nNode\u0026lt;int\u0026gt;* dfs(Node\u0026lt;int\u0026gt;* root, int target) { if (root == nullptr) return nullptr; if (root-\u0026gt;val == target) return root; // return non-null return value from the recursive calls Node\u0026lt;int\u0026gt;* left = dfs(root-\u0026gt;left, target); if (left != nullptr) return left; // at this point, we know left is null, and right could be null or non-null // we return right child\u0026#39;s recursive call result directly because // - if it\u0026#39;s non-null we should return it // - if it\u0026#39;s null, then both left and right are null, we want to return null return dfs(root-\u0026gt;right, target); } Max depth of a binary tree # Max depth of a binary tree is the longest root-to-leaf path. Given a binary tree, find its max depth. Here, we define the length of the path to be the number of edges on that path, not the number of nodes.\nint dfs(Node\u0026lt;int\u0026gt;* root) { // Null node adds no depth if (root == nullptr) return 0; // num nodes in longest path of current subtree = max num nodes of its two subtrees + 1 current node return std::max(dfs(root-\u0026gt;left), dfs(root-\u0026gt;right)) + 1; } int tree_max_depth(Node\u0026lt;int\u0026gt;* root) { return root? dfs(root) - 1 : 0; } Visible Tree Node | Number of Visible Nodes # In a binary tree, a node is labeled as \u0026ldquo;visible\u0026rdquo; if, on the path from the root to that node, there isn\u0026rsquo;t any node with a value higher than this node\u0026rsquo;s value.\nThe root is always \u0026ldquo;visible\u0026rdquo; since there are no other nodes between the root and itself. Given a binary tree, count the number of \u0026ldquo;visible\u0026rdquo; nodes.\nint dfs(Node\u0026lt;int\u0026gt;* root, int max_sofar) { if (!root) return 0; int total = 0; if (root-\u0026gt;val \u0026gt;= max_sofar) total++; total += dfs(root-\u0026gt;left, std::max(max_sofar, root-\u0026gt;val)); total += dfs(root-\u0026gt;right, std::max(max_sofar, root-\u0026gt;val)); return total; } int visible_tree_node(Node\u0026lt;int\u0026gt;* root) { // start max_sofar with smallest number possible so any value root has is greater than it return dfs(root, std::numeric_limits\u0026lt;int\u0026gt;::min()); } "},{"id":4,"href":"/tech-book/docs/algorithms/easy/","title":"Easy Complexity","section":"Algorithms","content":" Easy Complexity # "},{"id":5,"href":"/tech-book/docs/algorithms/priority-queue-and-heap/","title":"Priority Queue and Heap","section":"Algorithms","content":" Priority Queue and Heap # Priority Queue is an Abstract Data Type, and Heap is the concrete data structure we use to implement a priority queue.\nPriority Queue # A priority queue is a data structure that consists of a collection of items and supports the following operations:\ninsert: insert an item with a key. delete_min/delete_max: remove the item with the smallest/largest key and return it. Note that\nwe only allow getting and deleting the element with the min/max key and NOT any arbitrary key. Implement Priority Queue using an array # To do this, we could try using\nan unsorted array, insert would be O(1) as we just have to put it at the end, but finding and deleting min value would be O(N) since we have to loop through the entire array to find it a sorted array, finding min value would be easy O(1), but it would be O(N) to insert since we have to loop through to find the correct position of the value and move elements after the position to make space and insert into the space There must be a better way! \u0026ldquo;Patience you must have, my young padawan.\u0026rdquo; Enter Heap.\nHeap # Heaps are special tree based data structures. Usually when we say heap, we refer to the binary heap that uses the binary tree structure. However, the tree isn\u0026rsquo;t necessarily always binary, in particular, a k-ary heap (A.K.A. k-heap) is a tree where the nodes have k children. As long as the nodes follow the 2 heap properties, it is a valid heap.\nMax Heap and Min Heap # There are two kinds of heaps - Min Heap and Max Heap. A Min Heap is a tree that has two properties:\nalmost complete, i.e. every level is filled except possibly the last(deepest) level. The filled items in the last level are left-justified. for any node, its key (priority) is greater than its parent\u0026rsquo;s key (Min Heap). A Max Heap has the same property #1 and opposite property #2, i.e. for any node, its key is less than its parent\u0026rsquo;s key.\nOperations # insert # To insert a key into a heap,\nplace the new key at the first free leaf if property #2 is violated, perform a bubble-up def bubble_up(node): while node.parent exist and node.parent.key \u0026gt; node.key: swap node and node.parent node = node.parent As the name of the algorithm suggests, it \u0026ldquo;bubbles up\u0026rdquo; the new node by swapping it with its parent until the order is correct\nSince the height of a heap is O(log(N)), the complexity of bubble-up is O(log(N)).\ndelete_min # What this operation does is:\ndelete a node with min key and return it reorganize the heap so the two properties still hold To do that, we:\nremove and return the root since the node with the minimum key is always at the root replace the root with the last node (the rightmost node at the bottom) of the heap if property #2 is violated, perform a bubble-down def bubble_down(node): while node is not a leaf: smallest_child = child of node with smallest key if smallest_child \u0026lt; node: swap node and smallest_child node = smallest_child else: break Implementing Heap # Being a complete tree makes an array a natural choice to implement a heap since it can be stored compactly and no space is wasted. Pointers are not needed. The parent and children of each node can be calculated with index arithmetic\nFor node i, its children are stored at 2i+1 and 2i+2, and its parent is at floor((i-1)/2). So instead of node.left we\u0026rsquo;d do 2*i+1. (Note that if we are implementing a k-ary heap, then the childrens are at ki+1 to ki+k, and its parent is at floor((i-1)/k).)\n"},{"id":6,"href":"/tech-book/docs/algorithms/two-pointers/","title":"Two Pointers \u0026 Sliding Window","section":"Algorithms","content":" Two Pointers # Valid Palindrome # Determine whether a string is a palindrome, ignoring non-alphanumeric characters and case. Examples:\nInput: Do geese see God? Output: True\nInput: Was it a car or a cat I saw? Output: True\nInput: A brown fox jumping over Output: False\n#include \u0026lt;cctype\u0026gt; // isalnum, tolower #include \u0026lt;iostream\u0026gt; // boolalpha, cin, cout #include \u0026lt;string\u0026gt; // getline bool is_palindrome(std::string s) { int l = 0, r = s.size() - 1; while (l \u0026lt; r) { // Note 1, 2 while (l \u0026lt; r \u0026amp;\u0026amp; !std::isalnum(s[l])) { l++; } while (l \u0026lt; r \u0026amp;\u0026amp; !std::isalnum(s[r])) { r--; } // compare characters ignoring case if (std::tolower(s[l]) != std::tolower(s[r])) return false; l++; r--; } return true; } int main() { std::string s; std::getline(std::cin, s); bool res = is_palindrome(s); std::cout \u0026lt;\u0026lt; std::boolalpha \u0026lt;\u0026lt; res \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; } Remove Duplicates # Given a sorted list of numbers, remove duplicates and return the new length. You must do this in-place and without using extra memory.\nInput: [0, 0, 1, 1, 1, 2, 2].\nOutput: 3.\nYour function should modify the list in place so the first 3 elements becomes 0, 1, 2. Return 3 because the new length is 3.\n#include \u0026lt;algorithm\u0026gt; // copy #include \u0026lt;iostream\u0026gt; // boolalpha, cin, cout #include \u0026lt;iterator\u0026gt; // back_inserter, istream_iterator, ostream_iterator, prev #include \u0026lt;sstream\u0026gt; // istringstream #include \u0026lt;string\u0026gt; // getline, string #include \u0026lt;vector\u0026gt; // vector int remove_duplicates(std::vector\u0026lt;int\u0026gt;\u0026amp; arr) { int slow = 0; for (int fast = 0; fast \u0026lt; arr.size(); fast++) { if (arr.at(fast) != arr.at(slow)) { slow++; arr.at(slow) = arr.at(fast); } } return slow + 1; } template\u0026lt;typename T\u0026gt; std::vector\u0026lt;T\u0026gt; get_words() { std::string line; std::getline(std::cin, line); std::istringstream ss{line}; ss \u0026gt;\u0026gt; std::boolalpha; std::vector\u0026lt;T\u0026gt; v; std::copy(std::istream_iterator\u0026lt;T\u0026gt;{ss}, std::istream_iterator\u0026lt;T\u0026gt;{}, std::back_inserter(v)); return v; } template\u0026lt;typename T\u0026gt; void put_words(const std::vector\u0026lt;T\u0026gt;\u0026amp; v) { if (!v.empty()) { std::copy(v.begin(), std::prev(v.end()), std::ostream_iterator\u0026lt;T\u0026gt;{std::cout, \u0026#34; \u0026#34;}); std::cout \u0026lt;\u0026lt; v.back(); } std::cout \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; } int main() { std::vector\u0026lt;int\u0026gt; arr = get_words\u0026lt;int\u0026gt;(); int res = remove_duplicates(arr); arr.resize(res); put_words(arr); } Sliding Window # Sliding window problems is a variant of the same direction two pointers problems. The function performs on the entire interval between the two pointers instead of only at the two positions. Usually, we keep track of the overall result of the window, and when we \u0026ldquo;slide\u0026rdquo; the window (insert/remove an item), we simply manipulate the result to accomodate the changes to the window. Time complexity wise, this is much more efficient as we do not recalculate the overlapping intervals between two windows over and over again. We try to reduce a nested loop into two passes on the input (one pass with each pointer).\nFixed Size Sliding Window # Given an array (list) nums consisted of only non-negative integers, find the largest sum among all subarrays of length k in nums. For example, if the input is nums = [1, 2, 3, 7, 4, 1], k = 3, then the output would be 14 as the largest length 3 subarray sum is given by [3, 7, 4] which sums to 14.\n#include \u0026lt;algorithm\u0026gt; // copy #include \u0026lt;iostream\u0026gt; // boolalpha, cin, cout, streamsize #include \u0026lt;iterator\u0026gt; // back_inserter, istream_iterator #include \u0026lt;limits\u0026gt; // numeric_limits #include \u0026lt;sstream\u0026gt; // istringstream #include \u0026lt;string\u0026gt; // getline, string #include \u0026lt;vector\u0026gt; // vector int subarray_sum_fixed(std::vector\u0026lt;int\u0026gt; nums, int k) { int window_sum = 0; for (int i = 0; i \u0026lt; k; ++i) { window_sum = window_sum + nums[i]; } int largest = window_sum; for (int right = k; right \u0026lt; nums.size(); ++right) { int left = right - k; window_sum = window_sum - nums[left]; window_sum = window_sum + nums[right]; largest = std::max(largest, window_sum); } return largest; } template\u0026lt;typename T\u0026gt; std::vector\u0026lt;T\u0026gt; get_words() { std::string line; std::getline(std::cin, line); std::istringstream ss{line}; ss \u0026gt;\u0026gt; std::boolalpha; std::vector\u0026lt;T\u0026gt; v; std::copy(std::istream_iterator\u0026lt;T\u0026gt;{ss}, std::istream_iterator\u0026lt;T\u0026gt;{}, std::back_inserter(v)); return v; } void ignore_line() { std::cin.ignore(std::numeric_limits\u0026lt;std::streamsize\u0026gt;::max(), \u0026#39;\\n\u0026#39;); } int main() { std::vector\u0026lt;int\u0026gt; nums = get_words\u0026lt;int\u0026gt;(); int k; std::cin \u0026gt;\u0026gt; k; ignore_line(); int res = subarray_sum_fixed(nums, k); std::cout \u0026lt;\u0026lt; res \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; } Flexible Size Sliding Window - Longest # Recall finding the largest size k subarray sum of an integer array in Largest Subarray Sum. What if we dont need the largest sum among all subarrays of fixed size k, but instead, we want to find the length of the longest subarray with sum smaller than or equal to a target?\nGiven input nums = [1, 6, 3, 1, 2, 4, 5] and target = 10, then the longest subarray that does not exceed 10 is [3, 1, 2, 4], so the output is 4 (length of [3, 1, 2, 4]).\n#include \u0026lt;algorithm\u0026gt; // copy #include \u0026lt;iostream\u0026gt; // boolalpha, cin, cout, streamsize #include \u0026lt;iterator\u0026gt; // back_inserter, istream_iterator #include \u0026lt;limits\u0026gt; // numeric_limits #include \u0026lt;sstream\u0026gt; // istringstream #include \u0026lt;string\u0026gt; // getline, string #include \u0026lt;vector\u0026gt; // vector int subarray_sum_longest(std::vector\u0026lt;int\u0026gt; nums, int target) { int windowSum = 0, length = 0; int left = 0; for (int right = 0; right \u0026lt; nums.size(); ++right) { windowSum = windowSum + nums[right]; while (windowSum \u0026gt; target) { windowSum = windowSum - nums[left]; ++left; } length = std::max(length, right-left+1); } return length; } template\u0026lt;typename T\u0026gt; std::vector\u0026lt;T\u0026gt; get_words() { std::string line; std::getline(std::cin, line); std::istringstream ss{line}; ss \u0026gt;\u0026gt; std::boolalpha; std::vector\u0026lt;T\u0026gt; v; std::copy(std::istream_iterator\u0026lt;T\u0026gt;{ss}, std::istream_iterator\u0026lt;T\u0026gt;{}, std::back_inserter(v)); return v; } void ignore_line() { std::cin.ignore(std::numeric_limits\u0026lt;std::streamsize\u0026gt;::max(), \u0026#39;\\n\u0026#39;); } int main() { std::vector\u0026lt;int\u0026gt; nums = get_words\u0026lt;int\u0026gt;(); int target; std::cin \u0026gt;\u0026gt; target; ignore_line(); int res = subarray_sum_longest(nums, target); std::cout \u0026lt;\u0026lt; res \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; } Flexible Size Sliding Window - Shortest # Let\u0026rsquo;s continue on finding the sum of subarrays. This time given a positive integer array nums, we want to find the length of the shortest subarray such that the subarray sum is at least target. Recall the same example with input nums = [1, 4, 1, 7, 3, 0, 2, 5] and target = 10, then the smallest window with the sum \u0026gt;= 10 is [7, 3] with length 2. So the output is 2.\nWe\u0026rsquo;ll assume for this problem that it\u0026rsquo;s guaranteed target will not exceed the sum of all elements in nums.\n#include \u0026lt;algorithm\u0026gt; // copy #include \u0026lt;iostream\u0026gt; // boolalpha, cin, cout, streamsize #include \u0026lt;iterator\u0026gt; // back_inserter, istream_iterator #include \u0026lt;limits\u0026gt; // numeric_limits #include \u0026lt;sstream\u0026gt; // istringstream #include \u0026lt;string\u0026gt; // getline, string #include \u0026lt;vector\u0026gt; // vector int subarray_sum_shortest(std::vector\u0026lt;int\u0026gt; nums, int target) { int windowSum = 0, length = nums.size(); int left = 0; for (int right = 0; right \u0026lt; nums.size(); ++right) { windowSum = windowSum + nums[right]; while (windowSum \u0026gt;= target) { length = std::min(length, right-left+1); windowSum = windowSum - nums[left]; ++left; } } return length; } template\u0026lt;typename T\u0026gt; std::vector\u0026lt;T\u0026gt; get_words() { std::string line; std::getline(std::cin, line); std::istringstream ss{line}; ss \u0026gt;\u0026gt; std::boolalpha; std::vector\u0026lt;T\u0026gt; v; std::copy(std::istream_iterator\u0026lt;T\u0026gt;{ss}, std::istream_iterator\u0026lt;T\u0026gt;{}, std::back_inserter(v)); return v; } void ignore_line() { std::cin.ignore(std::numeric_limits\u0026lt;std::streamsize\u0026gt;::max(), \u0026#39;\\n\u0026#39;); } int main() { std::vector\u0026lt;int\u0026gt; nums = get_words\u0026lt;int\u0026gt;(); int target; std::cin \u0026gt;\u0026gt; target; ignore_line(); int res = subarray_sum_shortest(nums, target); std::cout \u0026lt;\u0026lt; res \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; } Linked List Cycle # Given a linked list with potentially a loop, determine whether the linked list from the first node contains a cycle in it. For bonus points, do this with constant space.\nbool has_cycle(Node\u0026lt;int\u0026gt;* nodes) { Node\u0026lt;int\u0026gt;* tortoise = next_node(nodes); Node\u0026lt;int\u0026gt;* hare = next_node(next_node(nodes)); while (tortoise != hare \u0026amp;\u0026amp; hare-\u0026gt;next != NULL) { tortoise = next_node(tortoise); hare = next_node(next_node(hare)); } return hare-\u0026gt;next != NULL; } "},{"id":7,"href":"/tech-book/docs/ai-ml-dc/ai-ml-networking/","title":"AI/ML Networking","section":"Data Center Networking for AI Clusters","content":" Intro # AI/ML Networking Part I: RDMA Basics # TBD\nAI/ML Networking: Part-II: Introduction of Deep Neural Networks # Machine Learning (ML) is a subset of Artificial Intelligence (AI). ML is based on algorithms that allow learning, predicting, and making decisions based on data rather than pre-programmed tasks. ML leverages Deep Neural Networks (DNNs), which have multiple layers, each consisting of neurons that process information from sub-layers as part of the training process. Large Language Models (LLMs), such as OpenAI’s GPT (Generative Pre-trained Transformers), utilize ML and Deep Neural Networks.\nFor network engineers, it is crucial to understand the fundamental operations and communication models used in ML training processes. To emphasize the importance of this, I quote the Chinese philosopher and strategist Sun Tzu, who lived around 600 BCE, from his work The Art of War.\nIf you know the enemy and know yourself, you need not fear the result of a hundred battles.\nWe don’t have to be data scientists to design a network for AI/ML, but we must understand the operational fundamentals and communication patterns of ML. Additionally, we must have a deep understanding of network solutions and technologies to build a lossless and cost-effective network for enabling efficient training processes.\nIn the upcoming two posts, I will explain the basics of: a) Data Models: Layers and neurons, forward and backward passes, and algorithms. b) Parallelization Strategies: How training times can be reduced by dividing the model into smaller entities, batches, and even micro-batches, which are processed by several GPUs simultaneously.\nThe number of parameters, the selected data model, and the parallelization strategy affect the network traffic that crosses the data center switch fabric.\nAfter these two posts, we will be ready to jump into the network part. At this stage, you may need to read (or re-read) my previous post about Remote Direct Memory Access (RDMA), a solution that enables GPUs to write data from local memory to remote GPUs\u0026rsquo; memory. AI/ML Networking: Part-III: Basics of Neural Networks Training Process # Neural Network Architecture Overview # Deep Neural Networks (DNN) leverage various architectures for training, with one of the simplest and most fundamental being the Feedforward Neural Network (FNN). Figure 2-1 illustrates our simple, three-layer FNN.\nInput Layer: # The first layer doesn’t have neurons, instead the input data parameters X1, X2, and X3 are in this layer, from where they are fed to first hidden layer. Hidden Layer: # The neurons in the hidden layer calculate a weighted sum of the input data, which is then passed through an activation function. In our example, we are using the Rectified Linear Unit (ReLU) activation function. These calculations produce activation values for neurons. The activation value is modified input data value received from the input layer and published to upper layer.\nOutput Layer: # Neurons in this layer calculate the weighted sum in the same manner as neurons in the hidden layer, but the result of the activation function is the final output.\nThe process described above is known as the Forwarding pass operation. Once the forward pass process is completed, the result is passed through a loss function, where the received value is compared to the expected value. The difference between these two values triggers the backpropagation process. The Loss calculation is the initial phase of Backpropagation process. During backpropagation, the network fine-tunes the weight values , neuron by neuron, from the output layer through the hidden layers. The neurons in the input layer do not participate in the backpropagation process because they do not have weight values to be adjusted.\nAfter the backpropagation process, a new iteration of the forward pass begins from the first hidden layer. This loop continues until the received and expected values are close enough to expected value, indicating that the training is complete. Figure 2-1: Deep Neural Network Basic Structure and Operations.\nForwarding Pass # Next, let\u0026rsquo;s examine the operation of a Neural Network in more detail. Figure 2-2 illustrates a simple, three-layer Feedforward Neural Network (FNN) data model. The input layer has two neurons, H1 and H2, each receiving one input data value: a value of one (1) is fed to neuron H1 by input neuron X1, and a value of zero (0) is fed to neuron H2 by input neuron X2. The neurons in the input layer do not calculate a weighted sum or an activation value but instead pass the data to the next layer, which is the first hidden layer.\nThe hidden layer in our example consists of two neurons. These neurons use the ReLU activation function to calculate the activation value. During the initialization phase, the weight values for these neurons are assigned using the He Initialization method, which is often used with the ReLU function. The He Initialization method calculates the variance as 2/n where n is the number of neurons in the previous layer. In this example, with two input neurons, this gives a variance of 1 (=2/2). The weights are then drawn from a normal distribution ~N(0,√variance), which in this case is ~N(0,1). Basically, this means that the randomly generated weight values are centered around zero with a standard deviation of one.\nIn Figure 2-2, the weight value for neuron H3 in the hidden layer is 0.5 for both input sources X1 (input data 1) and X2 (input data 0). Similarly, for the hidden layer neuron H4, the weight value is 1 for both input sources X1 (input data 1) and X2 (input data 0). Neurons in the hidden and output layers also have a bias variable. If the input to a neuron is zero, the output would also be zero if there were no bias. The bias ensures that a neuron can still produce a meaningful output even when the input is zero (i.e., the neuron is inactive). Neurons H3 and O5 have a bias value of 0.5, while neuron H4 has a bias value of 0 (I am using zero for simplify the calculation). Let’s start the forward pass process from neuron H3 in the hidden layer. First, we calculate the weighted sum using the formula below, where Z3 represents the weighted sum of input. Here, Xn is the actual input data value received from the input layer’s neuron, and Wn is the weight associated with that particular input neuron.\nThe weighted sum calculation (Z3) for neuron H3:\nZ3 = (X1 ⋅ W31) + (X2 ⋅ W32) + b3 Given: Z3 = (1 ⋅ 0.5) + (0 ⋅ 0.5) + 0 Z3 = 0.5 + 0 + 0 Z3 = 0.5 To get the activation value a3 (shown as H3=0.5 in figure), we apply the ReLU function. The ReLU function outputs zero (0) if the calculated weighted sum Z is less than or equal to zero; otherwise, it outputs the value of the weighted sum Z.\nThe activation value a3 for H3 is:\nReLU (Z3) = ReLU (0.5) = 0.5\nThe weighted sum calculation for neuron H4:\nZ4 = (X1 ⋅ W41) + (X2 ⋅ W42) + b4 Given: Z4 = (1 ⋅ 1) + (0 ⋅1) + 0.5 Z4 = 1 + 0 + 0.5 Z4 = 1.5 The activation value using ReLU for Z4 is:\nReLU (Z4) = ReLU (1.5) = 1.5\nFigure 2-2: Forwarding Pass on Hidden Layer.\nAfter neurons H3 and H4 publish their activation values to neuron O5 in the output layer, O5 calculates the weighted sum Z5 for inputs with weights W53=1and W54=1. Using Z5, it calculates the output using the ReLU function. The difference between the received output value (Yr) and the expected value (Ye) triggers a backpropagation process. In our example, Yr−Ye=0.5.\nBackpropagation process # The loss function measures the difference between the predicted output and the actual expected output. The loss function value indicates how well the neural network is performing. A high loss value means the network\u0026rsquo;s predictions are far from the actual values, while a low loss value means the predictions are close.\nAfter calculating the loss, backpropagation is initiated to minimize this loss. Backpropagation involves calculating the gradient of the loss function with respect to each weight and bias in the network. This step is crucial for adjusting the weights and biases to reduce the loss in subsequent forwarding pass iterations.\nLoss function is calculated using the formula below:\nLoss (L) = (H3 x W53 + H4 x W54 + b5 – Ye)2 Given: L = (0.5 x 1 + 1.5 x 1 + 0.5 - 2)2 L = (0.5 + 1.5 + 0.5 - 2)2 L = 0.52 L= 0.25 Figure 2-3: Forwarding Pass on Output Layer.\nThe result of the loss function is then fed into the gradient calculation process, where we compute the gradient of the loss function with respect to each weight and bias in the network. The gradient calculation result is then used to fine-tune the old weight values. The Eta hyper-parameter η (the learning rate) controls the step size during weight updates in the backpropagation process, balancing the speed of convergence with the stability of training. In our example, we are using a learning rate of 1/100 = 0.01. The term hyper-parameters refers to parameters that affect the final result.\nFirst, we compute the partial derivative of the loss function (gradient calculation) with respect to the old weight values. The following example shows the gradient calculation for weight W53. The same computation applies to W54 and b3.\nGradient Calculation:\n∂L = 2W53 x (Yr – Ye) ∂W53 Given = 2 x 0.5 x (2.5 - 2) = 1 x 0.5 = 0.5 New weight value calculation.\nW53 (new) = W53(old) – η x ∂L/∂W53 Given: W53 (new) = 1–0.01 x 0.5 W53 (new) = 0.995 Figure 2-4: Backpropagation - Gradient Calculation and New Weight Value Computation.\nFigure 2-5 shows the formulas for calculating the new bias b3. The process is the same than what was used with updating the weight values.\nFigure 2-5: Backpropagation - Gradient Calculation and New Bias Computation.\nAfter updating the weights and biases, the backpropagation process moves to the hidden layer. Gradient computation in the hidden layer is more complex because the loss function only includes weights from the output layer as you can see from the Loss function formula below:\nLoss (L) = (H3 x W53 + H4 x W54 + b5 – Ye)2\nThe formula for computing the weights and biases for neurons in the hidden layers uses the chain rule. The mathematical formula shown below, but the actual computation is beyond the scope of this chapter.\n∂L = ∂L x ∂H3 ∂W31 ∂H3 ∂W31 After the backpropagation process is completed, the next iteration of the forward pass starts. This loop continues until the received result is close enough to the expected result.\nIf the size of the input data exceeds the GPU’s memory capacity or if the computing power of one GPU is insufficient for the data model, we need to decide on a parallelization strategy. This strategy defines how the training workload is distributed across several GPUs. Parallelization impacts network load if we need more GPUs than are available on one server. Dividing the workload among GPUs within a single GPU-server or between multiple GPU-servers triggers synchronization of calculated gradients between GPUs. When the gradient is calculated, the GPUs synchronize the results and compute the average gradient, which is then used to update the weight values.\nThe upcoming chapter introduces pipeline parallelization and synchronization processes in detail. We will also discuss why lossless connection is required for AI/ML.\nAI/ML Networking: Part-IV: Convolutional Neural Network (CNN) Introduction # Feed-forward Neural Networks are suitable for simple tasks like basic time series prediction without long-term relationships. However, FNNs is not a one-size-fits-all solution. For instance, digital image training process uses pixel values of image as input data. Consider training a model to recognize a high resolution (600 dpi), 3.937 x 3.937 inches digital RGB (red, green, blue) image. The number of input parameters can be calculated as follows:\nWidth: 3.937 in x 600 ≈ 2362 pixels\nHeight: 3.937 in x 600 ≈ 2362 pixels\nPixels in image: 2362 x 2362 = 5,579,044 pixels\nRGB (3 channels): 5,579,044 pxls x 3 channels = 16 737 132\nTotal input parameters: 16 737 132\nMemory consumption: ≈ 16 MB\nFNNs are not ideal for digital image training. If we use FNN for training in our example, we fed 16,737,132 input parameters to the first hidden layer, each having unique weight. For image training, there might be thousands of images, handling millions of parameters demands significant computation cycles and is a memory-intensive process. Besides, FNNs treat each pixel as an independent unit. Therefore, FNN algorithm does not understand dependencies between pixels and cannot recognize the same image if it shifts within the frame. Besides, FNN does not detect edges and other crucial details. A better model for training digital images is Convolutional Neural Networks (CNNs). Unlike in FFN neural networks where each neuron has a unique set of weights, CNNs use the same set of weights (Kernel/Filter) across different regions of the image, which reduces the number of parameters. Besides, CNN algorithm understands the pixel dependencies and can recognize patterns and objects regardless of their position in the image. The input data processing in CNNs is hierarchical. The first layer, convolutional layers, focuses on low-level features such as textures and edges. The second layer, pooling layer, captures higher-level features like shapes and objects. These two layers significantly reduce the input data parameters before they are fed into the neurons in the first hidden layer, the fully connected layer, where each neuron has unique weights (like FNNs).\nConvolution Layer # The convolution process uses a shared kernel (also known as filters), which functions similarly to a neuron in a feed-forward neural network. The kernel\u0026rsquo;s coverage area, 3x3 in our example, defines how many pixels of an input image is covered at a given stride. The kernel assigns a unique weight (w) to each covered pixel (x) in a one-to-one mapping fashion and calculates the weighted sum (z) from the input data. For instance, in figure 3-1 the value of the pixel W1 is multiplied with the weight value W1 (X1W1), and pixel X2 is multiplies with weight value W2 (X2W2) and so on. Then the results are summed, which returns a weighted sum Z. The result Z is then passed through the ReLU function, which defines the value of the new pixel (P1). This new pixel is then placed into a new image matrix.\nFigure 3-1: CNN Overview – Convolution, Initial State (Stride 0).\nAfter calculating the new pixel value for the initial coverage area with stride zero, the kernel shifts to the right by the number of steps defined in the kernel\u0026rsquo;s stride value. In our example, the stride is set to one, and the kernel moves one step to the right, covering pixels shown in Figure 3-2. Then the weighted sum is (z) calculated and run through the ReLU function, and the second pixel is added to the new matrix. Since there are no more pixels to the right, the kernel moves down by one step and returns to the first column (Figure 3-3).\nFigure 3-2: CNN Overview – Convolution, Second State (Stride 1).\nFigures 3-3 and 3-4 shows the last two steps of the convolution. Notice that we have used the same weight values in each iteration. In the initial state weight w1 was associated with the first pixel (X1W1), and in the second phase with the second pixel (X2W1) and so on. The new image matrix produced by the convolutional process have decreased the 75% from the original digital image. Figure 3-3: CNN Overview – Convolution, Third State (Stride 2).\nFigure 3-4: CNN Overview – Convolution, Fourth State (Stride 3).\nIf we don\u0026rsquo;t want to decrease the size of the new matrix, we must use padding. Padding adds pixels to the edges of the image. For example, a padding value of one (1) adds one pixel to the image edges, providing a new matrix that is the same size as the original image.\nFigure 3-5: CNN Overview – Padding.\nFigure 3-6 illustrates the progression of the convolution layer from the initial state (which I refer to as stride 0) to the final phase, stride 3. The kernel covers a 3x3 pixel area in each phase and moves with a stride of 1. I use the notation SnXn to denote the stride and the specific pixel. For example, in the initial state, the first pixel covered by the kernel is labeled as S0X1, and the last pixel is labeled as S0X11.\nWhen the kernel shifts to the left, covering the next region, the first pixel is marked as S1X2 and the last as S1X12. The same notation is applied to the weighted sum calculation. The weighted value for the first pixel is represented as (S0X1) · W1 = Z1 and for the last one as (S0X11) · W9 = Z9. The weighted input for all pixel values is then summed, and a bias is added to obtain the weighted sum for the given stride.\nIn the initial state, this calculation results in = Z0, which is passed through the ReLU function. The output of this function provides the value for the first pixel in the new image matrix.\nFigure 3-6: CNN Overview – Convolution Summary.\nConvolution Layer Example # In Figure 3-7, we have a simple 12x12 = 144 pixels grayscale image representing the letter \u0026ldquo;H.\u0026rdquo; In the image, the white pixels have a binary value of 255, the gray pixels have a binary value of 87, and the darkest pixels have a binary value of 2. Our kernel size is 4x4, covering 16 pixels in each stride. Because the image is grayscale, we only have one channel. The kernel uses the ReLU activation function to determine the value for the new pixel. Initially, at stride 0, the kernel is placed over the first region in the image. The kernel has a unique weight value for each pixel it covers, and it calculates the weighted sum for all 16 pixels. The value of the first pixel (X1 = 87) is multiplied by its associated kernel weight (W1 = 0.12), which gives us a new value of 10.4. This computation runs over all 16 pixels covered by the kernel (results shown in Figure 3-7). The new values are then summed, and a bias is added, giving a weighted sum of Z0 = 91.4. Because the value of Z0 is positive, the ReLU function returns an activation value of 91.4 (if Z \u0026gt; 0, Z = Z; otherwise, Z = 0). The activation value of 91.4 becomes the value of our new pixel in the new image matrix.\nFigure 3-7: Convolution Layer Operation Example – Stride 0 (Initial State).\nNext, the kernel shifts one step to the right (Stride 1) and multiplies the pixel values by the associated weights. The changing parameters are the values of the pixels, while the kernel weight values remain the same. After the multiplication process is done and the weighted sum is calculated, the result is run through the ReLU function. At this stride, the result of the weighted sum (Z1) is negative, so the ReLU function returns zero (0). This value is then added to the matrix. At this phase, we have two new pixels in the matrix.\nFigure 3-8: Convolution Layer Operation Example – Stride 1.\nThe next four figures 3-9, 3-10, and 3-11 illustrates how the kernel is shifted over the input digital image and producing a new 9x9 image matrix.\nFigure 3-9: Convolution Layer Operation Example – Stride 2.\nFigure 3-10: Convolution Layer Operation Example – Stride 8.\nFigure 3-11: Convolution Layer Operation Example – Stride 10.\nFigure 3-12 illustrates the completed convolutional layer computation. At this stage, the number of pixels in the original input image has decreased from 144 to 81, representing a reduction of 56.25%.\nFigure 3-12: Convolution Layer Operation Example – The Last Stride. Pooling Layer # After the original image is processed by the convolution layer, the resulting output is used as input data for the next layer, the pooling layer. The pooling layer performs a simpler operation than the convolution layer. Like the convolution layer, the pooling layer uses a kernel to generate new values. However, instead of applying a convolution operation, the pooling layer selects the highest value within the kernel (if MaxPooling is applied) or computes the average of the values covered by the kernel (Average Pooling).\nIn this example, we use MaxPooling with a kernel size of 2x2 and a stride of 2. The first pixel is selected from values 91, 0, 112, and 12, corresponding to pixels in positions 1, 2, 10, and 11, respectively. Since the pixel at position 10 has the highest value (112), it is selected for the new matrix.\nFigure 3-13: Pooling Layer – Stride 0 (Initial Phase).\nAfter selecting the highest value from the initial phase, the kernel moves to the next region, covering the values 252, 153, 212, and 52. The highest value, 252, is then placed into the new matrix. Figure 3-14: Pooling Layer – Stride 1.\nFigure 3-15 illustrates how MaxPooling progresses to the third region, covering the values 141, 76, 82, and 35. The highest value, 141, is then placed into the matrix.\nFigure 3-15: Pooling Layer – Stride 2.\nFigure 3-16 describes how the original 12x12 (144 pixels) image is first processed by the convolution layer, reducing it to a 9x9 (81 pixels) matrix, and then by the pooling layer, further reducing it to a 5x5 (25 pixels) matrix.\nFigure 3-16: CNN Convolution and Pooling Layer Results.\nAs the final step, the matrix generated by the pooling layer is flattened and used as input for the neurons in the fully connected layer. This means we have 25 input neurons, each with a unique weight assigned to every input parameter. The neurons in the input layer calculate the weighted sum, which neurons then use to determine the activation value. This activation value serves as the input data for the output layer. The neurons in the output layer produce the result based on their calculations, with the output H proposed by three output neurons.\nFigure 3-17: The Model of Convolution Neural Network (CNN).\nReferences:\nhttps://nwktimes.blogspot.com/2024/06/aiml-networking-part-i-rdma-basics.html https://nwktimes.blogspot.com/2024/07/aiml-networking-part-ii-introduction-of.html https://nwktimes.blogspot.com/2024/07/aiml-networking-part-iii-basics-of.html https://nwktimes.blogspot.com/2024/08/aiml-networking-part-iv-convolutional.html "},{"id":8,"href":"/tech-book/docs/5g/5g-intro/","title":"An Overview of 5G Networking","section":"5G","content":"from https://datatracker.ietf.org/doc/draft-ietf-teas-5g-ns-ip-mpls/\nAppendix B. An Overview of 5G Networking # This section provides a brief introduction to 5G mobile networking with a perspective on the Transport Network. This section does not intend to replace or define 3GPP architecture, instead its objective is to provide an overview for readers that do not have a mobile background. For more comprehensive information, refer to [TS-23.501].\nB.1. Key Building Blocks # [TS-23.501] defines the Network Functions (UPF, Access and Mobility Function (AMF), etc.) that compose the 5G System (5GS) Architecture together with related interfaces (e.g., N1 and N2). This architecture has built-in control and user plane separation, and the control plane leverages a Service- Based Architecture (SBA). Figure 33 outlines an example 5GS architecture with a subset of possible NFs and network interfaces.\n+-----+ +-----+ +-----+ +-----+ +-----+ +-----+ |NSSF | | NEF | | NRF | | PCF | | UDM | | AF | +--+--+ +--+--+ +--+--+ +--+--+ +--+--+ +--+--+ Nnssf| Nnef| Nnrf| Npcf| Nudm| |Naf ---+--------+--+-----+----------+---+----+--------+---- Nausf| Namf| Nsmf| +--+--+ +--+--+ +--+------+ |AUSR | | AMF | | SMF | +-----+ +--+--+ +--+------+ / | | \\\\ Control Plane N1 / |N2 |N4 \\\\N4 ------------------------------------------------------------ User Plane / | | \\\\ +---+ +--+--+ N3 +--+--+ N9 +-----+ N6 .---. |UE +--+(R)AN+-----+ UPF +----+ UPF +----( DN ) +---+ +-----+ +-----+ +-----+ \u0026#39;---\u0026#39; Figure 33: 5GS Architecture and Service-based Interfaces\nSimilar to previous versions of 3GPP mobile networks [RFC6459], a 5G mobile network is split into the following four major domains (Figure 34):\nUE, MS, MN, and Mobile:\nThe terms User Equipment (UE), Mobile Station (MS), Mobile Node (MN), and mobile refer to the devices that are hosts with the ability to obtain Internet connectivity via a 3GPP network. An MS is comprised of a Terminal Equipment (TE) and a Mobile Terminal (MT).\nRadio Access Network (RAN):\nProvides wireless connectivity to UEs. A RAN is made up of the Antenna that transmits and receives signals to UEs and the Base Station that digitizes the signal and converts the Radio Frequency (RF) data stream to IP packets.\nCore Network (CN):\nControls the CP of the RAN and provides connectivity to the Data Network (e.g., the Internet or a private VPN). The Core Network hosts dozens of services such as authentication, phone registry, charging, access to Public Switched Telephony Network (PSTN) and handover.\nTransport Network (TN):\nProvides connectivity between 5G NFs. The TN may provide connectivity from the RAN to the CN as well as within the RAN or within the CN. The traffic generated by NFs is - mostly - based on IP or Ethernet.\n+----------------------------------------------+ | +------------+ +------------+ | | +----+ | | | | | .-------. | | UE +------+ RAN | | CN +----( DN ) | +----+ | | | | | \u0026#39;-------\u0026#39; | +------+-----+ +------+-----+ | | | | | | +-----+-----------------+----+ | | | Transport Network | | | +----------------------------+ | | | | 5G System | +----------------------------------------------+ Figure 34: Building Blocks of 5G Architecture (A High-Level Representation)\nB.2. Core Network (CN) # The 5G Core Network (5GC) is made up of a set of NFs which fall into two main categories (Figure 35):\n5GC User Plane:\nThe UPF is the interconnect point between the mobile infrastructure and the Data Network (DN). It interfaces with the RAN via the N3 interface by encapsulating/ decapsulating the user plane traffic in GTP tunnels (aka GTP-U or Mobile user plane).\n5GC Control Plane:\nThe 5G control plane is made up of a comprehensive set of NFs. The description of these entities is out of the scope of this document. The following NFs and interfaces are worth mentioning, since their connectivity may rely on the Transport Network:\nthe AMF connects with the RAN control plane over the N2 interface\nthe SMF controls the 5GC UPF via the N4 interface\n+---------+ +-------------------------+ | RAN | | 5G Core (5GC) | | | | | | | | \\[AUSF NRF UDM ...\\] | | | | (SBA) | | | | | | | N2 | +-----+ N11 +-----+ | | CP -----------+ AMF +-----+ SMF | | | | | +-----+ +--+--+ | | | | | | Control Plane ----------------------------------------------------------- | | | | N4 | User Plane | | N3 | +--+--+ | N6 .-------. | UP -----------------------+ UPF +-------\u0026gt;( DN ) | | | +-----+ | \\`-------\u0026#39; +---------+ +-------------------------+ Figure 35: 5G Core Network (CN)\nB.3. Radio Access Network (RAN) # The RAN connects cellular wireless devices to a mobile Core Network. The RAN is made up of three components, which form the Radio Base Station:\nThe Baseband Unit (BBU) provides the interface between the Core Network and the Radio Network. It connects to the Radio Unit and is responsible for the baseband signal processing to packet.\nThe Radio Unit (RU) is located close to the Antenna and controlled by the BBU. It converts the Baseband signal received from the BBU to a Radio frequency signal.\nThe Antenna converts the electric signal received from the RU to radio waves\nThe 5G RAN Base Station is called a gNodeB (gNB). It connects to the Core Network via the N3 (User Plane) and N2 (Control Plane) interfaces.\nThe 5G RAN architecture supports RAN disaggregation in various ways. Notably, the BBU can be split into a DU (Distributed Unit) for digital signal processing and a CU (Centralized Unit) for RAN Layer 3 processing. Furthermore, the CU can be itself split into Control Plane (CU-CP) and User Plane (CU-UP).\nFigure 36 depicts a disaggregated RAN with NFs and interfaces.\n+---------------------------------+ +-----------+ | | N3 | | +----+ NR | +----+ 5G Core | | UE +------+ gNodeB | | | +----+ | +----+ (5GC) | | | N2 | | +---------------------------------+ +-----------+ | | .+ +. \\\\ / \\\\ / +---------------------------------+ +-----------+ | +-------------------+ | | | | | | | | | +----+ NR | +----+ F2 |+----+ F1-U +-----+| | N3 | +-----+ | | UE +--------+ RU +-----+ DU +------+CU-UP+----------+ UPF | | +----+ | +----+ |+-+--+ +--+--+| | | +-----+ | | | | |E1 | | | | | | | F1-C | | | | | | | | +--+--+| | N2 | +-----+ | | | +---------+CU-CP+----------+ AMF | | | | +-----+| | | +-----+ | | | BBU split | | | 5G Core | | +-------------------+ | | | | Disaggregated gNodeB | | | +---------------------------------+ +-----------+ Figure 36: RAN Disaggregation\nB.4. Transport Network (TN) # The 5G transport architecture defines three main segments for the Transport Network, which are commonly referred to as Fronthaul (FH), Midhaul (MH), and Backhaul (BH) [TR-GSTR-TN5G]:\nFronthaul happens before the BBU processing. In 5G, this interface is based on eCPRI with Ethernet or IP encapsulation.\nMidhaul is optional: this segment is introduced in the BBU split presented in Appendix B.3, where Midhaul network refers to the DU- CU interconnection (i.e., F1 interface). At this level, all traffic is encapsulated in IP (signaling and user plane).\nBackhaul happens after BBU processing. Therefore, it maps to the interconnection between the RAN and the CN. All traffic is encapsulated in IP.\nFigure 37 illustrates the different segments of the Transport Network with the relevant NFs.\n+---------------------------------------------------------+ | Transport Network | | | | Fronthaul Midhaul Backhaul | | +-----------+ +------------+ +-----------+ | | | | | | | | | +--|-----------|-|------------|-|-----------|-------------+ +-+--+ +-+-++ +-+-++ +-+---+ .---. | RU | | DU | | CU | | UPF :----( DN ) +----+ +----+ +----+ +-----+ \\`---\u0026#39; Figure 37: 5G Transport Segments\nA given part of the transport network can carry several 5G transport segments concurrently, as outlined in Figure 38. This is because different types of 5G NFs might be placed in the same location (e.g., the UPF from one slice might be placed in the same location as the CU-UP from another slice).¶\n+---------+ |+----+ | Colocated ||RU-1| | RU/DU |+-+--+ | | | FH-1 | |+-+--+ | ||DU-1| | +----+ +-----+ .---. |+-+--+ | |CU-1| |UPF-1+--------( DN ) +--|------+ +-+-++ +-+---+ \\`---\u0026#39; +--|-----------|-|------------|----------------------------+ | | MH-1 | | BH-1 | Transport Network | | +-----------+ +------------+ | | +-----------+ +------------+ +-----------+ | | | FH-2 | | MH-2 | | BH-2 | | +--|-----------|-|------------|-|-----------|--------------+ +-+--+ +-+-++ +-+-++ +-+---+ .---. |RU-2| |DU-2| |CU-2| |UPF-2+----( DN ) +----+ +----+ +----+ +-----+ \\`---\u0026#39; Figure 38: Concurrent 5G Transport Segments\n"},{"id":9,"href":"/tech-book/docs/ai-ml-dc/challenges-in-ai-fabric/","title":"Challenges in AI Fabric Design","section":"Data Center Networking for AI Clusters","content":" Intro # Figure 10-1 illustrates a simple distributed GPU cluster consisting of three GPU hosts. Each host has two GPUs and a Network Interface Card (NIC) with two interfaces. Intra-host GPU communication uses high-speed NVLink interfaces, while inter-host communication takes place via NICs over slower PCIe buses.\nGPU-0 on each host is connected to Rail Switch A through interface E1. GPU-1 uses interface E2 and connects to Rail Switch B. In this setup, inter-host communication between GPUs connected to the same rail passes through a single switch. However, communication between GPUs on different rails goes over three hops Rail–Spine–Rail switches.\nIn Figure 10-1, we use a data parallelization strategy where a training dataset is split into six micro-batches, which are distributed across the GPUs. All GPUs use the shared feedforward neural network model and compute local model outputs. Next, each GPU calculates the model error and begins the backward pass to compute neuron-based gradients. These gradients indicate how much, and in which direction, the weight parameters should be adjusted to improve the training result (see Chapter 2 for details).\nFigure 10-1: Rail-Optimized Topology.\nEgress Interface Congestions # After computing all gradients, each GPU stores the results in a local memory buffer and starts a communication phase where computed gradients are shared with other GPUs. During this process, the data (gradients) is being sent from one GPU and written to another GPU\u0026rsquo;s memory (RDMA Write operation). RDMA is explained in detail in Chapter 9.\nOnce all gradients have been received, each GPU averages the results (AllReduce) and broadcasts the aggregated gradients to the other GPUs. This ensures that all GPUs update their model parameters (weights) using the same gradient values. The Backward pass process and gradient calculation are explained in Chapter 2.\nFigure 10-2 illustrates the traffic generated during gradient synchronization from the perspective of GPU-0 on Host-2. Gradients from the local host’s GPU-1 are received via the high-speed NVLink interface, while gradients from GPUs in other hosts are transmitted over the backend switching fabric. In this example, all hosts are connected to Rail Switches using 200 Gbps fiber links. Since GPUs can communicate at line rate, gradient synchronization results in up to 800 Gbps of egress traffic toward interface E1 on Host-2, via Rail Switch A. This may cause congestion, and packet drops if the egress buffer on Rail Switch A or the ingress buffer on interface E1 is not deep enough to accommodate the queued packets.\nFigure 10-2: Congestion During Backward Pass.\nSingle Point of Failure # The training process of a neural network is a long-running, iterative task where GPUs must communicate with each other. The frequency and pattern of this communication depend on the chosen parallelization strategy. For example, in data parallelism, communication occurs during the backward pass, where GPUs synchronize gradients. In contrast, model parallelism and pipeline parallelism involve communication even during the forward pass, as one GPU sends activation results to the next GPU holding the subsequent layer. It is important to understand that communication issues affecting even a single GPU can delay or interrupt the entire training process. This makes the AI fabric significantly more sensitive to single points of failure compared to traditional data center fabrics.\nFigure 10-3 highlights several single points of failure that may occur in real-world environments. A host connection can become degraded or completely fail due to issues in the host, NIC, rail switch, transceiver, or connecting cable. Any of these failures can isolate a GPU. While this might not seem serious in large clusters with thousands of GPUs, as discussed in the previous section, even one isolated or failed GPU can halt the training process.\nProblems with interfaces, transceivers, or cables in inter-switch links can cause congestion and delays. Similar issues arise if a spine switch is malfunctioning. These types of failures typically affect inter-rail traffic but not intra-rail communication. A failure in a rail switch can isolate all GPUs connected to that rail, creating a critical point of failure for a subset of the GPU cluster.\nFigure 10-3: Single-Point Failures.\nHead-of-Line Blocking # In this example GPU clusters, NCCL (NVIDIA Collective Communications Library) has built a topology where gradients are first sent from GPU-0 to GPU-1 over NVLink, and then forwarded from GPU-1 to other GPU-1s via Rail switch B.\nHowever, this setup may lead to head-of-line blocking. This happens when GPU-1 is already busy sending its own gradients to the other GPUs, and now it also needs to forward GPU-0’s gradients. Since the PCIe and NIC bandwidth is limited, GPU-0’s traffic may need to wait in line behind GPU-1’s traffic. This queueing delay is called head-of-line blocking, and it can slow down the whole training process. The problem is more likely to happen when many GPUs rely on a single GPU or NIC for forwarding traffic to another rail. Even if only one GPU is overloaded, it can cause delays for others too.\nFigure 10-4: Head-of-Line Blocking.\nHash-Polarization with ECMP # First, when two GPUs open a Queue Pair (QP) between each other, all gradient synchronization traffic is typically sent over that QP. From the network point of view, this looks like one large flow between the GPUs. In deep learning training, gradient data can be hundreds of megabytes or even gigabytes, depending on the model size. So, when it is sent over one QP, the network sees it as a single high-bandwidth flow. This kind of traffic is often called an elephant flow, because it can take a big share of the link bandwidth. This becomes a problem when multiple large flows are hashed to the same uplink or spine port. If that happens, one link can get overloaded while others remain underused. This is one of the reasons we see hash polarization and head-of-line blocking in AI clusters. Hash polarization is a condition where the load-balancing hash algorithm used in ECMP (Equal-Cost Multi-Path) forwarding results in uneven distribution of traffic across multiple available paths.\nFor example, in Figure 10-5, GPU-0 in Host-1 and GPU-0 in Host-2 both send traffic to GPU-1 at a rate of 200 Gbps. The ECMP hash function in Rail Switch A selects the link to Spine 1 for both flows. This leads to a situation where one spine link carries 400 Gbps of traffic, while the other remains idle. This is a serious problem in AI clusters because training jobs generate large volumes of east-west traffic between GPUs, often at line rate. When traffic is unevenly distributed due to hash polarization, some links become congested while others are idle. This causes packet delays and retransmissions, which can slow down gradient synchronization and reduce the overall training speed. In large-scale clusters, even a small imbalance can have a noticeable impact on job completion time and resource efficiency.\nFigure 10-5: ECMP Hash-Polarization.\nThe rest of this book focuses on how these problems can be mitigated or even fully avoided. We will look at design choices, transport optimizations, network-aware scheduling, and alternative topologies that help improve the robustness and efficiency of the AI fabric.\nReferences:\n"},{"id":10,"href":"/tech-book/docs/data-center/data-center-ethernet/","title":"Data Center Ethernet","section":"Data Center Tips","content":" Intro # Residential vs Data Center Ethernet # Link Aggregation Control Protocol (LACP) # Spanning Tree Protocol # Multiple Spanning Tree Protocol # Multiple Spanning Tree Protocol # ISIS Protocol # Shortest Path Bridge (SPB) # VLAN # LLDP # Data Center Bridging # Pause Frame # Priority Based flow control (PFC) # Enhanced Transmission Selection # # Congestion Notification # DCBX # References:\nRaj Jain Data Center Ethernet by Raj Jain Data Center Tutorial "},{"id":11,"href":"/tech-book/docs/data-center/data-center-technologies/","title":"Data Center Technologies","section":"Data Center Tips","content":" Intro # Data Center Network Topologies: 3-Tier # 3-Tier Data Center Networks # 3-Tier Data Center Networks (Cont) # 3-Tier Hierarchical Network Design # Problem with 3-Tier Topology # Clos Networks # Fat-Tree DCN # Fat-Tree Topology (Cont) # Advantages of 2-Tier Architecture # Rack-Scale Architecture # References:\nRaj Jain Data Center Network Topologies by Raj Jain "},{"id":12,"href":"/tech-book/docs/ai-ml-dc/ai-deep-learning-basics/","title":"Deep Learning Basics | Artificial Neuron","section":"Data Center Networking for AI Clusters","content":" Content # Introduction Artificial Neuron Weighted Sum for Pre-Activation Value ReLU Activation Function for Post-Activation Bias Term S-Shaped Functions – TANH and SIGMOID Network Impact Summary Introduction # Artificial Intelligence (AI) is a broad term for solutions that aim to mimic the functions of the human brain. Machine Learning (ML), in turn, is a subset of AI, suitable for tasks like simple pattern recognition and prediction. Deep Learning (DL), the focus of this section, is a subset of ML that leverages algorithms to extract meaningful patterns from data. Unlike ML, DL does not necessarily require human intervention, such as providing structured, labeled datasets (e.g., 1,000 bird images labeled as “bird” and 1,000 cat images labeled as “cat”). DL utilizes layered, hierarchical Deep Neural Networks (DNNs), where hidden and output layers consist of computational units, artificial neurons, which individually process input data. The nodes in the input layer pass the input data to the first hidden layer without performing any computations, which is why they are not considered neurons or computational units. Each neuron calculates a pre-activation value (z) based on the input received from the previous layer and then applies an activation function to this value, producing a post-activation output (ŷ) value. There are various DNN models, such as Feed-Forward Neural Networks (FNN), Convolutional Neural Networks (CNN), and Recurrent Neural Networks (RNN), each designed for different use cases. For example, FNNs are suitable for simple, structured tasks like handwritten digit recognition using the MNIST dataset [1], CNNs are effective for larger image recognition tasks such as with the CIFAR-10 dataset [2], and RNNs are commonly used for time-series forecasting, like predicting future sales based on historical sales data. To provide accurate predictions based on input data, neural networks are trained using labeled datasets. The MNIST (Modified National Institute of Standards and Technology) dataset [1] contains 60,000 training and 10,000 test images of handwritten digits (grayscale, 28x28 pixels). The CIFAR-10 [2] dataset consists of 60,000 color images (32x32 pixels), with 50,000 training images and 10,000 test images, divided into 10 classes. The CIFAR-100 dataset [3], as the name implies, has 100 image classes, with each class containing 600 images (500 training and 100 test images per class). Once the test results reach the desired level, the neural network can be deployed to production.\nFigure 1-1: Deep Learning Introduction.\nArtificial Neuron # As we saw in the previous section, DNNs leverage a hierarchical, layered, role-based (input layer, n hidden layers, and output layer) network model, where each layer consists of artificial neurons. Neurons in different layers may be fully or partially connected to neurons in the other layers, depending on the DNN’s network model. However, neurons within the same layer are not connected.\nThe logical structure and functionality of an artificial neuron aim to emulate those of a biological neuron. A biological neuron transmits electrical signals to its peer neuron via the output axon to the axon terminal, which connects to the dendrites of the peer neuron at a connection point called a synapse. A biological neuron may receive signals from multiple dendrites, and if the signal is strong enough, it activates the neuron, causing the cell body to trigger a signal through its output axon, which connects to another neuron, and the process continues.\nAn artificial neuron, as a computational unit, calculates the weighted sum (z = pre-activation value) of the input (x) received from the previous layer, adds a bias value (b), and applies an activation function to produce the output (ŷ = post-activation value). The weight value for the input can be loosely compared to a synapse since it represents a connection, input values are assigned with weight. The weights-to-neuron association, in turn, can be seen as analogous to dendrites. The computational processes (weighted sum of inputs, bias addition, and activation functions) represent the cell body, while the connections to other neurons can be compared to output axons. In Figure 1-2, bn refers to a biological neuron. From now on, \u0026ldquo;neuron\u0026rdquo; will refer to an artificial neuron.\nFigure 1-2: Construct of an Artificial Neuron.\nWeighted Sum for Pre-Activation Value # The lower part of Figure 1-2 depicts the mathematical formulas of a neuron. The pre-activation value z is the weighted sum of the inputs. Although the bias is not part of the input data, a neuron treats it as an input variable when calculating the weighted sum. Each input x has a corresponding weight w. The calculation process is straightforward: each input value is multiplied by its corresponding weight, and the results are summed to obtain the weighted sum z.\nThe capital Greek letter sigma ∑ in the formula indicates that we are summing a series of terms, which, in this case, are the input values multiplied by their respective weights. The letter n specifies how many terms are being summed (four pairs of inputs and weights), while the letter iii denotes the starting point of the summation (bias b0 and weight w0). The equation for the weighted sum is:\nz = b0w0 + x1w1 + x2w2 + x3w3. ReLU Activation Function for Post-Activation # Next, the process applies an activation function, ReLU (Rectified Linear Unit) [4] in our example, to the weighted sum z obtain the post-activation value ŷ. The output of the ReLU function is z if z is greater than zero (0); otherwise, the result is zero (0). This can be written as: ŷ = MAX (0, z) which selects the larger value between 0 and variable z. The figure 1-3 depicts the ReLU activation function.\nFigure 1-3: Construct of an Artificial Neuron.\nBased on the figure 1-3 we can use the mathematical definition below for ReLU:\nBias Term # In the example calculation above, imagine that all input values are zero. Without a bias term, the activation value will be zero, regardless of how large the weight parameters are. Therefore, the bias term allows the neuron to produce non-zero outputs, even when all input values are zero.\nS-Shaped Functions – TANH and SIGMOID # The ReLU function is a non-linear activation function. Naturally, there are other activation functions as well. The Hyperbolic Tangent (tanh) [5] and the logistic Sigmoid [6] functions are examples of S-shaped functions that are symmetric around zero. Figure 1-4 illustrates that as the positive z value increases, the tanh function approaches one (1), while as the negative z value decreases, it approaches -1. Thus, the range of the tanh function is from -1 to 1. Similarly, the sigmoid function\u0026rsquo;s S-curve is also symmetric around zero, but its range is from 0 to 1.\nFigure 1-4: Tanh and Sigmoid functions.\nHere are examples of both functions using the same pre-activation value of 3.02, as in the ReLU activation function example.\nNote, the ⅇ represents Euler’s Number ⅇ≈ 2.718. The symbol σ represents the sigmoid function.\nThe formula for tanh function is:\nThe tanh function for z = 3,02\nThe formula for sigmoid function is:\nThe sigmoid function for z = 3,02\nThe symbol σ represents sigmoid function.\nFor z=3.02, both the sigmoid and tanh functions return values close to 1, but the tanh function is slightly higher, approaching its maximum of 1 more quickly than the sigmoid.\nWhich activation function should we use? The ReLU function has largely replaced the tanh and sigmoid functions due to its simplicity, which reduces the required computation cycles. However, if tanh and sigmoid are used, tanh is typically applied in the hidden layers, while sigmoid is used in the output layer.\nBackpropagation Algorithm: Introduction # This chapter introduces the training model of a neural network based on the Backpropagation algorithm. The goal is to provide a clear and solid understanding of the process without delving deeply into the mathematical formulas, while still explaining the fundamental operations of the involved functions. The chapter also briefly explains why, and in which phases the training job generates traffic to the network, and why lossless packet transport is required. The Backpropagation algorithm is composed of two phases: the Forward pass (computation phase) and the Backward pass (adjustment and communication phase).\nIn the Forward pass, neurons in the first hidden layer calculate the weighted sum of input parameters received from the input layer, which is then passed to the neuron\u0026rsquo;s activation function. Note that neurons in the input layer are not computational units; they simply pass the input variables to the connected neurons in the first hidden layer. The output from the activation function of a neuron is then used as input for the connected neurons in the next layer, whether it is another hidden layer or the output layer. The result of the activation function in the output layer represents the model\u0026rsquo;s prediction, which is compared to the expected value (ground truth) using the error function. The output of the error function indicates the accuracy of the current training iteration. If the result is sufficiently close to the expected value, the training is complete. Otherwise, it triggers the Backward pass process.\nIn the Backward pass process, the Backpropagation algorithm first calculates the derivative of the error function. This derivative is then used to compute the error term for each neuron in the model. Neurons use their calculated error terms to determine how much and in which direction the current weight values must be fine-tuned. Depending on the model and the parallelization strategy, GPUs in multi-GPU clusters synchronize information during the Backpropagation process. This process affects network utilization.\nOur feedforward neural network, shown in Figure 2-1, has one hidden layer and one output layer. If we wanted our example to be a deep neural network, we would need to add additional layers, as the definition of \u0026ldquo;deep\u0026rdquo; requires two or more hidden layers. For simplicity, the input layer is not shown in the figure.\nWe have three input parameters connected to neuron-a in the hidden layer as follows:\nInput X1 = 0.2 \u0026gt; neuron-a via weight Wa1 = 0.1 Input X2 = 0.1 \u0026gt; neuron-a via weight Wa2 = 0.2 Input X3 = 0.4 \u0026gt; neuron-a via weight Wa3 = 0.3 Bias ba0 = 1.0 \u0026gt; neuron-a via weight Wa0 = 0.6 The bias term helps ensure that the neuron is active, meaning its output value is not zero.\nThe input parameters are treated as constant values, while the weight values are variables that will be adjusted during the Backward pass if the training result does not meet expectations. The initial weight values are our best guess for achieving the desired training outcome. The result of the weighted sum calculation is passed to the activation function, which provides the input for neuron-b in the output layer. We use the ReLU (Rectified Linear Unit) activation function in both layers due to its simplicity. There are other activation functions, such as hyperbolic tangent (tanh), sigmoid, and softmax, but those are outside the scope of this chapter.\nThe input values and weights for neuron-b are:\nNeuron-a activation function output f(af) \u0026gt; neuron-b via weight Wb1 Bias ba0 = 1.0 \u0026gt; neuron-b via weight Wa0 = 0.5 The output, Ŷ, from neuron-b represents our feedforward neural network\u0026rsquo;s prediction. This value is used along with the expected result, y, as input for the error function. In this example, we use the Mean Squared Error (MSE) error function. As we will see, the result of the first training iteration does not match our expected value, leading us to initiate the Backward pass process.\nIn the first step of the Backward pass, the Backpropagation algorithm calculates the derivative of the error function (MSE’). Neurons-a and b use this result as input to compute their respective error terms by multiplying MSE’ with the result of the activation function and the weight value associated with the connection to the next neuron. Note that for neuron-b, there is no next layer—just the error function—so the weight parameter is excluded from the error term calculation of neuron-b. Next, the error term value is multiplied by an input value and learning rate, and this adjustment value is added to the current weight.\nAfter completing the Backward pass, the Backpropagation algorithm starts a new iteration of the Forward pass, gradually improving the model\u0026rsquo;s prediction until it closely matches the expected value, at which point the training is complete.\nFigure 2-1: Backpropagation Algorithm.\nIntroduction # This chapter introduces the training model of a neural network based on the Backpropagation algorithm. The goal is to provide a clear and solid understanding of the process without delving deeply into the mathematical formulas, while still explaining the fundamental operations of the involved functions. The chapter also briefly explains why, and in which phases the training job generates traffic to the network, and why lossless packet transport is required. The Backpropagation algorithm is composed of two phases: the Forward pass (computation phase) and the Backward pass (adjustment and communication phase).\nIn the Forward pass, neurons in the first hidden layer calculate the weighted sum of input parameters received from the input layer, which is then passed to the neuron\u0026rsquo;s activation function. Note that neurons in the input layer are not computational units; they simply pass the input variables to the connected neurons in the first hidden layer. The output from the activation function of a neuron is then used as input for the connected neurons in the next layer. The result of the activation function in the output layer represents the model\u0026rsquo;s prediction, which is compared to the expected value (ground truth) using the error function. The output of the error function indicates the accuracy of the current training iteration. If the result is sufficiently close to the expected value (error function close to zero), the training is complete. Otherwise, it triggers the Backward pass process.\nAs the first step in the backward pass, the backpropagation algorithm calculates the derivative of the error function, providing the output error (gradient) of the model. Next, the algorithm computes the error term (gradient) for the neuron(s) in the output layer by multiplying the derivative of each neuron’s activation function by the model\u0026rsquo;s error term. Then, the algorithm moves to the preceding layer and calculates the error term (gradient) for its neuron(s). This error term is now calculated using the error term of the connected neuron(s) in the next layer, the derivative of each neuron’s activation function, and the value of the weight parameter associated with the connection to the next layer.\nAfter calculating the error terms, the algorithm determines the weight adjustment values for all neurons simultaneously. This computation is based on the input values, the adjustment values, and a user-defined learning rate. Finally, the backpropagation algorithm refines all weight values by adding the adjustment values to the initial weights. Once the backward pass is complete, the backpropagation algorithm starts a new iteration of the forward pass, gradually improving the model\u0026rsquo;s predictions until they closely match the expected values, at which point the training is complete.\nFigure 2-1: Backpropagation Overview.\nThe First Iteration - Forward Pass # Training a model often requires multiple iterations of forward and backward passes. In the forward pass, neurons in the first hidden layer calculate the weighted sum of input values, each multiplied by its associated weight parameter. These neurons then apply an activation function to the result. Neurons in subsequent layers use the activation output from previous layers as input for their own weighted sum calculations. This process continues through all the layers until reaching the output layer, where the activation function produces the model\u0026rsquo;s prediction.\nAfter the forward pass, the backpropagation algorithm calculates the error by comparing the model\u0026rsquo;s output with the expected value, providing a measure of accuracy. If the model\u0026rsquo;s output is close to the expected value, training is complete. Otherwise, the backpropagation algorithm initiates the backward pass to adjust the weights and reduce the error in subsequent iterations.\nNeuron-a Forward Pass Calculations # Weighted Sum # In Figure 2-2, we have an imaginary training dataset with three inputs and a bias term. Input values and their respective initial weight values are listed below: x1 = 0.2 , initial weight wa1 = 0.1 x2 = 0.1, initial weight wa2 = 0.2 x3 = 0.4 , initial weight wa3 = 0.3 ba0 = 1.0 , initial weight wa0 = 0.6 From the model training perspective, the input values are constant, unchageable values, while weight values are variables which will be refined during the backward pass process.\nThe standard way to write the weighted sum formula is: Where:\nn = 3 represents the number of input values (x1, x2, x3). Each input xi is multiplied by its respective weight wi, and the sum of these products is added to the bias term b. In this case, the equation can be explicitly stated as:\nWhich with our parameters gives:\nActivation Function # Neuron-a uses the previously calculated weighted sum as input for the activation function. We are using the ReLU function (Rectified Linear Unit), which is more popular than the hyperbolic tangent and sigmoid functions due to its simplicity and lower computational cost.\nThe standard way to write the ReLU function is:\nWhere:\nf(a) represents the activation function. z is the weighted sum of inputs.\nThe ReLU function returns the z if z \u0026gt; 0. Otherwise, it returns 0 if z ≤ 0.\nIn our example, the weighted sum za is 0.76, so the ReLU function returns:\nFigure 2-2: Activation Function for Neuron-a.\nNeuron-b Forward Pass Calculations # Weighted Sum # Besides the bias term value of 1.0, Neuron-b uses the result provided by the activation function of neuron-a as an input to weighted sum calculation. Input values and their respective initial weight values are listed below: This gives us:\nActivation Function # Just like Neuron-a, Neuron-b uses the previously calculated weighted sum as input for the activation function. Because the zb = 0.804 is greater than zero, the ReLU activation function f(b) returns:\nNeuron-b is in the output layer, so its activation function result y_b_ represents the prediction of the model. Figure 2-3: Activation Function for Neuron-b.\nError Function # To keep things simple, we have used only one training example. However, in real-life scenarios, there will always be more than one training example. For instance, a training dataset might contain 10 images of cats and 10 images of dogs, each having 28x28 pixels. Each image represents a training example, giving us a total of 20 training examples. The purpose of the error function is to provide a single error metric for all training examples. In this case, we are using the Mean Squared Error (MSE).\nWe can calculate the MSE using the formula below where the expected value y is 1.0 and the model’s prediction for the training example yb = 0.804. This gives an error metric of 0.019, which can be interpreted as an indicator of the model\u0026rsquo;s accuracy.\nThe result of the error function is not sufficiently close to the desired value, which is why this result triggers the backward pass process.\nFigure 2-4: Calculating the Error Function for Training Examples.\nBackward Pass # In the forward pass, we calculate the model’s accuracy using several functions. First, Neuron-a computes the weighted sum Σ(za ) by multiplying the input values and the bias term with their respective weights. The output, za, is then passed through the activation function f(a), producing ya. Neuron-b, in turn, takes ya and the bias term to calculate the weighted sum Σ(zb ). The activation function f(b) then uses zb to compute the model’s output, yb. Finally, the error function f(e) calculates the model’s accuracy based on the output.\nSo, dependencies between function can be seen as:\nThe backpropagation algorithm combines these five functions to create a new error function, enew(x), using function composition and the chain rule. The following expression shows how the error function relates to the weight parameter w1 used by Neuron-a:\nThis can be expressed using the composition operator (∘) between functions:\nNext, we use a method called gradient descent to gradually adjust the initial weight values, refining them to bring the model\u0026rsquo;s output closer to the expected result. To do this, we compute the derivative of the composite function using the chain rule, where we take the derivatives of:\nThe error function (e) with respect to the activation function (b). The activation function b with respect to the weighted sum (zb). The weighted sum (zb) with respect to the activation function (a). The activation function (a) with respect to weighted sum (za(w1)). In Leibniz’s notation, this looks like:\nFigure 2-5 illustrates the components of the backpropagation algorithm, along with their relationships and dependencies.\nFigure 2-5: The Backward Pass Overview.\nPartial Derivative for Error Function – Output Error (Gradient) # As a recap, and for illustrating that the prediction of the first iteration fails, Figure 2-6 includes the computation for the error function (MSE = 0.019). As a first step, we calculate the partial derivative of the error function. In this case, the partial derivative describes the rate of change of the error function when the input variable yb changes. The derivative is called partial when one of its input values is held constant (i.e., not adjusted by the algorithm). In our example, the expected value y is constant input. The result of the partial derivative of the error function describes how the predicted output should change yb to minimize the model’s error.\nWe use the following formula for computing the derivative of the error function:\nFigure 2-6: The Backward Pass – Derivative of the Error Function.\nThe following explanation is meant for readers interested in why there is a minus sign in front of the function.\nWhen calculating the derivative, we use the Power Rule. The Power Rule states that if we have a function f(x) = xn , then its derivative is f’(x) = n ⋅ xn-1. In our case, this applies to the error function:\nUsing the Power Rule, the derivative becomes:\nNext, we apply the chain rule by multiplying this result by the derivative of the inner function (y − yb), with respect to yb . Since y is treated as a constant (because it represents our target value, which doesn\u0026rsquo;t change during optimization), the derivative of (y − yb) with respect to yb is simply −1, as the derivative of − yb with respect to yb is −1, and the derivative of y (a constant) is 0.\nTherefore, the final derivative of the error function with respect to yb is:\nPartial Derivative for the Activation Function # After computing the output error, we calculate the derivative of the activation function f(b) with respect to zb . Neuron b uses the ReLU activation function, which states that if the input to the function is greater than 0, the derivative is 1; otherwise, it is 0. In our case, the result of the activation function f(b)=0.804, so the derivative is 1.\nError Term for Neurons (Gradient) # The error term (Gradient) for neuron-b is calculated by multiplying the output error, the partial derivative of the error function, by the derivative of the neuron\u0026rsquo;s activation function. This means that now we propagate the model\u0026rsquo;s error backward using it as a base value for finetuning the model accuracy (i.e., refining new weight values). This is why the term backward pass fits perfectly for the process.\nFigure 2-7: The Backward Pass – Error Term (Gradient) for Neuron-b.\nAfter computing the error term for Neuron-b, the backward pass moves to the preceding layer, the hidden layer, and calculates the error term for Neuron-a. The algorithm computes the derivative for the activation function f(a) = 1, as it did with the Neuron-b. Next, it multiplies the result by Neuron-b\u0026rsquo;s error term (-0.196) and the connected weight parameter , wb1 =0.4. The result -0.0784 is the error term for Neuron-a.\nFigure 2-8: The Backward Pass – Error Term (Gradient) for Neuron-a.\nWeight Adjustment Value # After computing error terms for all neurons in every layer, the algorithm simultaneously calculates the weight adjustment value for every weight. The process is simple, the error term is multiplied with an input value connected to weight and with learning rate (η). The learning rate balances convergence speed and training stability. We have set it to -0.6 for the first iteration. The learning rate is a hyper-parameter, meaning it is set by the user rather than learned by the model during training. It affects the behavior of the backpropagation algorithm by controlling the size of the weight updates. It is also possible to adjust the learning rate during training—using a higher learning rate at the start to allow faster convergence and lowering it later to avoid overshooting the optimal result. Weight adjustment value for weight wb1 and wa1 respectively:\nNote! It is not recommended to use a negative learning rate. I use it here because we get a good enough output for the second forward pass iteration.\nFigure 2-9: The Backward Pass – Weight Adjustment Value for Neurons.\nRefine Weights # As the last step, the backpropagation algorithm computes new values for every weight parameter in the model by simply summing the initial weight value and weight adjustment value.\nNew values for weight parameters wb1 and wa1 respectively:\nFigure 2-10: The Backward Pass – Compute New Weight Values.\nThe Second Iteration - Forward Pass # After updating all the weight values (wa0, wa1, wa2, and wa3 ), the backpropagation process begins the second iteration of the forward pass. As shown in Figure 2-11, the model output yb = 0.9982 is very close to the expected value y = 1.0. The new MSE = 0.0017, is much better than 0.019 computed in the first iteration.\nFigure 2-11: The Second Iteration of the Forward Pass.\nNetwork Impact # Figure 2-12 shows a hypothetical example of Data Parallelization, where our training data set is split into two batches, A and B, which are processed by GPU-A and GPU-B, respectively. The training model is the same on both GPUs: Fully-Connected, with two hidden layers of four neurons each, and one output neuron in the output layer.\nAfter computing a model prediction during the forward pass, the backpropagation algorithm begins the backward pass by calculating the gradient (output error) for the error function. Once computed, the gradients are synchronized between the GPUs. The algorithm then averages the gradients, and the process moves to the preceding layer. Neurons in the preceding layer calculate their gradient by multiplying the weighted sum of their connected neurons’ averaged gradients and connected weight with the local activation function’s partial derivative. These neuron-based gradients are then synchronized over connections. Before the process moves to the preceding layer, gradients are averaged. The backpropagation algorithm executes the same process through all layers. If packet loss occurs during the synchronization, it can ruin the entire training process, which would need to be restarted unless snapshots were taken. The cost of losing even a single packet could be enormous, especially if training has been ongoing for several days or weeks. Why is a single packet so important? If the synchronization between the gradients of two parallel neurons fails due to packet loss, the algorithm cannot compute the average, and the neurons in the preceding layer cannot calculate their gradient. Besides, if the connection, whether the synchronization happens over NVLink, InfiniBand, Ethernet (RoCE or RoCEv2), or wireless connection, causes a delay, the completeness of the training slows down. This causes GPU under-utilization which is not efficient from the business perspective.\nFigure 2-12: Backward Pass – Gradient Synchronization and Averaging.\nTo be conntinued\u0026hellip;\nReferences:\nhttps://nwktimes.blogspot.com/2024/09/ai4ne-ch1-artificial-neuron.html https://nwktimes.blogspot.com/2024/10/ai-for-network-engineers.html "},{"id":13,"href":"/tech-book/docs/systemdesign-tips/code-deployment-system/","title":"Design A Code-Deployment System","section":"SystemDesign-Tips","content":" 1. Gathering System Requirements # As with any systems design interview question, the first thing that we want to do is to gather system requirements; we need to figure out what system we\u0026rsquo;re building exactly.\nFrom the answers we were given to our clarifying questions (see Prompt Box), we\u0026rsquo;re building a system that involves repeatedly (in the order of thousands of times per day) building and deploying code to hundreds of thousands of machines spread out across 5-10 regions around the world.\nBuilding code will involve grabbing snapshots of source code using commit SHA identifiers; beyond that, we can assume that the actual implementation details of the building action are taken care of. In other words, we don\u0026rsquo;t need to worry about how we would build JavaScript code or C++ code; we just need to design the system that enables the repeated building of code.\nBuilding code will take up to 15 minutes, it\u0026rsquo;ll result in a binary file of up to 10GB, and we want to have the entire deployment process (building and deploying code to our target machines) take at most 30 minutes.\nEach build will need a clear end-state (SUCCESS or FAILURE), and though we care about availability (2 to 3 nines), we don\u0026rsquo;t need to optimize too much on this dimension.\n2. Coming Up With A Plan # It\u0026rsquo;s important to organize ourselves and to lay out a clear plan regarding how we\u0026rsquo;re going to tackle our design. What are the major, distinguishable components of our how system?\nIt seems like this system can actually very simply be divided into two clear subsystems:\nthe Build System that builds code into binaries the Deployment System that deploys binaries to our machines across the world Note that these subsystems will of course have many components themselves, but this is a very straightforward initial way to approach our problem.\n3. Build System \u0026ndash; General Overview # From a high-level perspective, we can call the process of building code into a binary a job, and we can design our build system as a queue of jobs. Jobs get added to the queue, and each job has a commit identifier (the commit SHA) for what version of the code it should build and the name of the artifact that will be created (the name of the resulting binary). Since we\u0026rsquo;re agnostic to the type of the code being built, we can assume that all languages are handled automatically here.\nWe can have a pool of servers (workers) that are going to handle all of these jobs. Each worker will repeatedly take jobs off the queue (in a FIFO manner—no prioritization for now), build the relevant binaries (again, we\u0026rsquo;re assuming that the actual implementation details of building code are given to us), and write the resulting binaries to blob storage (Google Cloud Storage or S3 for instance). Blob storage makes sense here, because binaries are literally blobs of data.\n4. Build System \u0026ndash; Job Queue # A naive design of the job queue would have us implement it in memory (just as we would implement a queue in coding interviews), but this implementation is very problematic; if there\u0026rsquo;s a failure in our servers that hold this queue, we lose the entire state of our jobs: queued jobs and past jobs.\nIt seems like we would be unnecessarily complicating matters by trying to optimize around this in-memory type of storage, so we\u0026rsquo;re likely better off implementing the queue using a SQL database.\n5. Build System \u0026ndash; SQL Job Queue # We can have a jobs table in our SQL database where every record in the database represents a job, and we can use record-creation timestamps as the queue\u0026rsquo;s ordering mechanism.\nOur table will be:\nid: string, the ID of the job, auto-generated created_at: timestamp commit_sha: string name: string, the pointer to the job\u0026rsquo;s eventual binary in blob storage status: string, QUEUED, RUNNING, SUCCEEDED, FAILED We can implement the actual dequeuing mechanism by looking at the oldest creation_timestamp with a QUEUED status. This means that we\u0026rsquo;ll likely want to index our table on both created_at and status.\n6. Build System \u0026ndash; Concurrency # ACID transactions will make it safe for potentially hundreds of workers to grab jobs off the queue without unintentionally running the same job twice (we\u0026rsquo;ll avoid race conditions). Our actual transaction will look like this:\nBEGIN TRANSACTION; SELECT * FROM jobs_table WHERE status = \u0026#39;QUEUED\u0026#39; ORDER BY created_at ASC LIMIT 1; // if there\u0026#39;s none, we ROLLBACK; UPDATE jobs_table SET status = \u0026#39;RUNNING\u0026#39; WHERE id = id from previous query; COMMIT; All of the workers will be running this transaction every so often to dequeue the next job; let\u0026rsquo;s say every 5 seconds. If we arbitrarily assume that we\u0026rsquo;ll have 100 workers sharing the same queue, we\u0026rsquo;ll have 100/5 = 20 reads per second, which is very easy to handle for a SQL database.\n7. Build System \u0026ndash; Lost Jobs # Since we\u0026rsquo;re designing a large-scale system, we have to expect and handle edge cases. Here, what if there\u0026rsquo;s a network partition with our workers or one of our workers dies mid-build? Since builds last around 15 minutes on average, this will very likely happen. In this case, we want to avoid having a \u0026ldquo;lost job\u0026rdquo; that we were never made aware of, and with our current design, the job will remain RUNNING forever. How do we handle this?\nWe could have an extra column on our jobs table called last_heartbeat. This will be updated in a heartbeat fashion by the worker running a particular job, where that worker will update the relevant row in the table every 3-5 minutes to just let us know that it\u0026rsquo;s still running the job.\nWe can then have a completely separate service that polls the table every so often (say, every 5 minutes, depending on how responsive we want this build system to be), checks all of the RUNNING jobs, and if their last_heartbeat was last modified longer than 2 heartbeats ago (we need some margin of error here), then something\u0026rsquo;s likely wrong, and this service can reset the status of the relevant jobs to QUEUED, which would effectively bring them back to the front of the queue.\nThe transaction that this auxiliary service will perform will look something like this:\nUPDATE jobs_table SET status = \u0026#39;QUEUED\u0026#39; WHERE status = \u0026#39;RUNNING\u0026#39; AND last_heartbeat \u0026lt; NOW() - 10 minutes; 8. Build System \u0026ndash; Scale Estimation # We previously arbitrarily assumed that we would have 100 workers, which made our SQL-database queue able to handle the expected load. We should try to estimate if this number of workers is actually realistic.\nWith some back-of-the-envelope math, we can see that, since a build can take up to 15 minutes, a single worker can run 4 jobs per hour, or ~100 (96) jobs per day. Given thousands of builds per day (say, 5000-10000), this means that we would need 50-100 workers (5000 / 100). So our arbitrary figure was accurate.\nEven if the builds aren\u0026rsquo;t uniformly spread out (in other words, they peak during work hours), our system scales horizontally very easily. We can automatically add or remove workers whenever the load warrants it. We can also scale our system vertically by making our workers more powerful, thereby reducing the build time.\n9. Build System \u0026ndash; Storage # We previously mentioned that we would store binaries in blob storage (GCS). Where does this storage fit into our queueing system exactly?\nWhen a worker completes a build, it can store the binary in GCS before updating the relevant row in the jobs table. This will ensure that a binary has been persisted before its relevant job is marked as SUCCEEDED.\nSince we\u0026rsquo;re going to be deploying our binaries to machines spread across the world, it\u0026rsquo;ll likely make sense to have regional storage rather than just a single global blob store.\nWe can design our system based on regional clusters around the world (in our 5-10 global regions). Each region can have a blob store (a regional GCS bucket). Once a worker successfully stores a binary in our main blob store, the worker is released and can run another job, while the main blob store performs some asynchronous replication to store the binary in all of the regional GCS buckets. Given 5-10 regions and 10GB files, this step should take no more than 5-10 minutes, bringing our total build-and-deploy duration so far to roughly 20-25 minutes (15 minutes for a build and 5-10 minutes for global replication of the binary).\n10. Deployment System \u0026ndash; General Overview # From a high-level perspective, our actual deployment system will need to allow for the very fast distribution of 10GB binaries to hundreds of thousands of machines across all of our global regions. We\u0026rsquo;re likely going to want some service that tells us when a binary has been replicated in all regions, another service that can serve as the source of truth for what binary should currently be run on all machines, and finally a peer-to-peer-network design for our actual machines across the world.\n11. Deployment System \u0026ndash; Replication-Status Service # We can have a global service that continuously checks all regional GCS buckets and aggregates the replication status for successful builds (in other words, checks that a given binary in the main blob store has been replicated across all regions). Once a binary has been replicated across all regions, this service updates a separate SQL database with rows containing the name of a binary and a replication_status. Once a binary has a \u0026ldquo;complete\u0026rdquo; replication_status, it\u0026rsquo;s officially deployable.\n12. Deployment System \u0026ndash; Blob Distribution # Since we\u0026rsquo;re going to deploy 10 GBs to hundreds of thousands of machines, even with our regional clusters, having each machine download a 10GB file one after the other from a regional blob store is going to be extremely slow. A peer-to-peer-network approach will be much faster and will allow us to hit our 30-minute time frame for deployments. All of our regional clusters will behave as peer-to-peer networks.\n13. Deployment System \u0026ndash; Trigger # Let\u0026rsquo;s describe what happens when an engineer presses a button on some internal UI that says \u0026ldquo;Deploy build/binary B1 to every machine globally\u0026rdquo;. This is the action that triggers the binary downloads on all the regional peer-to-peer networks.\nTo simplify this process and to support having multiple builds getting deployed concurrently, we can design this in a goal-state oriented manner.\nThe goal-state will be the desired build version at any point in time and will look something like: \u0026ldquo;current_build: B1\u0026rdquo;, and this can be stored in some dynamic configuration service (a key-value store like Etcd or ZooKeeper). We\u0026rsquo;ll have a global goal-state as well as regional goal-states.\nEach regional cluster will have a K-V store that holds configuration for that cluster about what builds should be running on that cluster, and we\u0026rsquo;ll also have a global K-V store.\nWhen an engineer clicks the \u0026ldquo;Deploy build/binary B1\u0026rdquo; button, our global K-V store\u0026rsquo;s build_version will get updated. Regional K-V stores will be continuously polling the global K-V store (say, every 10 seconds) for updates to the build_version and will update themselves accordingly.\nMachines in the clusters/regions will be polling the relevant regional K-V store, and when the build_version changes, they\u0026rsquo;ll try to fetch that build from the P2P network and run the binary.\n14. System Diagram # Final Systems Architecture\n"},{"id":14,"href":"/tech-book/docs/systemdesign-tips/stock-broker/","title":"Design A Stock-Broker System","section":"SystemDesign-Tips","content":" 1. Gathering System Requirements # As with any systems design interview question, the first thing that we want to do is to gather system requirements; we need to figure out what system we\u0026rsquo;re building exactly.\nWe\u0026rsquo;re building a stock-brokerage platform like Robinhood that functions as the intermediary between end-customers and some central stock exchange. The idea is that the central stock exchange is the platform that actually executes stock trades, whereas the stockbroker is just the platform that customers talk to when they want to place a trade\u0026ndash;the stock brokerage is \u0026ldquo;simpler\u0026rdquo; and more \u0026ldquo;human-readable\u0026rdquo;, so to speak.\nWe only care about supporting market trades\u0026ndash;trades that are executed at the current stock price\u0026ndash;and we can assume that our system stores customer balances (i.e., funds that customers may have previously deposited) in a SQL table.\nWe need to design a PlaceTrade API call, and we know that the central exchange\u0026rsquo;s equivalent API method will take in a callback that\u0026rsquo;s guaranteed to be executed upon completion of a call to that API method.\nWe\u0026rsquo;re designing this system to support millions of trades per day coming from millions of customers in a single region (the U.S., for example). We want the system to be highly available.\n2. Coming Up With A Plan # It\u0026rsquo;s important to organize ourselves and to lay out a clear plan regarding how we\u0026rsquo;re going to tackle our design. What are the major, distinguishable components of our how system?\nWe\u0026rsquo;ll approach the design front to back:\nthe PlaceTrade API call that clients will make the API server(s) handling client API calls the system in charge of executing orders for each customer We\u0026rsquo;ll need to make sure that the following hold:\ntrades can never be stuck forever without either succeeding or failing to be executed a single customer\u0026rsquo;s trades have to be executed in the order in which they were placed balances can never go in the negatives 3. API Call # The core API call that we have to implement is PlaceTrade.\nWe\u0026rsquo;ll define its signature as:\nPlaceTrade( customerId: string, stockTicker: string, type: string (BUY/SELL), quantity: integer, ) =\u0026gt; ( tradeId: string, stockTicker: string, type: string (BUY/SELL), quantity: integer, createdAt: timestamp, status: string (PLACED), reason: string, ) The customer ID can be derived from an authentication token that\u0026rsquo;s only known to the user and that\u0026rsquo;s passed into the API call.\nThe status can be one of:\nPLACED IN PROGRESS FILLED REJECTED That being said, PLACED will actually be the defacto status here, because the other statuses will be asynchronously set once the exchange executes our callback. In other words, the trade status will always be PLACED when the PlaceTrade API call returns, but we can imagine that a GetTrade API call could return statuses other than PLACED.\nPotential reasons for a REJECTED trade might be:\ninsufficient funds\nrandom error\npast market hours\n4. API Server(s) # We\u0026rsquo;ll need multiple API servers to handle all of the incoming requests. Since we don\u0026rsquo;t need any caching when making trades, we don\u0026rsquo;t need any server stickiness, and we can just use some round-robin load balancing to distribute incoming requests between our API servers.\nOnce API servers receive a PlaceTrade call, they\u0026rsquo;ll store the trade in a SQL table. This table needs to be in the same SQL database as the one that the balances table is in, because we\u0026rsquo;ll need to use ACID transactions to alter both tables in an atomic way.\nThe SQL table for trades will look like this:\nid: string, a random, auto-generated string customer_id: string, the id of the customer making the trade stockTicker: string, the ticker symbol of the stock being traded type: string, either BUY or SELL quantity: integer (no fractional shares), the number of shares to trade status: string, the status of the trade; starts as PLACED created_at: timestamp, the time when the trade was created reason: string, the human-readable justification of the trade\u0026rsquo;s status The SQL table for balances will look like this:\nid: string, a random, auto-generated string customer_id: string, the id of the customer related to the balance amount: float, the amount of money that the customer has in USD last_modified: timestamp, the time when the balance was last modified 5. Trade-Execution Queue # With hundreds of orders placed every second, the trades table will be pretty massive. We\u0026rsquo;ll need to figure out a robust way to actually execute our trades and to update our table, all the while making sure of a couple of things:\nWe want to make sure that for a single customer, we only process a single BUY trade at any time, because we need to prevent the customer\u0026rsquo;s balance from ever reaching negative values. Given the nature of market orders, we never know the exact dollar value that a trade will get executed at in the exchange until we get a response from the exchange, so we have to speak to the exchange in order to know whether the trade can go through. We can design this part of our system with a Publish/Subscribe pattern. The idea is to use a message queue like Apache Kafka or Google Cloud Pub/Sub and to have a set of topics that customer ids map to. This gives us at-least-once delivery semantics to make sure that we don\u0026rsquo;t miss new trades. When a customer makes a trade, the API server writes a row to the database and also creates a message that gets routed to a topic for that customer (using hashing), notifying the topic\u0026rsquo;s subscriber that there\u0026rsquo;s a new trade.\nThis gives us a guarantee that for a single customer, we only have a single thread trying to execute their trades at any time.\nSubscribers of topics can be rings of 3 workers (clusters of servers, essentially) that use leader election to have 1 master worker do the work for the cluster (this is for our system\u0026rsquo;s high availability)\u0026ndash;the leader grabs messages as they get pushed to the topic and executes the trades for the customers contained in the messages by calling the exchange. As mentioned above, a single customer\u0026rsquo;s trades are only ever handled by the same cluster of workers, which makes our logic and our SQL queries cleaner.\nAs far as how many topics and clusters of workers we\u0026rsquo;ll need, we can do some rough estimation. If we plan to execute millions of trades per day, that comes down to about 10-100 trades per second given open trading hours during a third of a day and non-uniform trading patterns. If we assume that the core execution logic lasts about a second, then we should have roughly 10-100 topics and clusters of workers to process trades in parallel.\n~100,000 seconds per day (3600 * 24) ~1,000,000 trades per day trades bunched in 1/3rd of the day --\u0026gt; (1,000,000 / 100,000) * 3 = ~30 trades per second 6. Trade-Execution Logic # The subscribers (our workers) are streaming / waiting for messages. Imagine the following message were to arrive in the topic queue:\n{\u0026#34;customerId\u0026#34;: \u0026#34;c1\u0026#34;} The following would be pseudo-code for the worker logic:\n// We get the oldest trade that isn\u0026#39;t in a terminal state. trade = SELECT * FROM trades WHERE customer_id = \u0026#39;c1\u0026#39; AND (status = \u0026#39;PLACED\u0026#39; OR status = \u0026#39;IN PROGRESS\u0026#39;) ORDER BY created_at ASC LIMIT 1; // If the trade is PLACED, we know that it\u0026#39;s effectively // ready to be executed. We set it as IN PROGRESS. if trade.status == \u0026#34;PLACED\u0026#34; { UPDATE trades SET status = \u0026#39;IN PROGRESS\u0026#39; WHERE id = trade.id; } // In the event that the trade somehow already exists in the // exchange, the callback will do the work for us. if exchange.TradeExists(trade.id) { return; } // We get the balance for the customer. balance = SELECT amount FROM balances WHERE customer_id = \u0026#39;c1\u0026#39;; // This is the callback that the exchange will execute once // the trade actually completes. We\u0026#39;ll define it further down // in the walkthrough. callback = ... exchange.Execute( trade.stockTicker, trade.type, trade.quantity, max_price = balance, callback, ) 7. Exchange Callback # Below is some pseudo code for the exchange callback:\nfunction exchange_callback(exchange_trade) { if exchange_trade.status == \u0026#39;FILLED\u0026#39; { BEGIN TRANSACTION; trade = SELECT * FROM trades WHERE id = database_trade.id; if trade.status \u0026lt;\u0026gt; \u0026#39;IN PROGRESS\u0026#39; { ROLLBACK; pubsub.send({customer_id: database_trade.customer_id}); return; } UPDATE balances SET amount -= exchange_trade.amount WHERE customer_id = database_trade.customer_id; UPDATE trades SET status = \u0026#39;FILLED\u0026#39; WHERE id = database_trade.id; COMMIT; } else if exchange_trade.status == \u0026#39;REJECTED\u0026#39; { BEGIN TRANSACTION; UPDATE trades SET status = \u0026#39;REJECTED\u0026#39; WHERE id = database_trade.id; UPDATE trades SET reason = exchange_trade.reason WHERE id = database_trade.id; COMMIT; } pubsub.send({customer_id: database_trade.customer_id}); return http.status(200); } 8. System Diagram # Final Systems Architecture\n"},{"id":15,"href":"/tech-book/docs/systemdesign-tips/design-amazon/","title":"Design Amazon","section":"SystemDesign-Tips","content":" 1. Gathering System Requirements # As with any systems design interview question, the first thing that we want to do is to gather system requirements; we need to figure out what system we\u0026rsquo;re building exactly.\nWe\u0026rsquo;re designing the e-commerce side of the Amazon website, and more specifically, the system that supports users searching for items on the Amazon home page, adding items to cart, submitting orders, and those orders being assigned to relevant Amazon warehouses for shipment.\nWe need to handle items going out of stock, and we\u0026rsquo;ve been given some guidelines for a simple \u0026ldquo;stock-reservation\u0026rdquo; system when users begin the checkout process.\nWe have access to two smart services: one that handles user search queries and one that handles warehouse order assignment. It\u0026rsquo;s our job to figure out how these services fit into our larger design.\nWe\u0026rsquo;ll specifically be designing the system that supports amazon.com (i.e., Amazon\u0026rsquo;s U.S. operations), and we\u0026rsquo;ll assume that this system can be replicated for other regional Amazon stores. For the rest of this walkthrough, whenever we refer to \u0026ldquo;Amazon,\u0026rdquo; we\u0026rsquo;ll be referring specifically to Amazon\u0026rsquo;s U.S. store.\nWhile the system should have low latency when searching for items and high availability in general, serving roughly 10 orders per second in the U.S., we\u0026rsquo;ve been told to focus mostly on core functionality.\n2. Coming Up With A Plan # We\u0026rsquo;ll tackle this system by first looking at a high-level overview of how it\u0026rsquo;ll be set up, then diving into its storage components, and finally looking at how the core functionality comes to life. We can divide the core functionality into two main sections:\nThe user side. The warehouse side. We can further divide the user side as follows:\nBrowsing items given a search term. Modifying the cart. Beginning the checkout process. Submitting and canceliing orders. 3. High-Level System Overview # Within a region, user and warehouse requests will get round-robin-load-balanced to respective sets of API servers, and data will be written to and read from a SQL database for that region.\nWe\u0026rsquo;ll go with a SQL database because all of the data that we\u0026rsquo;ll be dealing with (items, carts, orders, etc.) is, by nature, structured and lends itself well to a relational model.\n4. SQL Tables # We\u0026rsquo;ll have six SQL tables to support our entire system\u0026rsquo;s storage needs.\nItems\nThis table will store all of the items on Amazon, with each row representing an item.\nitemId: uuid name: string description: string price: integer currency: enum other\u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; Carts\nThis table will store all of the carts on Amazon, with each row representing a cart. We\u0026rsquo;ve been told that each user can only have a single cart at once.\ncartId: uuid customerId: uuid items: []{itemId, quantity} \u0026hellip; \u0026hellip; \u0026hellip; Orders\nThis table will store all of the orders on Amazon, with each row representing an order.\n| orderId: uuid |\tcustomerId: uuid\t| orderStatus: enum |\titems: []{itemId, quantity} |\tprice: integer |\tpaymentInfo: PaymentInfo\t| shippingAddress: string\t| timestamp: datetime\t| other\u0026hellip; | | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; | | \u0026hellip; |\t\u0026hellip; |\t\u0026hellip; |\t\u0026hellip; |\t\u0026hellip; |\t\u0026hellip; |\t\u0026hellip; |\t\u0026hellip; |\t\u0026hellip;|\nAggregated Stock\nThis table will store all of the item stocks on Amazon that are relevant to users, with each row representing an item. See the Core User Functionality section for more details.\nitemId: uuid stock: integer \u0026hellip; \u0026hellip; Warehouse Orders\nThis table will store all of the orders that Amazon warehouses get, with each row representing a warehouse order. Warehouse orders are either entire normal Amazon orders or subsets of normal Amazon orders.\nwarehouseOrderId: uuid parentOrderId: uuid warehouseId: uuid orderStatus: enum items: []{itemId, quantity} shippingAddress: string \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; Warehouse Stock\nThis table will store all of the item stocks in Amazon warehouses, with each row representing an {item, warehouse} pairing. The physicalStock field represents an item\u0026rsquo;s actual physical stock in the warehouse in question, serving as a source of truth, while the availableStock field represents an item\u0026rsquo;s effective available stock in the relevant warehouse; this stock gets decreased when orders are assigned to warehouses. See the Core Warehouse Functionality section for more details.\nitemId: uuid warehouseId: uuid physicalStock: integer availableStock: integer \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; 5. Core User Functionality # GetItemCatalog(search)\nThis is the endpoint that users call when they\u0026rsquo;re searching for items. The request is routed by API servers to the smart search-results service, which interacts directly with the items table, caches popular item searches, and returns the results.\nThe API servers also fetch the relevant item stocks from the aggregated_stock table.\nUpdateCartItemQuantity(itemId, quantity)\nThis is the endpoint that users call when they\u0026rsquo;re adding or removing items from their cart. The request writes directly to the carts table, and users can only call this endpoint when an item has enough stock in the aggregated_stock table.\nBeginCheckout() \u0026amp; CancelCheckout()\nThese are the endpoints that users call when they\u0026rsquo;re beginning the checkout process and cancelling it. The BeginCheckout request triggers another read of the aggregated_stock table for the relevant items. If some of the items in the cart don\u0026rsquo;t have enough stock anymore, the UI alerts the users accordingly. For items that do have enough stock, the API servers write to the aggregated_stock table and decrease the relevant stocks accordingly, effectively \u0026ldquo;reserving\u0026rdquo; the items during the duration of the checkout. The CancelCheckout request, which also gets automatically called after 10 minutes of being in the checkout process, writes to the aggregated_stock table and increases the relevant stocks accordingly, thereby \u0026ldquo;unreserving\u0026rdquo; the items. Note that all of the writes to the aggregated_stock are ACID transactions, which allows us to comfortably rely on this SQL table as far as stock correctness is concerned.\nSubmitOrder(), CancelOrder(), \u0026amp; GetMyOrders()\nThese are the endpoints that users call when they\u0026rsquo;re submitting and cancelling orders. Both the SubmitOrder and CancelOrder requests write to the orders table, and CancelOrder also writes to the aggregated_stock table, increasing the relevant stocks accordingly (SubmitOrder doesn\u0026rsquo;t need to because the checkout process already has). GetMyOrders simply reads from the orders table. Note that an order can only be cancelled if it hasn\u0026rsquo;t yet been shipped, which is knowable from the orderStatus field.\n6. Core Warehouse Functionality # On the warehouse side of things, we\u0026rsquo;ll have the smart order-assignment service read from the orders table, figure out the best way to split orders up and assign them to warehouses based on shipping addresses, item stocks, and other data points, and write the final warehouse orders to the warehouse_orders table.\nIn order to know which warehouses have what items and how many, the order-assignment service will rely on the availableStock of relevant items in the warehouse_stock table. When the service assigns an order to a warehouse, it decreases the availableStock of the relevant items for the warehouse in question in the warehouse_stock table. These availableStock values are re-increased by the relevant warehouse if its order ends up being cancelled.\nWhen warehouses get new item stock, lose item stock for whatever reason, or physically ship their assigned orders, they\u0026rsquo;ll update the relevant physicalStock values in the warehouse_stock table. If they get new item stock or lose item stock, they\u0026rsquo;ll also write to the aggregated_stock table (they don\u0026rsquo;t need to do this when shipping assigned orders, since the aggregated_stock table already gets updated by the checkout process on the user side of things).\n7. System Diagram # Final Systems Architecture\n"},{"id":16,"href":"/tech-book/docs/systemdesign-tips/design-slack/","title":"Design Slack","section":"SystemDesign-Tips","content":" 1. Gathering System Requirements # As with any systems design interview question, the first thing that we want to do is to gather system requirements; we need to figure out what system we\u0026rsquo;re building exactly.\nWe\u0026rsquo;re designing the core communication system behind Slack, which allows users to send instant messages in Slack channels.\nSpecifically, we\u0026rsquo;ll want to support:\nLoading the most recent messages in a Slack channel when a user clicks on the channel. Immediately seeing which channels have unread messages for a particular user when that user loads Slack. Immediately seeing which channels have unread mentions of a particular user, for that particular user, when that user loads Slack, and more specifically, the number of these unread mentions in each relevant channel. Sending and receiving Slack messages instantly, in real time. Cross-device synchronization: if a user has both the Slack desktop app and the Slack mobile app open, with an unread channel in both, and if they read this channel on one device, the second device should immediately be updated and no longer display the channel as unread. The system should have low latencies and high availability, catering to a single region of roughly 20 million users. The largest Slack organizations will have as many as 50,000 users, with channels of the same size within them.\nThat being said, for the purpose of this design, we should primarily focus on latency and core functionality; availability and regionality can be disregarded, within reason.\n2. Coming Up With A Plan # We\u0026rsquo;ll tackle this system by dividing it into two main sections:\nHandling what happens when a Slack app loads. Handling real-time messaging as well as cross-device synchronization. We can further divide the first section as follows:\nSeeing all of the channels that a user is a part of. Seeing messages in a particular channel. Seeing which channels have unread messages. Seeing which channels have unread mentions and how many they have. 3. Persistent Storage Solution \u0026amp; App Load # While a large component of our design involves real-time communication, another large part of it involves retrieving data (channels, messages, etc.) at any given time when the Slack app loads. To support this, we\u0026rsquo;ll need a persistent storage solution.\nSpecifically, we\u0026rsquo;ll opt for a SQL database since we can expect this data to be structured and to be queried frequently.\nWe can start with a simple table that\u0026rsquo;ll store every Slack channel.\nChannels\nid (channelId): uuid orgId: uuid name: string description: string \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; Then, we can have another simple table representing channel-member pairs: each row in this table will correspond to a particular user who is in a particular channel. We\u0026rsquo;ll use this table, along with the one above, to fetch a user\u0026rsquo;s relevant when the app loads.\nChannel Members\nid: uuid orgId: uuid channelId: uuid userId: uuid \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; We\u0026rsquo;ll naturally need a table to store all historical messages sent on Slack. This will be our largest table, and it\u0026rsquo;ll be queried every time a user fetches messages in a particular channel. The API endpoint that\u0026rsquo;ll interact with this table will return a paginated response, since we\u0026rsquo;ll typically only want the 50 or 100 most recent messages per channel.\nAlso, this table will only be queried when a user clicks on a channel; we don\u0026rsquo;t want to fetch messages for all of a user\u0026rsquo;s channels on app load, since users will likely never look at most of their channels.\nHistorical Messages\nid: uuid orgId: uuid channelId: uuid senderId: uuid sentAt: timestamp body: string mentions: List \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; In order not to fetch recent messages for every channel on app load, all the while supporting the feature of showing which channels have unread messages, we\u0026rsquo;ll need to store two extra tables: one for the latest activity in each channel (this table will be updated whenever a user sends a message in a channel), and one for the last time a particular user has read a channel (this table will be updated whenever a user opens a channel).\nLatest Channel Timestamps\nid: uuid orgId: uuid channelId: uuid lastActive: timestamp \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; Channel Read Receipts\nid: uuid orgId: uuid channelId: uuid userId: uuid lastSeen: timestamp \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; For the number of unread user mentions that we want to display next to channel names, we\u0026rsquo;ll have another table similar to the read-receipts one, except this one will have a count of unread user mentions instead of a timestamp. This count will be updated (incremented) whenever a user tags another user in a channel message, and it\u0026rsquo;ll also be updated (reset to 0) whenever a user opens a channel with unread mentions of themself.\nUnread Channel-User-Mention Counts\nid: uuid orgId: uuid channelId: uuid userId: uuid count: int \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; 4. Load Balancing # For all of the API calls that clients will issue on app load, including writes to our database (when sending a message or marking a channel as read), we\u0026rsquo;re going to want to load balance.\nWe can have a simple round-robin load balancer, forwarding requests to a set of server clusters that will then handle passing requests to our database.\n5. \u0026ldquo;Smart\u0026rdquo; Sharding # Since our tables will be very large, especially the messages table, we\u0026rsquo;ll need to have some sharding in place.\nThe natural approach is to shard based on organization size: we can have the biggest organizations (with the biggest channels) in their individual shards, and we can have smaller organizations grouped together in other shards.\nAn important point to note here is that, over time, organization sizes and Slack activity within organizations will change. Some organizations might double in size overnight, others might experience seemingly random surges of activity, etc.. This means that, despite our relatively sound sharding strategy, we might still run into hot spots, which is very bad considering the fact that we care about latency so much.\nTo handle this, we can add a \u0026ldquo;smart\u0026rdquo; sharding solution: a subsystem of our system that\u0026rsquo;ll asynchronously measure organization activity and \u0026ldquo;rebalance\u0026rdquo; shards accordingly. This service can be a strongly consistent key-value store like Etcd or ZooKeeper, mapping orgIds to shards. Our API servers will communicate with this service to know which shard to route requests to.\n6. Pub/Sub System for Real-Time Behavior # There are two types of real-time behavior that we want to support:\nSending and receiving messages in real time. Cross-device synchronization (instantly marking a channel as read if you have Slack open on two devices and read the channel on one of them). For both of these functionalities, we can rely on a Pub/Sub messaging system, which itself will rely on our previously described \u0026ldquo;smart\u0026rdquo; sharding strategy.\nEvery Slack organization or group of organizations will be assigned to a Kafka topic, and whenever a user sends a message in a channel or marks a channel as read, our previously mentioned API servers, which handle speaking to our database, will also send a Pub/Sub message to the appropriate Kafka topic.\nThe Pub/Sub messages will look like:\n{ \u0026#34;type\u0026#34;: \u0026#34;chat\u0026#34;, \u0026#34;orgId\u0026#34;: \u0026#34;AAA\u0026#34;, \u0026#34;channelId\u0026#34;: \u0026#34;BBB\u0026#34;, \u0026#34;userId\u0026#34;: \u0026#34;CCC\u0026#34;, \u0026#34;messageId\u0026#34;: \u0026#34;DDD\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2020-08-31T01:17:02\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;this is a message\u0026#34;, \u0026#34;mentions\u0026#34;: [\u0026#34;CCC\u0026#34;, \u0026#34;EEE\u0026#34;] }, { \u0026#34;type\u0026#34;: \u0026#34;read-receipt\u0026#34;, \u0026#34;orgId\u0026#34;: \u0026#34;AAA\u0026#34;, \u0026#34;channelId\u0026#34;: \u0026#34;BBB\u0026#34;, \u0026#34;userId\u0026#34;: \u0026#34;CCC\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2020-08-31T01:17:02\u0026#34; } We\u0026rsquo;ll then have a different set of API servers who subscribe to the various Kakfa topics (probably one API server cluster per topic), and our clients (Slack users) will establish long-lived TCP connections with these API server clusters to receive Pub/Sub messages in real time.\nWe\u0026rsquo;ll want a load balancer in between the clients and these API servers, which will also use the \u0026ldquo;smart\u0026rdquo; sharding strategy to match clients with the appropriate API servers, which will be listening to the appropriate Kafka topics.\nWhen clients receive Pub/Sub messages, they\u0026rsquo;ll handle them accordingly (mark a channel as unread, for example), and if the clients refresh their browser or their mobile app, they\u0026rsquo;ll go through the entire \u0026ldquo;on app load\u0026rdquo; system that we described earlier.\nSince each Pub/Sub message comes with a timestamp, and since reading a channel and sending Slack messages involve writing to our persistent storage, the Pub/Sub messages will effectively be idempotent operations.\n7. System Diagram # Final Systems Architecture\n"},{"id":17,"href":"/tech-book/docs/networking-tips/dns/","title":"DNS Overview","section":"Networking Tips","content":" Intro # DNS (Domain Name System) allows you to interact with devices on the Internet without having to remember long strings of numbers. DNS is designed to be used in both the ways like as a TCP or as a UDP. It converts to TCP when it is not able to communicate on UDP.\nWhat is the Need for DNS? # Every host is identified by the IP address but remembering numbers is very difficult for people also the IP addresses are not static therefore a mapping is required to change the domain name to the IP address. So DNS is used to convert the domain name of the websites to their numerical IP address.\nTypes of Domain # Domain Hierarchy # TLD (Top-Level Domain) is the rightmost part of a domain name. The TLD for geeksforgeeks.com is “.com”. TLDs are divided into two categories: gTLDs (generic top-level domains) and ccTLDs (country code top-level domains). Historically, the purpose of a common top-level domain (gTLD) was to inform users of the purpose of the domain name; For example, a.com would be for business purposes, .org for organization, .edu for education, and .gov for the government. And a country code top-level domain (ccTLD) was used for geographic purposes, such as .ca for Canadian sites, .co.uk for UK sites, and so on. As a result of the high demand, many new gTLDs have emerged, including.online,.club,.website,.biz, and many others.\nSLD(Second-Level Domain): The .org component of geeksforgeeks.org is the top-level domain, while geeksforgeeks is the second-level domain. Second-level domains can only contain a-z 0-9 and hyphens and are limited to 63 characters and TLDs when registering a domain name (may not start or end with hyphens or contain consecutive hyphens).\nSubdomain: A period is used to separate a subdomain from a second-level domain. For example, the admin part is a subdomain named admin.geeksforgeeks.org. A subdomain name, like a second-level domain, is restricted to 63 characters and can only contain the letters a-z, 0-9, and hyphens (cannot begin or end with hyphens or consecutive hyphens).To create longer names, you can use multiple subdomains separated by periods, such as mailer.servers.geeksforgeeks.org. However, the maximum length should not exceed 253 characters. You can create as many subdomains as you want for your domain name.\nDNS Record Types: However, DNS is not just for websites, and there are many other types of DNS records as well. We’ll go through some of the most common ones you’re likely to encounter.\nA Record – For example, 104.26.10.228 is an IPv4 address that these entries resolve to. AAAA Record – For example, 2506:4700:20::681a:bc6 resolves to an IPv6 address. CNAME Record – For example, the subdomain name of Geeksforgeeks’s online shop is marketing.geeksforgeeks.org, which gives a CNAME record of marketing.shopify.com. To determine the IP address, another DNS request will be sent to marketing.shopify.com. MX Record – These records point to the servers that handle the email for the domain you are looking for. For example, the MX record response for geeksforgeeks.com would look like alt1.aspmx.l.google.com. There is also a priority sign on these documents. It instructs the client in which order to try the servers. This is useful when the primary server fails and the email needs to be sent to a backup server. TXT Record – TXT records are text fields that can be used to store any text-based data. TXT records can be used for a variety of things, but one of the most common is to identify the server that has the authorization to send an email on behalf of the domain (this can help in the fight against spam and fake email). is). They can also be used to verify domain ownership when registering for third-party services. How Does DNS Work? # What Are The Steps in a DNS Lookup? # References:\nDNS "},{"id":18,"href":"/tech-book/docs/networking-tips/ecmp/","title":"ECMP Load Balancing","section":"Networking Tips","content":" Intro # ECMP Hashing # ECMP is classified into per-flow load balancing and per-packet load balancing.\nPer-flow load balancing can ensure the packet sequence and ensure that the same data flow is forwarded according to the routing entry with the same next hop and different data flows are forwarded according to routing entries with different next hops.\nWhen multiple routes are installed in the routing table, a hash is used to determine which path a packet follows.\nHashes on the following fields:\nIP protocol Ingress interface Source IPv4 or IPv6 address Destination IPv4 or IPv6 address Further, hashes on these additional fields:\nSource MAC address Destination MAC address Ethertype VLAN ID For TCP/UDP frames, also hashes on:\nSource port Destination port Per-packet load balancing improves ECMP bandwidth utilization and evenly load balances traffic among equal-cost routes, but it cannot prevent out-of-order packet delivery. To resolve this issue, the device or terminal that receives traffic must support reassembly of out-of-order packets.\nThe following per-packet load balancing modes are supported: Random mode: A route is randomly selected among multiple equal-cost routes to forward packets. When the IP address and MAC address of known unicast packets remain unchanged, configure random per-packet load balancing. Round-robin mode: Each equal-cost route is used to forward packets in turn. When known unicast packets have a similar length, configure round-robin per-packet load balancing.\nEth-Trunk/Bundle Load Balancing # Ethernet link aggregation, also known as Eth-Trunk, bundles multiple physical links into a logical link to increase link bandwidth. The bundled links back up each other, increasing reliability.\nEth-Trunk load balancing is classified into per-packet load balancing and per-flow load balancing.\nPer-packet load balancing improves Eth-Trunk bandwidth utilization, but it cannot prevent out-of-order packet delivery. To resolve this issue, the device or terminal that receives traffic must support reassembly of out-of-order packets. The following per-packet load balancing modes are supported: Random mode: The outbound interface of packets is selected randomly based on the time when packets reach the Eth-Trunk. When the IP address and MAC address of known unicast packets remain unchanged, configure random per-packet load balancing. Round-robin mode: Each member interface of an Eth-Trunk forwards traffic in turn. When known unicast packets have a similar length, configure round-robin per-packet load balancing. Per-flow load balancing ensures the packet sequence and ensures that the same data flow is forwarded through the same physical link and different data flows are forwarded through different physical links. Table 1-2 describes the per-flow load balancing modes for different types of packets. How Do I Solve the Hash Polarization Problem? # Hash polarization, also known as hash imbalance, indicates that traffic is unevenly load balanced after being hashed twice or more. This situation is common when hash operations are performed across devices multiple times. For example, a device performs ECMP hashing and forwards traffic to two or more connected devices, which then perform ECMP or Eth-Trunk hashing again. Hash polarization may also occur if the outbound interfaces of ECMP routes are multiple Eth-Trunk interfaces on the same device. The implementation of the hash function on switches heavily depends on chips. Therefore, hash polarization may occur if switches using the same type of chips are located at adjacent network layers. If both Eth-Trunk hashing and ECMP hashing exist on the same device, hash polarization may also occur. Therefore, if ECMP or Eth-Trunk hashing is deployed on a multi-layer network, consider the risk of hash polarization.\nSuggestions # If load imbalance or hash polarization occurs during traffic forwarding, you can adjust the hash algorithm on the device to resolve the problem.\nHash algorithm: is configured by specifying the hash-mode hash-mode-id parameter. Seed value: is configured by specifying the seed seed-data parameter. If devices from multiple vendors exist on the network, you are advised to configure the same seed value for these devices. Offset: is configured by specifying the universal-id universal-id parameter. Typically, one hash algorithm corresponds to one offset. When devices from multiple vendors exist on the network, you are advised to configure the same offset for these devices. Resilient Hashing # When a next hop fails or is removed from an ECMP pool, the hashing or hash bucket assignment can change. For deployments where there is a need for flows to always use the same next hop, like TCP anycast deployments, this can create session failures.\nResilient hashing is an alternate mechanism for managing ECMP groups. The ECMP hash performed with resilient hashing is exactly the same as the default hashing mode. Only the method in which next hops are assigned to hash buckets differs — they’re assigned to buckets by hashing their header fields and using the resulting hash to index into the table of 2^n hash buckets. Since all packets in a given flow have the same header hash value, they all use the same flow bucket.\nReferences:\nhttps://docs.nvidia.com/networking-ethernet-software/cumulus-linux-43/Layer-3/Routing/Equal-Cost-Multipath-Load-Sharing-Hardware-ECMP/ https://support.huawei.com/enterprise/en/doc/EDOC1100086965 "},{"id":19,"href":"/tech-book/docs/systemdesign-tips/google-drive/","title":"Google Drive - Design","section":"SystemDesign-Tips","content":" 1. Gathering System Requirements # As with any systems design interview question, the first thing that we want to do is to gather system requirements; we need to figure out what system we\u0026rsquo;re building exactly.\nWe\u0026rsquo;re designing the core user flow of the Google Drive web application. This consists of storing two main entities: folders and files. More specifically, the system should allow users to create folders, upload and download files, and rename and move entities once they\u0026rsquo;re stored. We don\u0026rsquo;t have to worry about ACLs, sharing entities, or any other auxiliary Google Drive features.\nWe\u0026rsquo;re going to be building this system at a very large scale, assuming 1 billion users, each with 15GB of data stored in Google Drive on average. This adds up to approximately 15,000 PB of data in total, without counting any metadata that we might store for each entity, like its name or its type.\nWe need this service to be Highly Available and also very redundant. No data that\u0026rsquo;s successfully stored in Google Drive can ever be lost, even through catastrophic failures in an entire region of the world.\n2. Coming Up With A Plan # It\u0026rsquo;s important to organize ourselves and to lay out a clear plan regarding how we\u0026rsquo;re going to tackle our design. What are the major, distinguishable components of our how system?\nFirst of all, we\u0026rsquo;ll need to support the following operations:\nFor Files\nUploadFile DownloadFile DeleteFile RenameFile MoveFile For Folders\nCreateFolder GetFolder DeleteFolder RenameFolder MoveFolder Secondly, we\u0026rsquo;ll have to come up with a proper storage solution for two types of data:\nFile Contents: The contents of the files uploaded to Google Drive. These are opaque bytes with no particular structure or format. Entity Info: The metadata for each entity. This might include fields like entityID, ownerID, lastModified, entityName, entityType. This list is non-exhaustive, and we\u0026rsquo;ll most likely add to it later on. Let\u0026rsquo;s start by going over the storage solutions that we want to use, and then we\u0026rsquo;ll go through what happens when each of the operations outlined above is performed.\n3. Storing Entity Info # To store entity information, we can use key-value stores. Since we need high availability and data replication, we need to use something like Etcd, Zookeeper, or Google Cloud Spanner (as a K-V store) that gives us both of those guarantees as well as consistency (as opposed to DynamoDB, for instance, which would give us only eventual consistency).\nSince we\u0026rsquo;re going to be dealing with many gigabytes of entity information (given that we\u0026rsquo;re serving a billion users), we\u0026rsquo;ll need to shard this data across multiple clusters of these K-V stores. Sharding on entityID means that we\u0026rsquo;ll lose the ability to perform batch operations, which these key-value stores give us out of the box and which we\u0026rsquo;ll need when we move entities around (for instance, moving a file from one folder to another would involve editing the metadata of 3 entities; if they were located in 3 different shards that wouldn\u0026rsquo;t be great). Instead, we can shard based on the ownerID of the entity, which means that we can edit the metadata of multiple entities atomically with a transaction, so long as the entities belong to the same user.\nGiven the traffic that this website needs to serve, we can have a layer of proxies for entity information, load balanced on a hash of the ownerID. The proxies could have some caching, as well as perform ACL checks when we eventually decide to support them. The proxies would live at the regional level, whereas the source-of-truth key-value stores would be accessed globally.\n4. Storing File Data # When dealing with potentially very large uploads and data storage, it\u0026rsquo;s often advantageous to split up data into blobs that can be pieced back together to form the original data. When uploading a file, the request will be load balanced across multiple servers that we\u0026rsquo;ll call \u0026ldquo;blob splitters\u0026rdquo;, and these blob splitters will have the job of splitting files into blobs and storing these blobs in some global blob-storage solution like GCS or S3 (since we\u0026rsquo;re designing Google Drive, it might not be a great idea to pick S3 over GCS :P).\nOne thing to keep in mind is that we need a lot of redundancy for the data that we\u0026rsquo;re uploading in order to prevent data loss. So we\u0026rsquo;ll probably want to adopt a strategy like: try pushing to 3 different GCS buckets and consider a write successful only if it went through in at least 2 buckets. This way we always have redundancy without necessarily sacrificing availability. In the background, we can have an extra service in charge of further replicating the data to other buckets in an async manner. For our main 3 buckets, we\u0026rsquo;ll want to pick buckets in 3 different availability zones to avoid having all of our redundant storage get wiped out by potential catastrophic failures in the event of a natural disaster or huge power outage.\nIn order to avoid having multiple identical blobs stored in our blob stores, we\u0026rsquo;ll name the blobs after a hash of their content. This technique is called Content-Addressable Storage, and by using it, we essentially make all blobs immutable in storage. When a file changes, we simply upload the entire new resulting blobs under their new names computed by hashing their new contents.\nThis immutability is very powerful, in part because it means that we can very easily introduce a caching layer between the blob splitters and the buckets, without worrying about keeping caches in sync with the main source of truth when edits are made\u0026ndash;an edit just means that we\u0026rsquo;re dealing with a completely different blob.\n5. Entity Info Structure # Since folders and files will both have common bits of metadata, we can have them share the same structure. The difference will be that folders will have an is_folder flag set to true and a list of children_ids, which will point to the entity information for the folders and files within the folder in question. Files will have an is_folder flag set to false and a blobs field, which will have the IDs of all of the blobs that make up the data within the relevant file. Both entities can also have a parent_id field, which will point to the entity information of the entity\u0026rsquo;s parent folder. This will help us quickly find parents when moving files and folders.\n`File Info` `{` `blobs: [\u0026#39;blob_content_hash_0\u0026#39;, \u0026#39;blob_content_hash_1\u0026#39;],` `id: \u0026#39;some_unique_entity_id\u0026#39;` `is_folder: false,` `name: \u0026#39;some_file_name\u0026#39;,` `owner_id: \u0026#39;id_of_owner\u0026#39;,` `parent_id: \u0026#39;id_of_parent\u0026#39;,` `}` `Folder Info` `{` `children_ids: [\u0026#39;id_of_child_0\u0026#39;, \u0026#39;id_of_child_1\u0026#39;],` `id: \u0026#39;some_unique_entity_id\u0026#39;` `is_folder: true,` `name: \u0026#39;some_folder_name\u0026#39;,` `owner_id: \u0026#39;id_of_owner\u0026#39;,` `parent_id: \u0026#39;id_of_parent\u0026#39;,` `} `\n6. Garbage Collection # Any change to an existing file will create a whole new blob and de-reference the old one. Furthermore, any deleted file will also de-reference the file\u0026rsquo;s blobs. This means that we\u0026rsquo;ll eventually end up with a lot of orphaned blobs that are basically unused and taking up storage for no reason. We\u0026rsquo;ll need a way to get rid of these blobs to free some space.\nWe can have a Garbage Collection service that watches the entity-info K-V stores and keeps counts of the number of times every blob is referenced by files; these counts can be stored in a SQL table.\nReference counts will get updated whenever files are uploaded and deleted. When the reference count for a particular blob reaches 0, the Garbage Collector can mark the blob in question as orphaned in the relevant blob stores, and the blob will be safely deleted after some time if it hasn\u0026rsquo;t been accessed.\n7. End To End API Flow # Now that we\u0026rsquo;ve designed the entire system, we can walk through what happens when a user performs any of the operations we listed above.\nCreateFolder is simple; since folders don\u0026rsquo;t have a blob-storage component, creating a folder just involves storing some metadata in our key-value stores.\nUploadFile works in two steps. The first is to store the blobs that make up the file in the blob storage. Once the blobs are persisted, we can create the file-info object, store the blob-content hashes inside its blobs field, and write this metadata to our key-value stores.\nDownloadFile fetches the file\u0026rsquo;s metadata from our key-value stores given the file\u0026rsquo;s ID. The metadata contains the hashes of all of the blobs that make up the content of the file, which we can use to fetch all of the blobs from blob storage. We can then assemble them into the file and save it onto local disk.\nAll of the Get, Rename, Move, and Delete operations atomically change the metadata of one or several entities within our key-value stores using the transaction guarantees that they give us.\n8. System Diagram # Final Systems Architecture\n"},{"id":20,"href":"/tech-book/docs/manageability/why-grpc-on-http2/","title":"gRPC on HTTP/2","section":"Manageability","content":" Intro # tbd\nReferences:\nHTTP/2: Smarter at scale gRPC on HTTP/2: Engineering a robust, high performance protocol "},{"id":21,"href":"/tech-book/docs/networking-tips/ip-fragmentation/","title":"IP Fragmentation - IPv4 \u0026 IPv6","section":"Networking Tips","content":" Intro # Like IPv4, IPv6 fragmentation divides an IPv6 packet into smaller packets to facilitate transmission across networks with a smaller Maximum Transmission Unit (MTU). Unlike IPv4, fragmentation is not mandatory in IPv6, as all networks support an MTU of at least 1280 bytes.\nUnlike IPv4, IPv6 relies on the source device instead of intermediary routers for fragmentation Unlike IPv4, an IPv6 router does not fragment a packet unless it is the packet’s source. Intermediate nodes (routers) do not fragment. You will see how an IPv6 device fragments packets when it is the source of the packet with the use of extension headers. An IPv6 router drops packets too large for the egress interface and sends an ICMPv6 Packet Too Big message back to the source. Packet Too Big messages include the link’s MTU size in bytes so the source can resize the packet. Therefore, using the largest packet size supported by all the links from the source to the destination is preferable. Path MTUs (PMTUs) are used for this purpose. Path MTU Discovery: # In addition, IPv6 nodes can use the Path MTU Discovery (PMTUD) mechanism to dynamically determine the maximum MTU size along the path to a destination. PMTUD sends packets with the “Don’t Fragment” (DF) flag set and progressively reduces the packet size until a smaller MTU is found. Once the maximum MTU size is determined, the source node can adjust its packet size accordingly to avoid fragmentation.\nWhile IPv6 fragmentation is a valuable mechanism for ensuring packet delivery over networks with smaller MTU sizes, it should be used sparingly. Minimizing the need for fragmentation through proper MTU configuration and utilizing PMTUD can help improve network performance and reliability. ICMP and ICMPv6 # The Internet Control Messaging Protocol ( ICMP ) was initially introduced to aid network troubleshooting by providing tools to verify end-to-end reachability. ICMP also reports back errors on hosts. Unfortunately, due to its nature and lack of built-in security, it quickly became a target for many attacks. For example, an attacker can use ICMP REQUESTS for network reconnaissance.\nICMP’s lack of inherent security opened it up to some vulnerabilities. This results in many security teams blocking all ICMP message types, which harms useful ICMP features such as Path MTU. ICMP for v4 and v6 are entirely different. Unlike ICMP for IPv4, ICMPv6 is an integral part of v6 communication, and ICMPv6 has features required for IPv6 operation. For this reason, it is not possible to block ICMPv6 and all its message types. ICMPv6 is a legitimate part of V6; you must select what you can filter. ICMPv6 should not be completely filtered.\nFragmentation is normal # Fragmentation is a normal process on packet-switched networks. It occurs when a large packet is received, and the corresponding outbound interface’s maximum transmission unit (MTU) size is too small. Fragmentation dissects the IP packet into smaller packets before transmission. The receiving host performs fragment reassembly and passes the complete IP packet up the protocol stack.\nFragmentation is an IP process; TCP and other layers above IP are not involved. Reassembly is intended in the receiving host, but in practice, it may be done by an intermediate router. For example, network address translation (NAT) may need to reassemble fragments to translate data streams. This is where some differences between fragmentation in IPv4 and IPv6 are apparent.\nSo, in summary. IP fragmentation occurs at the Internet Protocol (IP) layer. Packets are fragmented to pass through a link with a smaller maximum transmission unit (MTU) than the original packet size. The receiving host then reassembles fragments.\nImpact on networks # In networks with multiple parallel paths, technologies such as LAG and CEF split traffic according to a hash algorithm. All packets from the same flow are sent out on the same path to minimize packet reordering. IP fragmentation can cause excessive retransmissions when fragments encounter packet loss. This is because reliable protocols such as TCP must retransmit all fragments to recover from the loss of a single fragment.\nFragmentation in IPv4 and IPv6 # Fragmentation in IPv6 is splitting a single inbound IP datagram into two or more outbound IP datagrams. The IP header is copied from the original IP datagram into the fragments. With IPv6, special bits are set in the fragments’ IPv6 headers to indicate that they are not complete IP packets. In the case of IPv6, we use IPv6 extension headers. Then, the payload is spread across the fragments. In IPv4, fragmentation is done whenever required, at the destination or routers, whereas, in IPv6, only the source, not the routers, is supposed to do fragmentation. This can only be done when the source knows the Maximum Transmission Unit (MTU) path. The IPv6 “do not fragment” bit is always 1, whereas the case is not the same in IPv4, and the ‘More fragment’ bit is the only flag in the fragmentation header, which is one bit. Two bits are reserved for future use, as shown in the picture below. The following diagram displays the Internet Protocol Version 6 Fragmentation Header.\nIPv6 fragmentation example # In an IPv6 world, the IPv6 header length is limited to 40 bytes, yet the IPv4 header has a max of 60. The primary IPv6 header remains a fixed size. IPv6 has the concept of extension headers to add optional IP layer information. Special handling with IPv4 was controlled by “IP options,” but there are no IP options in IPv6. All options are moved to different types of extension headers. The following IPv6 extension headers are currently defined.\nRouting – Extended routing, like IPv4 loose source route Fragmentation – Fragmentation and reassembly Authentication – Integrity and authentication, security Encapsulation – Confidentiality Hop-by-Hop Option – Special options that require hop-by-hop processing Destination Options – Optional information to be examined by the destination node The IPv6 fragment header # With IPv4, all this was contained in the IPv4 header. There is no separate fragment header in IPv4, and the fragment fields in the IPv4 header are moved to the IPv6 fragment header. The “Don’t fragment” bit (DF) is removed, and intermediate routers are not allowed to fragment. They only permitted end stations to create and reassemble fragments (RFC 2460), not intermediate routers. The design decision stems from the performance hit that fragmentation imposes on nodes.\nRouters are no longer required to perform packet fragmentation and reassembly, making dropped packets more significant than the router’s interface MTU. Instead, IPv6 hosts perform PMTU to determine the maximum packet size for the entire path. When a packet hits an interface with a smaller MTU, the routers send back an ICMPv6 type 2 error, known as Packet Too Big, to the sending host. The sending host receives the error message, reduces the size of the sending packet, and tries again.\nReferences:\nIPv6 Fragmentation "},{"id":22,"href":"/tech-book/docs/networking-tips/ip-tos-dscp/","title":"IP Precedence And TOS | DSCP","section":"Networking Tips","content":" Intro # 8 Bits of Type of Service in IP Header.\nIP Precedence # RFC791/RFC1349 Interpretation\n** Bits ** 7-5 IP Precedence\n111\tNetwork Control\n110\tInternetwork Control\n101\tCritic/ECP\n100\tFlash Override\n011\tFlash\n010\tImmediate\n001\tPriority\n000\tRoutine\nBits\n4 (1 = Low Delay; 0 = Normal Delay)\n3 (1 = High Throughput; 0 = Normal Throughput)\n2 (1 = High Reliability; 0 = Normal Reliability)\n1 (1 = Minimise monetary cost (RFC 1349))\n0 (Must be 0)\nTOS | DSCP (Differentiated Services Code Point) # RFC 2474 (Differentiated Services) Interpretation\nBits\n7-2\tDSCP\n1-0\tECN (Explicit Congestion Notification)\nDefault Forwarding (DF) PHB # Typically best-effort traffic The recommended DSCP for DF is 0\nExpedited Forwarding (EF) PHB # Dedicated to low-loss, low-latency traffic The recommended DSCP for EF is 101110B (46 or 2E(hex))\nAssured Forwarding (AF) PHB # Gives assurance of delivery under prescribed conditions\nDrop probability Class 1 Class 2 Class 3 Class 4 Low AF11 (DSCP 10) 001010 AF21 (DSCP 18) 010010 AF31 (DSCP 26) 011010 AF41 (DSCP 34) 100010 Medium AF12 (DSCP 12) 001100 AF22 (DSCP 20) 010100 AF32 (DSCP 28) 011100 AF42 (DSCP 36) 100100 High AF13 (DSCP 14) 001110 AF23 (DSCP 22) 010110 AF33 (DSCP 30) 011110 AF43 (DSCP 38) 100110 Class Selector PHBs # This maintains backward compatibility with the IP precedence field.\nService class DSCP Name DSCP Value IP precedence Examples of application Standard CS0 (DF) 0 0 (000) Low-priority data CS1 8 1 (001) File transfer (FTP, SMB) Network operations, administration and management (OAM) CS2 16 2 (010) SNMP, SSH, Ping, Telnet, syslog Broadcast video CS3 24 3 (011) RTSP broadcast TV, treaming of live audio and video events, video surveillance,video-on-demand Real-time interactive CS4 32 4 (100) Gaming, low priority video conferencing Signaling CS5 40 5 (101) Peer-to-peer (SIP, H.323, H.248), NTP Network control CS6 48 6 (110) Routing protocols (OSPF, BGP, ISIS, RIP) Reserved for future use CS7 56 7 (111) DF= Default Forwarding\nPHB == Per-Hop-Behavior\nCS: Class Selector (RFC 2474) AFxy: Assured Forwarding (x=class, y=drop precedence) (RFC2597) EF: Expedited Forwarding (RFC 3246) References:\nhttps://en.wikipedia.org/wiki/Differentiated_services https://bogpeople.com/networking/dscp.shtml "},{"id":23,"href":"/tech-book/docs/networking-tips/traceroute/","title":"Linux traceroute tool","section":"Networking Tips","content":" Intro # The linux traceroute tool is been great use for network engineer for their troubleshooting. The first version of this tool has been introduced 25 years ago. It\u0026rsquo;s relying on the TTL field of the ip-header, keep on incrementing starting on TTL as 1. Along the way, each router decreases the TTL by one, and when it hits zero, the router sends back an ICMP \u0026rsquo;time exceeded\u0026rsquo; message, revealing its identity. The modern network has evloved over time. Now a ways, it has always the multi-path support for destinations, NAT, etc. Hence, traceroute needs to enhanced to accomodate these use-cases.\nThere are two implementations emerged. lets go little deeper for both the implementations.\nparis-traceroute (https://paris-traceroute.net/about/) dublin-traceroute (https://dublin-traceroute.net/) Paris Traceroute # This maintain a per-flow session information for load-balancer to work. Other than this, it also maintain tcp, udp and icmp packet mode. Also, it sends multiple probe with different session information, which helps to find different packet path for same destination.\nDublin Traceroute # This is Paris-traceroute + NAT awareness.\nReferences:\nParis Traceroute Dublin Traceroute The Power of Paris Traceroute for Modern Load-Balanced Networks "},{"id":24,"href":"/tech-book/docs/algorithms/medium/","title":"Medium Complexity","section":"Algorithms","content":" Medium Complexity # Number Swapper: Write a function to swap a number in place (that is, without temporary variables). Hints - with just addition/substruction arithmatic, XOR.\nTic Tac Win: Design an algorithm to figure out if someone has won a game of tic-tac-toe.\nHashing # Two Sum: Find a pair in array whose sum equals to the target input: [10,4,1,3,2] Target: 7 Output: [4,3]\nHint - insert in hash and then start searching from first element, find the difference with the target and find the hash.\nLogest consecutive sequence: find length of longest consecutive sequence from given array input: [10, 4, 20, 1,3,2] Output: [1,2,3,4]\nSliding Window # Two Sum: find a pair whose sum is equal to target\ninput: [10,4,1,3,2] Target: 7 Output: [4,3]\nHint - sort, start from bengining and end at the same time\nTraping Water: Given an arrary of height of bars (width = 1) calculate the amount of water trapped.\ninput: 5, 2, 3, 4, 1, 3 Output = 5\nHint - use sliding window.\nRecursion # Combination Sum: Return the list of numbers from given array whose sum = target\nGenerate all pairs of valid parenthesis\nInput: 2\nOutput: {\n​\t()(),\n​\t(())\n​\t}\nBinary Tree # Height of the binary tree\nHints - use recursion\nDiameter of the binary tree\nThe diameter of the binary tree is the length of the longest path between any two nodes in the tree.\nHints - use recursion\nConvert to the Sum Tree: Convert it such that every node\u0026rsquo;s value is equal to sum of its left and right sub tree.\nHints - use recursion\nMaximum path sum in binary tree\nLowest common ancestor in a binary tree\n"},{"id":25,"href":"/tech-book/docs/networking-tips/mlag/","title":"Multi Chassis Link Aggregation Basics","section":"Networking Tips","content":" Intro # Link Aggregation Basics # Link aggregation is an ancient technology that allows you to bond multiple parallel links into a single virtual link (link aggregation group – LAG). With parallel links being replaced by a single virtual link, STP detects no loops and all the physical links can be fully utilized.\nI was also told that link aggregation makes your bridged network more stable. Every link state change triggers a Topology Change Notification in spanning tree, potentially sending a large part of your network through the STP listening-learning-forwarding state diagram. A link failure of a LAG member does not change the state of the aggregation group (unless it was the last working link in the group), and since STP rides on top of aggregated interfaces, it does not react to the failure.\nMulti-Chassis Link Aggregation(MLAG) # Imagine you could pretend two physical boxes use a single control plane and coordinated switching fabrics. The links terminated on two physical boxes would seem to terminate within the same control plane and you could aggregate them using LACP. Welcome to the wonderful world of Multi-Chassis Link Aggregation (MLAG).\nMLAG nicely solves two problems:\nWhen used in the network core, multiple links appear as a single link. No bandwidth is wasted due to STP loop prevention while you still retain almost full redundancy – just make sure you always read the smallprint to understand what happens when one of the two switches in the MLAG pair fails. When used between a server and a pair of top-of-rack switches, it allows the server to use the full aggregate bandwidth while still retaining resiliency against a link or switch failure. Multi-Chassis Link Aggregation Group (MC-LAG) # Overview # MC-LAG achieves similar redundancy and load balancing but is used primarily in service provider networks. Unlike MLAG, MC-LAG often uses a control plane protocol (e.g., ICCP - Inter-Chassis Control Protocol) to sync state information.\nArchitecture # Uses a control protocol (ICCP) to synchronize state between switches. Traffic forwarding decisions are synchronized between nodes. Unlike MLAG, which is mostly layer 2, MC-LAG may operate at Layer 3 for better scaling. Vendor-Specific Implementations # Vendor MC-LAG Implementation Key Features Juniper MC-LAG Uses ICCP for control-plane synchronization Nokia MC-LAG Supports both L2 and L3 redundancy Cisco (SP networks) MC-LAG Used in service provider routers (ASR, NCS series) Huawei MC-LAG Provides link aggregation for core networks Key Technical Differences: MLAG vs MC-LAG # Feature MLAG MC-LAG Primary Use Case Data center networks Service provider networks Interoperability Vendor-specific (Cisco vPC, Arista MLAG) More interoperable (Juniper, Nokia, Cisco SP) Control Plane No dedicated protocol; uses peer links Uses ICCP or proprietary control protocols Layer of Operation Layer 2 primarily Layer 2 \u0026amp; Layer 3 Failure Handling Uses peer-link recovery methods Uses ICCP for state synchronization Complexity Easier to configure More complex due to control-plane sync Scalability Works well for small to medium-sized networks Scales better in larger SP environments Link Aggregation Control Protocol (LACP) # LACP (Link Aggregation Control Protocol): a subcomponent of IEEE 802.3ad standard, provides a method to control the bundling of several physical ports together to form a single logical channel. LACP allows a network device to negotiate an automatic bundling of links by sending LACP packets to the peer.\nReferences:\nhttps://blog.ipspace.net/2010/10/multi-chassis-link-aggregation-basics.html https://community.fs.com/article/mlag-vs-stacking-vs-lacp.html "},{"id":26,"href":"/tech-book/docs/data-center/data-center-network-virtualization/","title":"Network Virtualization in Cloud Data Centers","section":"Data Center Tips","content":" Intro # Geographic Clusters of Data Centers # Data Center Interconnection (DCI) # Challenges of LAN Extension # TRILL # TRILL Architecture # TRILL Encapsulation Format # TRILL Features # GRE # GRE (Cont) # NVGRE # NVGRE (Cont) # NVO3 # NVO3 Terminology # NVO3 Terminology (Cont) # Current NVO Technologies # VXLAN # VXLAN Architecture # VXLAN Deployment Example # ![img|320x271](https://prasenjitmanna.com/tech-book/diagrams/dc-virtualization/VXLAN Deployment Example.png)\nVXLAN Encapsulation Format # ![img|320x271](https://prasenjitmanna.com/tech-book/diagrams/dc-virtualization/VXLAN Encapsulation Format.png)\nVXLAN Encapsulation Format (Cont) # Geneve # Geneve Frame Format # Geneve Frame Format (Cont) # LSO and LRO # Geneve Implementation Issues # Geneve Implementation Issues (Cont) # References:\nRaj Jain LAN Extension and Network Virtualization in Cloud Data Centers by Raj Jain Overlay Virtual Networking Explained by Ivan Pepelnjak(ip@ipSpace.net) "},{"id":27,"href":"/tech-book/docs/optical-knowledge/optical-breakout/","title":"Optical Transceiver(Grey) \u0026 Breakout Model","section":"Optical Knowledge","content":" Intro # There are two types of optics, grey and colored. In the grey optics, it has specific wavelength based on the pluggable types and they are used for short distances, while colored optics are designed for longer distance and it has more control on the frequency/wave-length configuration. In general, grey optics are used in router interface. This section covers mostly on Grey Optics.\nTransceiver Names: Your Guide to Decoding Fiber Optics\nHow to choose the right transceiver Fiber Connector Types - LC vs SC vs FC vs ST vs MTP vs MPO Everything You Need to Know About MTP® MPO Connectors\nHow to Breakout 100G to 4x25G\nHow to breakout 400G QSFP-DD DR4 to 4x 100G Multi-Mode Vs Single Mode Transceivers\nHow to Choose the Right Fiber Optic Cable For Your Optical Transceiver\nAsk an Engineer: Fiber Breakout Cable\nReferences: *\n"},{"id":28,"href":"/tech-book/docs/networking-tips/qos/","title":"QoS","section":"Networking Tips","content":" Intro # Traffic Shaping # The traffic shapers that we will look at here are commonly called token bucket shapers and are based on a token bucket algorithm which we will see can serve multiple purposes in achieving network QoS. We should note that frequently in networking literature such algorithms were also referred to as leaky bucket algorithms. In all but one case, that we will not be discussing here, these algorithms produce the same results. Use the token bucket and leaky bucket Wikipedia entries as a guide if you are reading older networking literature.\nThe basic token bucket traffic shaper is characterized by two parameters: a rate, r, and a bucket size, b. The rate can be in either bits or bytes per second and the bucket size can be in either bits or bytes.\nThe key pieces of the shaper are:\nToken generator with rate r. This requires some type of relatively accurate timing mechanism. The granularity of this mechanism heavily influences the accuracy to which r can be approximated.\nToken bucket with size b. This can be implemented with up/down counters or adders/subtractors in hardware or software.\nA buffer (queue) to hold packets\nLogic to control the release of packets based on packet size and current bucket fill.\nPolicing # A Single Rate Three Color Marker # A policer called a single rate three color marker (srTCM) is defined in RFC2697 an informational RFC. The single rate means we will use a single token generator with a specified rate parameter. By three color they mean that packets will be marked with by three levels of compliance (Green, Yellow, Red) from compliant to most out of compliance.\nThis particular policer is defined by three traffic parameters, an update algorithm, and marking criteria. The three traffic parameters are:\nCommitted Information Rate (CIR) measured in bytes of IP packets per second, i.e., it includes the IP header, but not link specific headers. This is the token rate.\nCommitted Burst Size (CBS) measured in bytes. This is the token bucket size associated with the rate.\nExcess Burst Size (EBS) measured in bytes. This is a bucket filled from the overflow of the first bucket hence the expression \u0026ldquo;excesses burst size\u0026rdquo;. This bucket allows us to save up tokens from periods of inactivity for later use. The CBS and EBS must be configured so that at least one of them is larger than 0.\nThere are two modes of operation for the srTCM one, called Color-Aware works with packets that have been previously marked by another policer in the network, the other mode called Color-Blind works with packets that have not been previously marked. The Color-Blind mode of operation is shown in below.\nTwo Rate Three Color Marker # Another policer called a two rate three color marker (trTCM) is defined in RFC2698, an informational RFC. The two rate means we will use a two token generators each with a specified rate parameter. Once again the three colors Green, Yellow, Red indicate the level of compliance which gets translated into packet drop precedence when the packets get marked. The parameters for this policer are: the Peak Information Rate (PIR), the Peak Burst Size (PBS), the Committed Information Rate (CIR), and the Committed Burst Size (CBS). All use units similar to that of the srTCM. The initialization, update, and marking behavior for the Color-Blind mode is shown below.\nReferences:\nhttps://www.grotto-networking.com/BBQoS.html "},{"id":29,"href":"/tech-book/docs/networking-tips/spine-leaf-arch/","title":"Spine-leaf Architecture Basics","section":"Networking Tips","content":" Intro # What Is Spine-leaf Architecture? # The spine-leaf architecture consists of only two layers of switches: spine and leaf switches. The spine layer consists of switches that perform routing and work as the core of the network. The leaf layer involves access switches that connect to servers, storage devices, and other end-users. This structure helps data center networks reduce hop count and reduce network latency.\nIn the spine-leaf architecture, each leaf switch is connected to each spine switch. With this design, any server can communicate with any other server, and there is no more than one interconnected switch path between any two leaf switches.\nWhy Use Spine-leaf Architecture? # The spine-leaf architecture has become a popular data center architecture, bringing many advantages to the data center, such as scalability, network performance, etc. The benefits of spine-leaf architecture in modern networks are summarized here in five points.\nIncreased redundancy: The spine-leaf architecture connects the servers with the core network, and has higher flexibility in hyper-scale data centers. In this case, the leaf switch can be deployed as a bridge between the server and the core network. Each leaf switch connects to all spine switches, which creates a large non-blocking fabric, increasing the level of redundancy and reducing traffic bottlenecks.\nIncreased bandwidth: The spine-leaf architecture can effectively avoid traffic congestion by applying protocols or techniques such as transparent interconnection of multiple links (TRILL) and shortest path bridging (SPB). The spine-leaf architecture can be Layer 2 or Layer 3, so uplinks can be added to the spine switch to expand inter-layer bandwidth and reduce oversubscription to secure network stability.\nImprove scalability: The spine-leaf architecture has multiple links that can carry traffic. The addition of switches will improve scalability and help enterprises to expand their business later.\nReduced expenses: A spine-leaf architecture increases the number of connections each switch can handle, so data centers require fewer devices to deploy. Many data center networks employ spine-leaf architecture to minimize costs.\nMinimal latency and congestion: By limiting the maximum number of hops to two between any source and destination nodes, we establish a more direct traffic path, enhancing overall performance and mitigating bottlenecks. The only exception is when the destination is on the same leaf switch.\nSpine-leaf vs. Traditional Three-Tier Architecture # The main difference between spine-leaf architecture and 3-tier architecture lies in the number of network layers, and the traffic they transform is north-south or east-west traffic.\nAs shown in the following figure, the traditional three-tier network architecture consists of three layers: core, aggregation and access. The access switches are connected to servers and storage devices, the aggregation layer aggregates the access layer traffic, provides redundant connections at the access layer, and the core layer provides network transmission. But this three-layer topology is usually designed for north-south traffic and uses the STP protocol, supporting up to 100 switches. In the case of continuous expansion of network data, this will inevitably result in port blockage and limited scalability.\nThe spine-leaf architecture is to add east-west traffic parallelism to the north-south network architecture of the backbone, fundamentally solving the bottleneck problem of the traditional three-tier network architecture. It increases the exchange layer under the access layer, and the data transmission between two nodes is completed directly at this layer, thereby diverting backbone network transmission. Compared with traditional three-tier architecture, the spine-leaf architecture provides a connection through the spine with a single hop between leaves, minimizing any latency and bottle necks. In spine-leaf architectures, the switch configuration is fixed so that no network changes are required for a dynamic server environment.\nHow to Design Spine-leaf Architecture? # Before designing a spine-leaf architecture, you need to figure out some important and relevant considerations, especially the oversubscription rate and the size of the spine switch. Surely, we have also given a detailed example for your reference.\nDesign Considerations of Spine-leaf Architecture # Oversubscription rate: It is the contention rate when all devices are sending traffic at the same time. It can be measured in the north/south direction (traffic entering/leaving the data center) and in the east/west direction (traffic between devices within the data center). The most appropriate oversubscription ratio for modern network architectures is 3:1 or less, which is measured and delineated as a ratio between upstream bandwidth (to backbone switches) and downstream capacity (to servers/storage).\nFor example, a leaf switch has 48 x 10G ports for a total of 480Gb/s of port capacity. If you connect 4 x 40G uplink ports from each leaf switch to a 40G spine switch, it will have 160Gb/s of uplink capacity. The ratio is 480:160, or 3:1. However, data center uplinks are typically 40G or 100G and can be migrated over time from a starting point of 40G (Nx 40G) to 100G (Nx 100G). It is important to note that the uplink should always run faster than the downlink to not have port link blockage.\nLeaf and spine sizing: The maximum number of leaf switches in the topology is determined by the port density of the spine switches. And the number of spine switches will be governed by the combination of the required throughput between the leaf switches, the number of redundant/ECMP (equivalent multipath) paths, and their port density. So the number of spine-leaf switches and port density need to be taken into account to prevent network problems.\nLayer 2 or Layer 3 design: A two-tier spine-leaf fabric can be built at either Layer 2 (configuring VLANs) or Layer 3 (subnetting). Layer 2 designs need to provide maximum flexibility, allowing VLANs to span anywhere and MAC addresses to migrate anywhere. Layer 3 designs need to provide the fastest convergence times and maximum scale with fan-out ECMP supporting up to 32 or more active spine switches.\nReferences:\nhttps://community.fs.com/article/leaf-spine-with-fs-com-switches.html "},{"id":30,"href":"/tech-book/docs/networking-tips/tcp-congestion/","title":"TCP Congestion Control","section":"Networking Tips","content":" Intro # TCP is a protocol that is used to transmit information from one computer on the internet to another, and is the protocol I’ll be focused on in this post. What distinguishes TCP from other networking protocols is that it guarantees 100% transmission. This means that if you send 100kb of data from one computer to another using TCP, all 100kb will make it to the other side. This property of TCP is very powerful and is the reason that many network applications we use, such as the web and email are built on top of it. The way TCP is able to accomplish this goal of trasmitting all the information that is sent over the wire is that for every segment of data that is sent from party A to party B, party B sends an “acknowledgement” segment back to party A indicating that it got that message.\nWhen does congestion happen? # Congestion is problem in computer networks because at the end of the day, information transfer rates are limited by physical channels like ethernet cables or cellular links, and on the internet, many individual devices are connected to these links.\nDetour: What is a link? # Before I dive into what some solutions to this problem are, I want to be a little bit more specific about the properties of links. There are three important details to know about a link:\ndelay (milliseconds) - the time it takes for one packet to get from the beginning to the end of a link bandwidth (megabits/second) - the number of packets that can get through the link in a second queue - the size of the queue for storing packets waiting to be sent out if a link is full, and the strategy for managing that queue when it hits its capacity Using the analogy of the link as a pipe, you can think of the delay as the length of the pipe, and the bandwidth as the circumference of the pipe. An important statistic about a link is the bandwidth-delay product (BDP). If senders are sending more bytes than the BDP, the link’s queue will fill and eventually start dropping packets.\nApproaches # There are two main indicators: packet loss and increased round trip times for packets. When congestion happens, queues on links begin to fill up, and packets get dropped. If a sender notices packet loss, it’s a pretty good indicator that congestion is occuring. Another consequence of queues filling up though is that if packets are spending more time in a queue before making it onto the link, the round trip time, which measures the time from when the sender sends a segment out to the time that it receives an acknowledgement, will increase. While today there are congestion control schemes that take into account both of these indicators, in the original implementations of congestion control, only packet loss was used.\nTherefore, in addition to being able to avoid congestion, congestion control approaches need to be able to “explore” the available bandwidth.\nControl-Based Algorithms # The Congestion Window # A key concept to understand about any congestion control algorithm is the concept of a congestion window. The congestion window refers to the number of segments that a sender can send without having seen an acknowledgment yet. If the congestion window on a sender is set to 2, that means that after the sender sends 2 segments, it must wait to get an acknowledgment from the receiver in order to send any more. The congestion window is often referred to as the “flight size”, because it also corresponds to the number of segments “in flight” at any given point in time.\nThe higher the congestion window, more packets you’ll be able to get across to the receiver in the same time period. To understand this intuitively, if the delay on the network is 88ms, and the congestion window is 10 segments, you’ll be able to send 10 segments for every round trip (88*2 = 176 ms), and if it’s 20 segments, you’ll be able to send 20 segments in the same time period.\nOf course, the risk with raising the congestion window too high is that it will lead to congestion. The goal of a congestion control algorithm, then, is to figure out the right size congestion window to use.\nFrom a theoretical perspective, the right size congestion window to use is the bandwidth-delay product of the link, which as we discussed earlier is the full capacity of the link. The idea here is that if the congestion window is equal to the BDP of the link, it will be fully utilized, and not cause congestion.\nTCP Tahoe # TCP Tahoe is a congestion control scheme that was invented back in the 80s, when congestion was first becoming a problem on the internet. The algorithm itself is fairly simple, and grows the congestion window in two phases.\nPhase 1 # Slow Start: The algorithm begins in a state called “slow start”. In Slow Start, the congestion window grows by 1 every time an acknowledgement is received. This effectively doubles the congestion window on every round trip. If the congestion window is 4, four packets will be in flight at once, and when each of those packets is acknowledged, the congestion window will increase by 1, resulting in a window of size 8. This process continues until the congestion window hits a value called the “Slow Start Threshold” ssthresh. This is a configurable number.\nPhase 2 # Congestion Avoidance: Once the congestion window has hit the ssthresh, it moves from “slow start” into congestion avoidance mode. In congestion avoidance, the congestion window increases by 1 on every round trip. So if the congestion window is 4, the window will increase to 5 after all four of those packets in flight have been acknowledged. This increases the window much more slowly.\nIf Tahoe detects that a packet is lost, it will resend the packet, the slow start threshold is updated to be half the current congestion window, the congestion window is set back to 1, and the algorithm goes back to slow start.\nDetecting Packet Loss \u0026amp; Fast Retransmit # There are two ways that a TCP sender could detect that a packet is lost.\nThe sender “times out”. The sender puts a timeout on every packet that is sent out into the wild, and when that timeout is hit without that packet having been acknowledged, it resends the packet and sets the congestion window to 1.\nThe receiver sends back “duplicate acks”. In TCP, receivers only acknowledge packets that are sent in order. If a packet is sent out of order, it will send out an acknowledgement for the last packet it saw in order. So, if a receiver has received segments 1,2, and 3, and then receives segment #5, it will ack segment #3 again, because #5 came in out of order. In Tahoe, if a sender receives 3 duplicate acks, it considers a packet lost. This is considered “Fast Retransmit”, because it doesn’t wait for the timeout to happen.\nThere are a number of issues with this approach though, which is why it is no longer used today. In particular, it takes a really long time, especially on higher bandwidth networks, for the algorithm to actually take full advantage of the available bandwidth. This is because the window size grows pretty slowly after hitting the slow start threshold. Another issue is that packet loss doesn’t necessarily mean that congestion is occuring–some links, like WiFi, are just inherently lossy. Reacting drastically by cutting the window size to 1 isn’t necessarily always appropriate. A final issue is that this algorithm uses packet loss as the indicator for whether there’s congestion. If the packet loss is happening due to congestion, you are already too late–the window is too high, and you need to let the queues drain.\nTCP CUBIC # This algorithm was implemented in 2005, and is currently the default congestion control algorithm used on Linux systems. Like Tahoe, it relies on packet loss as the indicator of congestion. However, unlike Tahoe, it works far better on high bandwidth networks, since rather than increasing the window by 1 on every round trip, it uses, as the name would suggest, a cubic function to determine what the window size should be, and therefore grows much more quickly.\nAvoidance-Based Algorithms # BBR(Bottleneck Bandwidth and RTT) - (Bufferbloat) # This is a very recent algorithm developed by Google, and unlike Tahoe or CUBIC, uses delay as the indicator of congestion, rather than packet loss. The rough thinking behind this is that delays are a leading indicator of congestion–they occur before packets actually start getting lost. Slowing down the rate of sending before the packets get lost ends up leading to higher throughput.\nActive Queue Management # Random Early Detection # Each router is programmed to monitor its own queue length and, when it detects that congestion is imminent, to notify the source to adjust its congestion window. RED, invented by Sally Floyd and Van Jacobson in the early 1990s.\nRED is most commonly implemented such that it implicitly notifies the source of congestion by dropping one of its packets. The source is, therefore, effectively notified by the subsequent timeout or duplicate ACK. In case you haven’t already guessed, RED is designed to be used in conjunction with TCP, which currently detects congestion by means of timeouts (or some other means of detecting packet loss such as duplicate ACKs). As the “early” part of the RED acronym suggests, the gateway drops the packet earlier than it would have to, so as to notify the source that it should decrease its congestion window sooner than it would normally have. In other words, the router drops a few packets before it has exhausted its buffer space completely, so as to cause the source to slow down, with the hope that this will mean it does not have to drop lots of packets later on.\nhow RED decides when to drop a packet and what packet it decides to drop. To understand the basic idea, consider a simple FIFO queue. Rather than wait for the queue to become completely full and then be forced to drop each arriving packet (the tail drop policy), we could decide to drop each arriving packet with some drop probability whenever the queue length exceeds some drop level. This idea is called early random drop. The RED algorithm defines the details of how to monitor the queue length and when to drop a packet.\nExplicit Congestion Notification || IP \u0026amp; TCP Flags # While TCP’s congestion control mechanism was initially based on packet loss as the primary congestion signal, it has long been recognized that TCP could do a better job if routers were to send a more explicit congestion signal. That is, instead of dropping a packet and assuming TCP will eventually notice (e.g., due to the arrival of a duplicate ACK), any AQM algorithm can potentially do a better job if it instead marks the packet and continues to send it along its way to the destination. This idea was codified in changes to the IP and TCP headers known as Explicit Congestion Notification (ECN), as specified in RFC 3168.\nSpecifically, this feedback is implemented by treating two bits in the IP TOS field as ECN bits. One bit is set by the source to indicate that it is ECN-capable, that is, able to react to a congestion notification. This is called the ECT bit (ECN-Capable Transport). The other bit is set by routers along the end-to-end path when congestion is encountered, as computed by whatever AQM algorithm it is running. This is called the CE bit (Congestion Encountered).\nIn addition to these two bits in the IP header (which are transport-agnostic), ECN also includes the addition of two optional flags to the TCP header. The first, ECE (ECN-Echo/Experienced), communicates from the receiver to the sender that it has received a packet with the CE bit set. The second, CWR (Congestion Window Reduced) communicates from the sender to the receiver that it has reduced the congestion window.\nBeyond TCP # Datacenters (DCTCP) # There have been several efforts to optimize TCP for cloud datacenters, where Data Center TCP was one of the first. There are several aspects of the datacenter environment that warrant an approach that differs from more traditional TCP. These include:\nRound trip time for intra-DC traffic are small; Buffers in datacenter switches are also typically small; All the switches are under common administrative control, and thus can be required to meet certain standards; A great deal of traffic has low latency requirements; That traffic competes with high bandwidth flows. It should be noted that DCTCP is not just a version of TCP, but rather, a system design that changes both the switch behavior and the end host response to congestion information received from switches.\nThe central insight in DCTCP is that using loss as the main signal of congestion in the datacenter environment is insufficient. By the time a queue has built up enough to overflow, low latency traffic is already failing to meet its deadlines, negatively impacting performance. Thus DCTCP uses a version of ECN to provide an early signal of congestion. But whereas the original design of ECN treated an ECN marking much like a dropped packet, and cut the congestion window in half, DCTCP takes a more finely-tuned approach. DCTCP tries to estimate the fraction of bytes that are encountering congestion rather than making the simple binary decision that congestion is present. It then scales the congestion window based on this estimate. The standard TCP algorithm still kicks in should a packet actually be lost. The approach is designed to keep queues short by reacting early to congestion while not over-reacting to the point that they run empty and sacrifice throughput.\nThe key challenge in this approach is to estimate the fraction of bytes encountering congestion. Each switch is simple. If a packet arrives and the switch sees the queue length (K) is above some threshold; e.g., K \u0026gt; (RTT * C) / 7\nwhere C is the link rate in packets per second, then the switch sets the CE bit in the IP header. The complexity of RED is not required.\nThe receiver then maintains a boolean variable for every flow, which we’ll denote DCTCP.CE, and sets it initially to false. When sending an ACK, the receiver sets the ECE (Echo Congestion Experienced) flag in the TCP header if and only if DCTCP.CE is true. It also implements the following state machine in response to every received packet:\nIf the CE bit is set and DCTCP.CE=False, set DCTCP.CE to True and send an immediate ACK.\nIf the CE bit is not set and DCTCP.CE=True, set DCTCP.CE to False and send an immediate ACK.\nOtherwise, ignore the CE bit.\nHTTP Performance (QUIC) # HTTP has been around since the invention of the World Wide Web in the 1990s and from its inception it has run over TCP. HTTP/1.0, the original version, had quite a number of performance problems due to the way it used TCP, such as the fact that every request for an object required a new TCP connection to be set up and then closed after the reply was returned. HTTP/1.1 was proposed at an early stage to make better use of TCP. TCP continued to be the protocol used by HTTP for another twenty-plus years.\nIn fact, TCP continued to be problematic as a protocol to support the Web, especially because a reliable, ordered byte stream isn’t exactly the right model for Web traffic. In particular, since most web pages contain many objects, it makes sense to be able to request many objects in parallel, but TCP only provides a single byte stream. If one packet is lost, TCP waits for its retransmission and successful delivery before continuing, while HTTP would have been happy to receive other objects that were not affected by that single lost packet. Opening multiple TCP connections would appear to be a solution to this, but that has its own set of drawbacks including a lack of shared information about congestion across connections.\nOther factors such as the rise of high-latency wireless networks, the availability of multiple networks for a single device (e.g., Wi-Fi and cellular), and the increasing use of encrypted, authenticated connections on the Web also contributed to the realization that the transport layer for HTTP would benefit from a new approach. The protocol that emerged to fill this need was QUIC.\nQUIC originated at Google in 2012 and was subsequently developed as a proposed standard at the IETF. It has already seen a solid amount of deployment—it is in most Web browsers, many popular websites, and is even starting to be used for non-HTTP applications. Deployability was a key consideration for the designers of the protocol. There are a lot of moving parts to QUIC—its specification spans three RFCs of several hundred pages—but we focus here on its approach to congestion control, which embraces many of the ideas we have seen to date in this book.\nLike TCP, QUIC builds congestion control into the transport, but it does so in a way that recognizes that there is no single perfect congestion control algorithm. Instead, there is an assumption that different senders may use different algorithms. The baseline algorithm in the QUIC specification is similar to TCP NewReno, but a sender can unilaterally choose a different algorithm to use, such as CUBIC. QUIC provides all the machinery to detect lost packets in support of various congestion control algorithms.\nMultipath Transport # While the early hosts connected to the Internet had only a single network interface, it is common these days to have interfaces to at least two different networks on a device. The most common example is a mobile phone with both cellular and WiFi interfaces. Another example is datacenters, which often allocate multiple network interfaces to servers to improve fault tolerance. Many applications use only one of the available networks at a time, but the potential exists to improve performance by using multiple interfaces simultaneously. This idea of multipath communication has been around for decades and led to a body of work at the IETF to standardize extensions to TCP to support end-to-end connections that leverage multiple paths between pairs of hosts. This is known as Multipath TCP (MPTCP).\nA pair of hosts sending traffic over two or more paths simultaneously has implications for congestion control. For example, if both paths share a common bottleneck link, then a naive implementation of one TCP connection per path would acquire twice as much share of the bottleneck bandwidth as a standard TCP connection. The designers of MPTCP set out to address this potential unfairness while also realizing the benefits of multiple paths. The proposed congestion control approach could equally be applied to other transports such as QUIC.\nReferences:\nhttps://tcpcc.systemsapproach.org/aqm.html https://squidarth.com/rc/programming/networking/2018/07/18/intro-congestion "},{"id":31,"href":"/tech-book/docs/networking-tips/tcp-data-transfer/","title":"TCP Data Transfer","section":"Networking Tips","content":" Intro # TCP Connection Establishment Handshake # Sync The initiator that is establishing a connection with a target generates a random sequence number (5,045 for this example) and sends a TCP packet with its sync flag set to 1 and its sequenceNumber set to the just-defined sequence number. Sync/Ack Upon receipt of the TCP Sync packet from the initiator, the target sets its ack number value to the received sequenceNumber + 1 (5,046 in this example). The target responds by setting its own sequence number to a random value (17,764 in this example) and sending a TCP packet whose sync and ack flags are both set to 1 and whose sequenceNumber is set to the just-defined sequence number value and whose ackNumber is set to the target’s just-set ack number. Ack Upon receipt of the TCP Sync/Ack packet, the initiator sets its ack number to the received sequenceNumber + 1. The initiator then sends a TCP Ack packet to the target whose ack flag is set to 1 and whose sequenceNumber and ackNumber are set to the initiator’s corresponding internal values. TCP Connection Termination Handshake # The steps in a four-way TCP connection termination handshake are thus.\nInitiator Finish One side of the connection (the initiator) sends a TCP packet to its connection partner whose finish bit is set to 1. Target Ack or Finish/Ack The target of the initial termination message responds by sending a TCP packet to the initiator whose ack bit is set to 1. This acknowledges that the initiator-to-target data connection is closed and that the initiator will no longer send TCP data packets to the target. The target responds by sending two TCP packets to the initiator: one whose ack bit is set to 1 and one whose finish bit is set to 1. If the target has no more data to send to the initiator, it may combine these two TCP packets into a single TCP packet where both ack and finish are set to 1. If the target does have more data to send, the connection may remain “half open” until the data transmissions are complete. Initiator Ack Finally the initiator sends a TCP packet whose ack bit is set to 1 to complete the connection termination process. Reliable Data Delivery # For a data connection to be reliable, three types of problems must be detected and corrected. These are: * out-of-order data * missing data * duplicated data\nTCP’s sequence numbers address all three of these problems.\nTCP Sequence Number Behavior # Briefly: The initial sequence number is chosen randomly. As the TCP sender transmits bytes to the receiver, each TCP packet includes a sequenceNumber value that indicates the cumulative sequence number of the last byte of each packet. The TCP receiver responds by transmitting TCP Ack packets whose ackNumber value identifies the highest-numbered byte received without any gaps from the first byte of the TCP session. Sequence numbers roll over through zero upon exceeding the maximum value that a 32-bit number can hold.\nTCP Data Reorder # You can see the presence of sequenceNumber in each packet means that a TCP receiver can position each received packet within a TCP reorder buffer where the original sequence of data bytes may be reassembled. Let’s consider a simple scenario where the initial sequence number is 0. If the first packet received by the TCP receiver has a sequenceNumber value of 1,000 and the packet has a TCP data payload of 1,000 bytes, then this packet neatly fits within the buffer at offset 0. The next packet received by the TCP receiver has a sequenceNumber value of 3,000 and carries 1,000 bytes of TCP data. The receiver subtracts the data byte count from the received sequenceNumber value to arrive at a sequence number of 2,000 for this packet’s first data byte. Thus, the data is written to the buffer starting at offset 2,000, leaving a 1,000-byte gap between the first and second received packets. Finally, a TCP packet with 1,000 data bytes and a sequenceNumber of 2,000 is received, meaning the sequence number of the first byte of this packet is 1,000. This third packet’s data is written into the 1,000-byte gap between the first two packets’ data, completing the in-order data transfer. Missing Data Retransmission # Using the data reordering example from above as a starting point, let’s assume that the TCP packet whose sequenceNumber value is 2,000 wasn’t simply delivered out of order, but was lost due to any of a variety of the usual things that happen on networks. From the TCP receiver’s perspective, the order in which the TCP data packets are received and its responses to the data packets are the same regardless of whether a packet was lost in transmission (and later retransmitted) or simply delivered out of order by the network. In response to the receipt of the first TCP data packet, the TCP receiver transmits a TCP Ack packet back to the sender with an ackNumber value of 1,000, indicating that is has successfully received bytes 0 through 999. The second TCP data packet received by the TCP receiver indicates that a gap exists from byte 1,000 through 1,999. To inform the TCP sender of this, the TCP receiver transmits another TCP Ack packet with the exact same ackNumber value of 1,000. This informs the TCP sender that, while the first 1,000 bytes have been successfully received, the next packet (and possibly more) is currently missing. This duplicate acknowledge (the sender has already received an acknowledge for the first 1,000 bytes) motivates the sender to retransmit the first unacknowledged packet (i.e., the packet whose sequenceNumber is 2,000). The TCP sender, in turn, keeps track of the TCP Ack messages that have been received and maintains an awareness of the highest-numbered data byte that’s been successfully transmitted. Every unacknowledged byte is subject to retransmission. Re-transmissions and acknowledges may also get lost. Hence, TCP maintains timers to detect these cases and automatically retransmit either data or Ack packets as necessary to keep the process moving along.\nDuplicate Data Discard # Duplicate data may be received by a TCP receiver if a packet that is assumed missing is simply delayed and delivered out of order. The TCP receiver may request another copy of the packet through the use of a TCP Ack message before the delayed data packet is received. If both copies of the data packet are eventually successfully received, then the receiver must deal with redundant data. If a TCP receiver receives a TCP data packet whose sequenceNumber corresponds to a data packet that had previously been received (regardless of whether or not it had been acknowledged), the TCP receiver may either discard the packet or simply overwrite the data in its receive buffer with the newly received data. Since the data is the same, the data in the buffer does not change.\nSelective Acknowledge # If a TCP data packet is lost before arriving at a receiver, but that missing packet is followed by several successfully received TCP data packets, the receiver has no effective means for alerting the sender to just retransmit the single missing packet. Selective acknowledgment (defined by RFC 2018 and known colloquially as SACK) is an optional extension to TCP that addresses this specific, common scenario.\nReferences:\n"},{"id":32,"href":"/tech-book/posts/2022-01-18-first-doc/","title":"First Blog","section":"Blog","content":" Preface # This is my black board for my future technical book. There is no structure of this blog posts. Whenever I find a good technical literature, I am planning to add it here.\nFeedback is very important for any development cycle. Please drop a message at prasenjit.manna@gmail.com.\nThanks, Prasenjit Manna\n"},{"id":33,"href":"/tech-book/docs/5g/","title":"5G","section":"Example Site","content":" 5G Introduction # "},{"id":34,"href":"/tech-book/docs/algorithms/","title":"Algorithms","section":"Example Site","content":" Algorithms # In this sections, all the interesting algorithms will be classified into simple, medium and hard.\n"},{"id":35,"href":"/tech-book/docs/ai-ml-dc/","title":"Data Center Networking for AI Clusters","section":"Example Site","content":" Data Center Networking for AI Clusters # In this sections, all the data center topics will be classified into simple, medium and hard.\n"},{"id":36,"href":"/tech-book/docs/data-center/","title":"Data Center Tips","section":"Example Site","content":" Data Center Topics # In this sections, all the data center topics will be classified into simple, medium and hard.\n"},{"id":37,"href":"/tech-book/docs/manageability/","title":"Manageability","section":"Example Site","content":" Manageability Topics # In this sections, all the manageability topics will be classified into simple, medium and hard.\n"},{"id":38,"href":"/tech-book/docs/networking-tips/","title":"Networking Tips","section":"Example Site","content":" Networking Tips # In this sections, all the interesting networking tips will be classified into simple, medium and hard.\n"},{"id":39,"href":"/tech-book/docs/optical-knowledge/","title":"Optical Knowledge","section":"Example Site","content":" Optical Knowledge Topics # "},{"id":40,"href":"/tech-book/docs/programming-tips/","title":"Programming Tips","section":"Example Site","content":" Programming Tips # Bit Manipulation # XORing a bit with 1 always flips the bit, whereas XO Ring with O will never change it. Miscellaneous # Passing a 2D array to a C++ function\nThere are three ways to pass a 2D array to a function:\nThe parameter is a 2D array\nint array[10][10]; void passFunc(int a[][10]) { // ... } passFunc(array); The parameter is an array containing pointers\nint *array[10]; for(int i = 0; i \u0026lt; 10; i++) array[i] = new int[10]; void passFunc(int *a[10]) //Array containing pointers { // ... } passFunc(array); The parameter is a pointer to a pointer\nint **array; array = new int *[10]; for(int i = 0; i \u0026lt;10; i++) array[i] = new int[10]; void passFunc(int **a) { // ... } passFunc(array); "},{"id":41,"href":"/tech-book/docs/systemdesign-tips/","title":"SystemDesign-Tips","section":"Example Site","content":" System-Tips # In this sections, all the essential concents will be described.\nStorage # Disk - HDD(Hard-disk drive) and SSD(solid state drive). SSD is faster than HDD, hence costlier also. Persistent Storage. Memory - RAM (Random access momory). Volatile storage Latency and Throughput # Latency - Time it takes for a certain operation to complete, unit msec or sec.\nReading 1 MB from RAM: 250 us (0.25ms)\nReading 1 MB from SSD: 1,000 ps (1 ms)\nReading 1 MB from HDD: 20,000 is (20 ms)\nTransfer 1 MB over Network: 10,000 pus (10 ms)\nInter-Continental Round Trip: 150,000 ps (150 ms)\nThroughput - The number of operations that a system can handle properly per time unit. For instance the throughput of a sec measured in requests per second (RPS or QPS).\nAvailability # Availability - The odds of a particular server or service being up and running at any point in time, usually measured in percentages. A server that has 99% availability will be operational 99% of the time (this would be described as having two nines of availability).\nHigh Availability - Used to describe systems that have particularly high levels of availability, typically 5 nines or more; sometimes abbreviated \u0026ldquo;HA\u0026rdquo;,\nNines - Typically refers to percentages of uptime. For example, 5 nines of availability means an uptime of 99.999% of the time. Below are the downtimes expected per year depending on those 9s:\n99% (two 9s): 87.7 hours\n99.9% (three 9s): 8.8 hours\n99.99%: 52.6 minutes\n99.999%: 5.3 minutes\nCaching # Cache - A piece of hardware or software that stores data, typically meant to retrieve that data faster than otherwise. Caches are often used to store responses to network requests as well as results of computationally tong operations. Note that data in a cache can become stale if the main source of truth for that data (i.e., the main database behind the cache) gets updated and the cache doesn\u0026rsquo;t.\nCache Hit When requested data is found in a cache.\nCache Miss When requested data could have been found in a cache but isn\u0026rsquo;t. This is typically used to refer to a negative consequence of a system failure or of a poor design choice. For example: If a server goes down, our load balancer will have to forward requests to a new server, which will result in cache misses.\nCache Eviction Policy The policy by which values get evicted or removed from a cache. Popular cache eviction policies include LRU (least-recently used), FIFO (first in first out), and LFU (least-frequently used).\nContent Delivery Network A CDN is a third-party service that acts like a cache for your servers. Sometimes, web applications can be slow for users in a particular region if your servers are located only in another region. A CDN has servers all around the world, meaning that the latency to a CDN\u0026rsquo;s servers will almost always be far better than the latency to your servers. A CDN\u0026rsquo;s servers are often referred to as PoPs (Points of Presence). Two of the most popular CDNs are Cloudflare and Google Claud CDN.\nProxies # Forward Proxy A server that sits between a client and servers and acts on behalf of the client, typically used to mask the client\u0026rsquo;s identity (IP address), Note that forward proxies are often referred to as just proxies.\nReverse Proxy A server that sits between clients and servers and acts on behalf of the servers, typically used for logging, load balancing, or caching. | nginx @ Pronounced \u0026ldquo;engine X°\u0026ndash;not \u0026ldquo;N jinx”, Nginx is a very popular webserver that\u0026rsquo;s often used as a reverse proxy and load balancer. Learn more: https://www.nginx.com/\nLoad Balancer A type of reverse proxy that distributes traffic across servers, Load balancers can be found in many parts of a system, from the DNS layer all the way to the database layer. Learn more: https://www.nginx.com/\nServer-Selection Strategy How a load balancer chooses servers when distributing traffic amongst multiple servers. Commonly used strategies include roundrobin, random selection, performance-based selection (choosing the server with the best performance metrics, like the fastest response time or the least amount of traffic), and IP-based routing.\nDatabases # Relational Database A type of structured database in which data is stored following a tabular format; often supports powerful querying using SQL. Learn more: https://www.postgresql.org/\nNon-Relational Database In contrast with relational database (SQL databases), a type of database that is free of Imposed, tabular-like structure. Non-relational databases are often referred to as NoSQL databases,\nACID Transaction\nAtomicity: The operations that constitute the transaction will either all succeed or ail fail. There is no in-between state. Consistency: The transaction cannot bring the database to an invalid state. After the transaction is committed or rolled back, the rules for each record will still apply, and all future transactions will see the effect of the transaction. Also named Strong Consistency. isolation: The execution of multiple transactions concurrently will have the same effect as if they had been executed sequentially. Durability: Any committed transaction is written to non-volatile storage. It will not be undone by a crash, power loss, or network partition. Strong Consistency Strong Consistency usually refers to the consistency of ACID transactions, as opposed to Eventual Consistency.\nEventual Consistency A consistency mode which is unlike Strong Consistency. In this model, reads might return a view of the system that is stale. An eventually consistent datastore will give guarantees that the state of the database will eventuall reflect writes within a time period.\nNo-SQL Databases # Key-Value Store A Key-Value Store is a flexible NoSQL database that\u0026rsquo;s often used for caching and dynamic configuration. Popular aptions include DynamoDB, Etcd, Redis, and ZooKeeper,\netcd Etcd is a strongly consistent and highly available key-value store that\u0026rsquo;s often used to Implement leader election in a system, Learn more: https://etcd.io/\nRedis An in-memory key-value store. Does offer some persistent storage options but is typically used as a really fast, best-effort caching solution, Redis is also often used to implement rate limiting. Learn more: https://redis.io/\nZookeeper Zookeeper Is a strongly consistent, highly available key-value store. It\u0026rsquo;s often used to store important configuration of to perform leader election. Learn more: https://zookeeper.apache.org/\nDynamoDB An key-value store by AWS, this provides eventual consistency.\nBlob Storage Widely used kind of storage, in small and large scale systems. They don’t really count as databases per se, partially because they only allow the user to store and retrieve data based on the name of the blob. This is sort of like a key-value store but usually blob stores have different guarantees. They might be slower than KV stores but values can be megabytes large (or sometimes gigabytes large). Usually people use this to store things like large binaries, database snapshots, or images and other static assets that a website might have. Blob storage is rather complicated to have on premise, and only giant companies like Google and Amazon have infrastructure that supports it. So usually in the context of System Design interviews you can assume that you will be able to use GCS or S3. These are blob storage services hosted by Google and Amazon respectively, that cost money depending on how much storage you use and how often you store and retrieve blobs from that storage.\nGoogle Cloud Storage(GCS) - is a blob storage service provided by Google. Learn more: https://cloud.google.com/storage\nS3 - ls a blob storage service provided by Amazon through Amazon Web Services (AWS). Learn more: https://aws.amazon.com/s3/\nTime Series Database A TSDB is a special kind of database optimized for storing and analyzing time-indexed data: data points that specifically occur at a given moment in time. Examples of TSDBs are InfluxDB, Prometheus, and Graphite.\ninfluxDB - popular open-source time series database, Learn more; https://www.influxdata.com/\nPrometheus - A popular open source time series database, typically used for monitoring purposes. https://prometheus.io\nGraph Database A type of database that stores data following the graph data model. Data entries in a graph database can have explicitly defined relationships, much like nodes in a graph can have edges. Graph databases take advantage of their underlying graph structure to perform complex queries on deeply connected data very fast. Graph databases are thus often preferred to relational databases when dealing with systems where data points naturally form a graph and have multiple tevels of relationships—for example, social networks.\nNeo4j - a popular grpah DB, consists of nodes, relationships, propreties and labels. https://neo4j.com Spatial Database A type of database optimized for storing and querying spatial data like locations on a map. Spatial databases rely on spatial indexes fike quadtrees to quickly perform spatial queries like finding all locations in the vicinity of a region.\nReplication \u0026amp; Shrading # Replication - The act of duplicating the data from one database server to others. This is sometimes used to increase the redundancy of your system and tolerate regional failures for instance. Other times you can use replication to move data closer to your clients, thus decreasing latency of accessing specific data.\nSharding - Sometimes called data partitioning, sharding is the act of splitting a database into two or more pieces called shards and is typically done to increase the throughput of your database. Popular sharding strategies include:\nSharding based on a client\u0026rsquo;s region. Sharding based on the type of data being stored (e.g: user data gets stored in one shard, payments data gets stored In another shard) Sharding based on the hash of a column (only for structured data) Peer-To-Peer Networks # Peer-To-Peer Network A collection of machines referred to as peers that divide a workload between themselves to presumably complete the workload faster than would otherwise be possible. Peer-to-peer networks are often used in file-distribution systems.\nGossip Protocol When a set of machines talk to each other in a uncoordinated manner in a cluster to spread information through a system without requiring a central source of data. - ad °\nRate Limiting # Rate Limiting The act of limiting the number of requests sent to or from a system. Rate limiting is most often used to limit the number of incoming requests in order to prevent DoS attacks and can be enforced at the IP-address level, at the user-account level, or at the region level, for example. Rate limiting can also be implemented in tiers; for instance, a type of network request could be limited to 1 per second, 5 per 10 seconds, and 10 per minute.\nDoS Attack Short for “denial-of-service attack”, a DoS attack is an attack in which a malicious user tries to bring down or damage a system in order to render it unavailable to users. Much of the time, it consists of flooding it with traffic. Some DoS attacks are easily preventable with rate limiting, while others can be far trickier to defend against.\nDDoS Attack\nShort for “distributed denial-of-service attack\u0026rdquo;, a DDoS attack is a DoS attack in which the traffic flooding the target system comes from many different sources (like thousands of machines), making It much harder to defend against.\nRedis An in-memory key-value store. Does offer some persistent storage options but Is typically used as a really fast, best-effort caching solution, Redis is also often used to implement rate limiting. Learn more: https://redis.io/\nPublish/Subscribe Pattern # Publish/Subscribe Pattern Often shortened as Pub/Sub, the Publish/Subscribe pattern Is a popular messaging model that consists of publishers and subscribers. Publishers publish messages to special topics (sometimes called channels) without caring about or even knowing who will read those messages, and subscribers subscribe to topics and read messages coming through those topics. Pub/Sub systems often come with very powerful guarantees like at-least-once delivery, persistent storage, ordering of messages, and replayability of messages.\nApache Kafka A distributed messaging system created by Linkedin. Very useful when using the streaming paradigm as opposed to polling. Learn more: https://kafka.apache.org/\nCloud pub/sub A highly-scalable Pub/Sub messaging service created by Google, Guarantees at-least-once delivery of messages and supports “rewinding” in order to reprocess messages. Learn more: https://cloud.google.com/pubsub/\nMapReduce # MapReduce A popular framework for processing very large datasets in a distributed setting efficiently, quickly, and in a fault-tolerant manner. A MapReduce job is comprised of 3 main steps:\nthe Map step, which runs a map function on the various chunks of the dataset and transforms these chunks into intermediate key-value pairs. the Shuffle step, which reorganizes the intermediate key-value pairs such that pairs of the same key are routed to the same machine in the final step. the Reduce step, which runs a reduce function on the newly shuffled key-value pairs and transforms them into more meaningful data. The canonical example of a MapReduce use case is counting the number of occurrences of words in a large text file. When dealing with a MapReduce library, engineers and/or systems administrators only need to worry about the map and reduce functions, as well as their inputs and outputs. All other concerns, including the parallelization of tasks and the fault-tolerance of the MapReduce job, are abstracted away and taken care of by the MapReduce implementation. Distributed File System A Distributed Ale System is an abstraction over a (usually large) cluster of machines that allows them to act like one large file system. The two most popular implementations of a DFS are the Google File System (GFS) and the Hadoop Distributed File System (HDFS). Typically, DFSs take care of the classic availability and replication guarantees that can be tricky to obtain in a distributed-system setting, The overarching idea is that files are split into chunks of a certain size (4MB or 64MB, for instance), and those chunks are sharded across a large cluster of machines. A central control plane is in charge of deciding where each chunk resides, routing reads to the right nodes, and handling communication between machines, Olfferent DFS implementations have slightly different APIs and semantics, but they achieve the same common goal: extremely largescale persistent storage,\nHadoop A popular, open-source framework that supports MapReduce jobs and many other kinds of data-processing pipelines. Its central component is HDFS (Hadoop Distributed File System), on top of which other technologies have been developed. Learn more: https://hadoop.apache.org/\nSecurity And HTTPS # Symmetric Encryption A type of encryption that relies on only a single key to both encrypt and decrypt data. The key must be known to all parties involved in the communication and must therefore typically be shared between the parties at one point or another. Symmetric-key algorithms tend to be faster than their asymmetric counterparts. The most widely used symmetric-key algorithms are part of the Advanced Encryption Standard (AES).\nAsymmetric Encryption Also known as public-key encryption, asymmetric encryption relies on two keys—a public key and a private key—to encrypt and decrypt data. The keys are generated using cryptographic algorithms and are mathematically connected such that data encrypted with the public key can only be decrypted with the private key. While the private key must be kept secure to maintain the fidelity of this encryption paradigm, the public key can be openly shared. Asymmetric-key algorithms tend to be slower than their symmetric counterparts.\nAES Stands for Advanced Encryption Standard. AES is a widely used encryption standard that has three symmetric-key algorithms (AES-128, AES-192, and AES-256). Of note, AES is considered to be the \u0026ldquo;gold standard\u0026rdquo; in encryption and is even used by the U.S. National Security Agency to encrypt top secret information. .\nHTTPS The HyperText Transfer Protocol Secure is an extension of HTTP that\u0026rsquo;s used for secure communication online. It requires servers to have trusted certificates (usually SSL certificates) and uses the Transport Layer Security (TLS), a security protocol built on top of TCP, to encrypt data communicated between a client and a server. { TLs The Transport Layer Security is a security protocol over which HTTP runs in order to achieve secure communication online. \u0026ldquo;HTTP over TLS\u0026rdquo; Is also known as HTTPS.\nSSL Certificate A digital certificate granted to a server by a certificate authority. Contains the server\u0026rsquo;s public key, to be used as part of the TLS handshake process in an HTTPS connection. An SSL certificate effectively confirms that a public key belongs to the server claiming it belongs to them. SSL certificates are a crucial defense against man-in-the-middie attacks,\nCertificate Authority A trusted entity that signs digital certificates—namely, SSL certificates that are relied on in HTTPS connections.\nTLS Handshake The process through which a client and a server communicating over HTTPS exchange encryption-related information and establish a secure communication. The typical steps in a TLS handshake are roughly as follows:\nThe client sends a client hello string of random bytes—to the server. The server responds with a server hello another string of random bytes—as well as its SSL certificate, which contains its publle key. The client verifies that the certificate was issued by a certificate authority and sends a premaster secret—yet another string of random bytes, this time encrypted with the server\u0026rsquo;s public key—to the server. The client and the server use the client hello, the server helio, and the premaster secret to then generate the same symmetric encryption session keys, to be used to encrypt and decrypt all data communicated during the remainder of the connection. "}]